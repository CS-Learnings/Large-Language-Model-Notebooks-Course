{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Large-Language-Model-Notebooks-Course/blob/inference-adaptative-attention-pruning/6-PRUNING/6_6b_Adaptive_Inference_Attention_Pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV5KPbfseL8S"
      },
      "source": [
        "<div>\n",
        "    <h1>Large Language Models Projects</a></h1>\n",
        "    <h3>Apply and Implement Strategies for Large Language Models</h3>\n",
        "    <h2>Pruning Attention Layers</h2>\n",
        "    <h3>Not All Attention is needed</h3>\n",
        "</div>\n",
        "\n",
        "by [Pere Martra](https://www.linkedin.com/in/pere-martra/)\n",
        "\n",
        "_______\n",
        "Models: meta-llama/Llama-3.2\n",
        "\n",
        "Colab Environment: GPU L4 for 3B Models\n",
        "\n",
        "T4 for 1B Model.\n",
        "\n",
        "Keys:\n",
        "* Pruning\n",
        "* Attention\n",
        "\n",
        "References:\n",
        "* [What Matters in Transformers? Not All Attention is Needed](https://arxiv.org/abs/2406.15786)\n",
        "* [Resource-Efficient Transformer Pruning for Finetuning of Large Models](https://openaccess.thecvf.com/content/CVPR2024/html/Ilhan_Resource-Efficient_Transformer_Pruning_for_Finetuning_of_Large_Models_CVPR_2024_paper.html)\n",
        "\n",
        "_______\n",
        "**disclaimer: The pruning / knowledge distillation section has been created after the first edition of the book was published. They are not included in the book’s original content but are intended to supplement and expand on the topics covered.**\n",
        "\n",
        "This is the unofficial repository for the book:\n",
        "        <a href=\"https://amzn.to/4eanT1g\"> <b>Large Language Models:</b> Apply and Implement Strategies for Large Language Models</a> (Apress).\n",
        "        The book is based on the content of this repository, but the notebooks are being updated, and I am incorporating new examples and chapters.\n",
        "        If you are looking for the official repository for the book, with the original notebooks, you should visit the\n",
        "        <a href=\"https://github.com/Apress/Large-Language-Models-Projects\">Apress repository</a>, where you can find all the notebooks in their original format as they appear in the book.\n",
        "\n",
        "______\n",
        "# Introduction\n",
        "This notebook implements the paper: [What Matters in Transformers? Not all Attention is Needed](https://arxiv.org/abs/2406.15786).\n",
        "Although I followed the paper's guidelines, I made some adjustments to make the code clearer and easier to understand.\n",
        "\n",
        "The original paper demonstrates that larger models tend to have excessive redundancy in their attention layers. They achieved a 48.4% increase in inference performance for a Llama-2-70B model with only a minor 2.4% drop in response quality, **just bypassing the 50% of the Attention layers!**\n",
        "\n",
        "In this notebook, tests have been conducted using Llama-3.2-1B and 3B models. With these models, I found that removing 50% of the attention layers significantly impacted the model's functionality. However, the 3B model handled the removal of these layers much better. This suggests that redundancy may become more pronounced as model size increases.\n",
        "\n",
        "# Methodology.\n",
        "To identify which layers contribute the least, the cosine distance between the layer's input and output is measured. In the paper, this distance is calculated using a test dataset, while in the notebook, I used a simple prompt to activate the layers.  \n",
        "\n",
        "This method of measuring the importance of attention layers and their contribution to the model allows pruning to be tailored to a specific dataset. This approach can lead to the creation of more efficient models for specialized sectors such as healthcare or finance.\n",
        "\n",
        "\n",
        "Once the layer contributing the least to the final output is identified (the one with the smallest difference between input and output), it is added to a list included in the configuration file.\n",
        "\n",
        "This list is then referenced during inference by a new forward function that replaces the original one for attention layers. When this new function detects that a layer is in the list, it skips its execution and simply returns the input without modifications.\n",
        "\n",
        "The process of identifying layers to deactivate and marking them as non-executable is one-shot. In other words, it does  determine all the layers to skip in one go, as recommended in the paper.\n",
        "\n",
        "The iterative implementation has a significant drawback: the test dataset must be processed for each layer to be deactivated. The paper's authors note that while the iterative method may bring slight improvements, the added computational cost is not justified. However, since this is an example notebook, and there is no test dataset—just a small prompt—and the layer selection process takes only seconds, I chose the iterative approach.\n",
        "\n",
        "This pruning method does not produce a smaller model, as the layers are not physically removed. They remain in the model but are not executed, resulting in improved inference response times.\n",
        "______"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwYeKwswnkTG"
      },
      "source": [
        "# Install libraries & Configure variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PblPrYCiYTl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdbc82b2-296a-4c9a-891e-e61cb4d01099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m317.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hf_xet\n",
            "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
            "Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf_xet\n",
            "Successfully installed hf_xet-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch==2.6.0\n",
        "!pip install -q torchvision==0.21.0\n",
        "!pip install -q transformers==4.51.3\n",
        "!pip install -q datasets==3.6.0\n",
        "!pip install -q lm-eval==0.4.8\n",
        "\n",
        "!pip install hf_xet #To speed up downloads from HF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6qN0mu6IHqpy"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptative Configuration."
      ],
      "metadata": {
        "id": "hYdRIDoV8R3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ADAPTIVE ATTENTION BYPASS (AAB) CONFIGURATION - CORRECTED SCALING TO 100%\n",
        "# =============================================================================\n",
        "\n",
        "GLOBAL_COMPLEXITIES = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "COMPLEXITY_WEIGHTS = {\n",
        "    \"token_count\": 0.65,\n",
        "    \"embedding_variance\": 0.35\n",
        "}\n",
        "\n",
        "ADAPTIVE_CONFIG = {\n",
        "    # Model size-based ratios with proportional scaling to 100%\n",
        "    \"model_size_ratios\": {\n",
        "        \"70B+\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.15, \"scaling_factor\": 0.85},\n",
        "            \"simple\": {\"min_ratio\": 0.35, \"scaling_factor\": 0.65},\n",
        "            \"medium\": {\"min_ratio\": 0.55, \"scaling_factor\": 0.45},\n",
        "            \"complex\": {\"min_ratio\": 0.75, \"scaling_factor\": 0.25},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"30B-70B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.25, \"scaling_factor\": 0.75},\n",
        "            \"simple\": {\"min_ratio\": 0.40, \"scaling_factor\": 0.60},\n",
        "            \"medium\": {\"min_ratio\": 0.60, \"scaling_factor\": 0.40},\n",
        "            \"complex\": {\"min_ratio\": 0.80, \"scaling_factor\": 0.20},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"10B-30B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.30, \"scaling_factor\": 0.75},\n",
        "            \"simple\": {\"min_ratio\": 0.45, \"scaling_factor\": 0.55},\n",
        "            \"medium\": {\"min_ratio\": 0.65, \"scaling_factor\": 0.35},\n",
        "            \"complex\": {\"min_ratio\": 0.82, \"scaling_factor\": 0.18},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"5B-10B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.35, \"scaling_factor\": 0.65},\n",
        "            \"simple\": {\"min_ratio\": 0.55, \"scaling_factor\": 0.45},\n",
        "            \"medium\": {\"min_ratio\": 0.75, \"scaling_factor\": 0.25},\n",
        "            \"complex\": {\"min_ratio\": 0.87, \"scaling_factor\": 0.13},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"2B-5B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.60, \"scaling_factor\": 0.40},\n",
        "            \"simple\": {\"min_ratio\": 0.75, \"scaling_factor\": 0.20},\n",
        "            \"medium\": {\"min_ratio\": 0.90, \"scaling_factor\": 0.20},\n",
        "            \"complex\": {\"min_ratio\": 0.95, \"scaling_factor\": 0.10},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"<2B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.80, \"scaling_factor\": 0.20},\n",
        "            \"simple\": {\"min_ratio\": 0.85, \"scaling_factor\": 0.15},\n",
        "            \"medium\": {\"min_ratio\": 0.90, \"scaling_factor\": 0.10},\n",
        "            \"complex\": {\"min_ratio\": 0.95, \"scaling_factor\": 0.05},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # 5-level complexity thresholds and descriptions\n",
        "    \"complexity_levels\": {\n",
        "        \"trivial\": {\n",
        "            \"range\": [0.0, 0.2],\n",
        "            \"description\": \"Single word answers, basic arithmetic\",\n",
        "            \"layer_groups\": [\"imprescindibles\", \"critical\"]\n",
        "        },\n",
        "        \"simple\": {\n",
        "            \"range\": [0.2, 0.4],\n",
        "            \"description\": \"Simple factual questions\",\n",
        "            \"layer_groups\": [\"imprescindibles\", \"critical\", \"important\"]\n",
        "        },\n",
        "        \"medium\": {\n",
        "            \"range\": [0.4, 0.6],\n",
        "            \"description\": \"Knowledge retrieval, completion tasks\",\n",
        "            \"layer_groups\": [\"imprescindibles\", \"critical\", \"important\", \"standard\"]\n",
        "        },\n",
        "        \"complex\": {\n",
        "            \"range\": [0.6, 0.8],\n",
        "            \"description\": \"Reasoning, explanations\",\n",
        "            \"layer_groups\": [\"imprescindibles\", \"critical\", \"important\", \"standard\", \"optional\"]\n",
        "        },\n",
        "        \"very_complex\": {\n",
        "            \"range\": [0.8, 1.0],\n",
        "            \"description\": \"Deep reasoning, analysis, creativity\",\n",
        "            \"layer_groups\": [\"imprescindibles\", \"critical\", \"important\", \"standard\", \"optional\", \"dispensable\"]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # Layer activation thresholds (1:1 correspondence)\n",
        "    \"activation_thresholds\": {\n",
        "        \"critical\": 0.0,      # Always active above trivial level\n",
        "        \"important\": 0.2,     # Active from simple level\n",
        "        \"standard\": 0.4,      # Active from medium level\n",
        "        \"optional\": 0.6,      # Active from complex level\n",
        "        \"dispensable\": 0.8    # Active from very_complex level\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "feRLhLsw8Voz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support & calculate functions"
      ],
      "metadata": {
        "id": "tlPEOEaj8b7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_model_size_category(model):\n",
        "    \"\"\"\n",
        "    Automatically detect model size category from model parameters\n",
        "    \"\"\"\n",
        "    try:\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        size_billion = total_params / 1e9\n",
        "\n",
        "        print(f\"🔍 Detected model size: {size_billion:.2f}B parameters\")\n",
        "\n",
        "        if size_billion >= 70:\n",
        "            return \"70B+\"\n",
        "        elif size_billion >= 30:\n",
        "            return \"30B-70B\"\n",
        "        elif size_billion >= 10:\n",
        "            return \"10B-30B\"\n",
        "        elif size_billion >= 5:\n",
        "            return \"5B-10B\"\n",
        "        elif size_billion >= 2:\n",
        "            return \"2B-5B\"\n",
        "        else:\n",
        "            return \"<2B\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error detecting model size: {e}\")\n",
        "        return \"1B-3B\"\n",
        "\n",
        "\n",
        "def get_context_window_size(model, tokenizer=None):\n",
        "    \"\"\"\n",
        "    Tries to detect the model's maximum context window size (in tokens).\n",
        "    \"\"\"\n",
        "    # Try model config first (most reliable)\n",
        "    if hasattr(model, 'config'):\n",
        "        config = model.config\n",
        "\n",
        "        # Common attribute names for context window\n",
        "        context_attrs = ['max_position_embeddings', 'n_positions', 'max_seq_len',\n",
        "                        'seq_length', 'max_sequence_length', 'context_length']\n",
        "\n",
        "        for attr in context_attrs:\n",
        "            if hasattr(config, attr):\n",
        "                value = getattr(config, attr)\n",
        "                if isinstance(value, int) and value > 0:\n",
        "                    return value\n",
        "\n",
        "    # Try tokenizer as fallback\n",
        "    if tokenizer:\n",
        "        if hasattr(tokenizer, 'model_max_length') and tokenizer.model_max_length != int(1e30):\n",
        "            return tokenizer.model_max_length\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def count_attention_layers_correctly(model):\n",
        "    \"\"\"\n",
        "    Correctly count attention layers by finding main decoder/transformer layers\n",
        "    \"\"\"\n",
        "    # Method 1: Count main decoder layers directly (most reliable)\n",
        "    decoder_layer_count = 0\n",
        "    for name, module in model.named_modules():\n",
        "        module_type = type(module).__name__\n",
        "        # Look for main transformer/decoder layers\n",
        "        if any(layer_type in module_type for layer_type in\n",
        "               ['DecoderLayer', 'TransformerBlock', 'Block', 'Layer']) and \\\n",
        "           any(exclude not in module_type for exclude in\n",
        "               ['Embedding', 'Norm', 'Linear', 'MLP', 'Attention']):\n",
        "            # Make sure it's a numbered layer (e.g., layers.0, layers.1, etc.)\n",
        "            if '.layers.' in name and name.count('.') == 2:  # e.g., \"model.layers.0\"\n",
        "                decoder_layer_count += 1\n",
        "\n",
        "    if decoder_layer_count > 0:\n",
        "        return decoder_layer_count\n",
        "\n",
        "    # Method 2: Use model config as fallback\n",
        "    try:\n",
        "        if hasattr(model, 'config'):\n",
        "            config_attrs = ['num_hidden_layers', 'n_layer', 'num_layers', 'n_layers']\n",
        "            for attr in config_attrs:\n",
        "                if hasattr(model.config, attr):\n",
        "                    return getattr(model.config, attr)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Method 3: Direct access to layers ModuleList\n",
        "    try:\n",
        "        if hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
        "            return len(model.model.layers)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return 16  # Conservative fallback\n",
        "\n",
        "\n",
        "def classify_complexity_level(complexity_score):\n",
        "    \"\"\"\n",
        "    Classify complexity score into one of 5 levels\n",
        "\n",
        "    Args:\n",
        "        complexity_score (float): Complexity score (0.0-1.0)\n",
        "\n",
        "    Returns:\n",
        "        str: Complexity level (\"trivial\", \"simple\", \"medium\", \"complex\", \"very_complex\")\n",
        "    \"\"\"\n",
        "    levels = ADAPTIVE_CONFIG[\"complexity_levels\"]\n",
        "\n",
        "    for level_name, level_config in levels.items():\n",
        "        min_val, max_val = level_config[\"range\"]\n",
        "        if min_val <= complexity_score < max_val:\n",
        "            return level_name\n",
        "\n",
        "    # Handle edge case for exactly 1.0\n",
        "    if complexity_score >= 0.8:\n",
        "        return \"very_complex\"\n",
        "\n",
        "    return \"trivial\"  # Fallback\n",
        "\n",
        "\n",
        "def calculate_active_layers(total_layers, model_size_category, complexity_score):\n",
        "    \"\"\"\n",
        "    Calculate number of active layers based on complexity and model size\n",
        "\n",
        "    Args:\n",
        "        total_layers (int): Total number of attention layers\n",
        "        model_size_category (str): Model size category\n",
        "        complexity_score (float): Complexity score (0.0-1.0)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (active_layers_count, complexity_level, layer_groups_used, min_guaranteed, max_possible)\n",
        "    \"\"\"\n",
        "    # Classify complexity level\n",
        "    complexity_level = classify_complexity_level(complexity_score)\n",
        "\n",
        "    # Get configuration for this model size and complexity\n",
        "    config = ADAPTIVE_CONFIG[\"model_size_ratios\"][model_size_category][complexity_level]\n",
        "    min_ratio = config[\"min_ratio\"]\n",
        "    scaling_factor = config[\"scaling_factor\"]\n",
        "\n",
        "    # Calculate layer counts\n",
        "    min_guaranteed = int(total_layers * min_ratio)\n",
        "    remaining_layers = total_layers - min_guaranteed\n",
        "    additional_layers = int(complexity_score * scaling_factor * remaining_layers)\n",
        "    active_layers = min_guaranteed + additional_layers\n",
        "\n",
        "    # Ensure we don't exceed total layers\n",
        "    active_layers = min(active_layers, total_layers)\n",
        "    max_possible = total_layers  # Always can reach 100%\n",
        "\n",
        "    # Get which layer groups should be used\n",
        "    layer_groups_used = ADAPTIVE_CONFIG[\"complexity_levels\"][complexity_level][\"layer_groups\"]\n",
        "\n",
        "    return active_layers, complexity_level, layer_groups_used, min_guaranteed, max_possible\n",
        "\n",
        "\n",
        "def get_complexity_info(complexity_level):\n",
        "    \"\"\"\n",
        "    Get detailed information about a complexity level\n",
        "    \"\"\"\n",
        "    return ADAPTIVE_CONFIG[\"complexity_levels\"][complexity_level]\n",
        "\n",
        "\n",
        "print(\"✅ Enhanced 5-level calculation functions loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n-xWD2o8hmA",
        "outputId": "a2ffa1bd-6839-4498-b240-6269c6dd2cd9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Enhanced 5-level calculation functions loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8iRr5iapy5q"
      },
      "source": [
        "# Download the Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9IFTYa9P6Zy",
        "outputId": "ed8b1dbd-e80d-4a78-c9d5-9b09f54c7a00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbnDW_tZRTGp"
      },
      "outputs": [],
      "source": [
        "model_name = 'meta-llama/Llama-3.2-1B'\n",
        "#model_name = 'meta-llama/Llama-3.2-3B'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#tokenizer.pad_token = tokenizer.eos_token  # Set pad token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "TkQnkc3U8l-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the configuration with clean, simplified output\n",
        "if 'model' in locals():\n",
        "    # Get model information with improved detection\n",
        "    total_attention_layers = count_attention_layers_correctly(model)\n",
        "    model_category = detect_model_size_category(model)\n",
        "    context_window = get_context_window_size(model, tokenizer if 'tokenizer' in locals() else None)\n",
        "\n",
        "    print(f\"\\n🏗️ Model Analysis:\")\n",
        "    print(f\"   Attention layers: {total_attention_layers}\")\n",
        "    print(f\"   Size category: {model_category}\")\n",
        "    print(f\"   Context window: {context_window if context_window else 'Unknown'} tokens\")\n",
        "    print(f\"   Architecture: {type(model).__name__}\")\n",
        "\n",
        "    # Show layer detection verification\n",
        "    print(f\"\\n🔍 Layer Detection Verification:\")\n",
        "    decoder_layers = [name for name, module in model.named_modules()\n",
        "                     if 'DecoderLayer' in type(module).__name__ and '.layers.' in name]\n",
        "    print(f\"   Found DecoderLayers: {len(decoder_layers)}\")\n",
        "\n",
        "    # Test all 5 complexity levels with simplified table\n",
        "    test_complexities = GLOBAL_COMPLEXITIES\n",
        "\n",
        "    print(\"\\n🧪 Layer Activation by Complexity Level:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"{'Level':<12} {'Active Layers':<15} {'Usage Ratio':<12}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for complexity in test_complexities:\n",
        "        active, level, groups, min_guaranteed, max_possible = calculate_active_layers(\n",
        "            total_attention_layers, model_category, complexity\n",
        "        )\n",
        "        ratio = active / total_attention_layers\n",
        "\n",
        "        print(f\"{level.capitalize():<12} {active:<15} {ratio:<12.1%}\")\n",
        "\n",
        "    print(f\"\\n📊 Summary for {model_category} model:\")\n",
        "    trivial_config = ADAPTIVE_CONFIG['model_size_ratios'][model_category]['trivial']\n",
        "    trivial_min = int(total_attention_layers * trivial_config['min_ratio'])\n",
        "    print(f\"   • Range: {trivial_min}-{total_attention_layers} layers ({trivial_min/total_attention_layers:.1%}-100%)\")\n",
        "    print(f\"   • Context window: {context_window if context_window else 'Unknown'} tokens\")\n",
        "    print(f\"   • All complexity levels can reach 100% layer usage\")\n",
        "\n",
        "else:\n",
        "    print(\"⏳ Load your model first to test the configuration\")\n",
        "    print(\"\\nTo test, make sure you have:\")\n",
        "    print(\"1. model = ... (your loaded model)\")\n",
        "    print(\"2. tokenizer = ... (optional, your tokenizer)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rUlhopw8s63",
        "outputId": "23acbfa6-98d4-464b-bde8-22416ad975ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Detected model size: 1.24B parameters\n",
            "\n",
            "🏗️ Model Analysis:\n",
            "   Attention layers: 16\n",
            "   Size category: <2B\n",
            "   Context window: 131072 tokens\n",
            "   Architecture: LlamaForCausalLM\n",
            "\n",
            "🔍 Layer Detection Verification:\n",
            "   Found DecoderLayers: 16\n",
            "\n",
            "🧪 Layer Activation by Complexity Level:\n",
            "==================================================\n",
            "Level        Active Layers   Usage Ratio \n",
            "--------------------------------------------------\n",
            "Trivial      12              75.0%       \n",
            "Simple       13              81.2%       \n",
            "Medium       14              87.5%       \n",
            "Complex      15              93.8%       \n",
            "Very_complex 16              100.0%      \n",
            "\n",
            "📊 Summary for <2B model:\n",
            "   • Range: 12-16 layers (75.0%-100%)\n",
            "   • Context window: 131072 tokens\n",
            "   • All complexity levels can reach 100% layer usage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0ifeungrbHw"
      },
      "source": [
        "## Study the structure.\n",
        "* Llama-3.2-1B\n",
        "```\n",
        "LlamaForCausalLM(\n",
        "  (model): LlamaModel(\n",
        "    (embed_tokens): Embedding(128256, 2048)\n",
        "    (layers): ModuleList(\n",
        "      (0-15): 16 x LlamaDecoderLayer(\n",
        "        (self_attn): LlamaSdpaAttention(\n",
        "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
        "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
        "          (rotary_emb): LlamaRotaryEmbedding()\n",
        "        )\n",
        "        (mlp): LlamaMLP(\n",
        "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
        "          (act_fn): SiLU()\n",
        "        )\n",
        "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "      )\n",
        "    )\n",
        "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "    (rotary_emb): LlamaRotaryEmbedding()\n",
        "  )\n",
        "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "The model follows the typical structure of modern Llama models, consisting of blocks made up of an Attention layer and an MLP layer with a GLU structure.\n",
        "\n",
        "> If you want to see an example of how to perform pruning on the MLP layers of the model, you can check out the notebook:[Pruning Llama 3.2.](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/6_3_pruning_structured_llama3.2-1b_OK.ipynb) y leer el paper [Exploring GLU expansion ratios: Structured pruning in Llama-3.2 models](https://osf.io/preprints/osf/qgxea)\n",
        "\n",
        "\n",
        "\n",
        "Since the layers form a block, the attention layer cannot be removed without also removing the accompanying MLP layer. For this reason, the decision was made to deactivate their execution during inference.\n",
        "\n",
        "The 1B model has 16 layers, as shown in the structure above, while the 3B model has 28 layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference function & Test Base Model\n",
        "\n",
        "The `get_output` function is designed to generate text  and measure the time taken for different stages of the generation process.\n",
        "\n",
        "It provides insights into the performance of the model and can be used to evaluate the efficiency of text generation."
      ],
      "metadata": {
        "id": "vF4cHUb_rICs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2igvy4z6rGgy"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def get_output(prompt, model=model, tokenizer=tokenizer, num_runs=1, max_length=50):\n",
        "    total_time = 0\n",
        "    generated_outputs = []\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        # Start timing\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Tokenization time\n",
        "        token_start = time.time()\n",
        "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "        token_time = time.time() - token_start\n",
        "\n",
        "        # Generation time\n",
        "        gen_start = time.time()\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            temperature=None,\n",
        "            top_p=None,\n",
        "            do_sample=False,  # Disable sampling\n",
        "            num_beams=5,      # Use beam search\n",
        "            early_stopping=True,  # Stop when end-of-sequence token is generated\n",
        "            no_repeat_ngram_size=2  # Prevent repetition of 2-grams\n",
        "        )\n",
        "        gen_time = time.time() - gen_start\n",
        "\n",
        "        # Decoding time\n",
        "        decode_start = time.time()\n",
        "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        decode_time = time.time() - decode_start\n",
        "\n",
        "        # Total time for this run\n",
        "        total_time += time.time() - start_time\n",
        "        generated_outputs.append(generated)\n",
        "\n",
        "        if num_runs > 1:\n",
        "            print(f\"\\nRun {run + 1}:\")\n",
        "        print(f\"Tokenization time: {token_time*1000:.2f} ms\")\n",
        "        print(f\"Generation time: {gen_time*1000:.2f} ms\")\n",
        "        print(f\"Decoding time: {decode_time*1000:.2f} ms\")\n",
        "        print(f\"Total time: {(time.time() - start_time)*1000:.2f} ms\")\n",
        "\n",
        "    if num_runs > 1:\n",
        "        avg_time = total_time / num_runs\n",
        "        print(f\"\\nAverage time over {num_runs} runs: {avg_time*1000:.2f} ms\")\n",
        "\n",
        "    return generated_outputs[0] if num_runs == 1 else generated_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8bb0f40-d8e0-4a56-e24f-c6b55d31d2aa",
        "id": "lH7cotAxrhO3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run 1:\n",
            "Tokenization time: 3.46 ms\n",
            "Generation time: 3385.14 ms\n",
            "Decoding time: 0.32 ms\n",
            "Total time: 3389.02 ms\n",
            "\n",
            "Run 2:\n",
            "Tokenization time: 0.62 ms\n",
            "Generation time: 1983.51 ms\n",
            "Decoding time: 0.22 ms\n",
            "Total time: 1984.46 ms\n",
            "\n",
            "Average time over 2 runs: 2686.64 ms\n",
            "Generated text: ['Paris is the capital of France and one of the most visited cities in the world. It is a city with a rich history and culture, as well as a vibrant and diverse population. Paris is home to many famous landmarks, including the Eiff', 'Paris is the capital of France and one of the most visited cities in the world. It is a city with a rich history and culture, as well as a vibrant and diverse population. Paris is home to many famous landmarks, including the Eiff']\n"
          ]
        }
      ],
      "source": [
        "# Test the original model\n",
        "prompt = \"Paris is the capital of\"\n",
        "generated = get_output(prompt, num_runs=2)\n",
        "print(f\"Generated text: {generated}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text generation of the original model, as expected, works perfectly and returns a correct and meaningful sentence."
      ],
      "metadata": {
        "id": "mo4IjOYGry0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cpu\")               # actual data moves ↙\n",
        "torch.cuda.empty_cache()      # allocator drops cached blocks"
      ],
      "metadata": {
        "id": "bLN1_gLdt7Rx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjgf4WA_vF3B"
      },
      "source": [
        "# Pruning the Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TQDCkoL6RW3C"
      },
      "outputs": [],
      "source": [
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#from torch.nn import functional as F\n",
        "#from copy import deepcopy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOuKyH0MjyJM"
      },
      "source": [
        "## Execute Pruning."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disclaimer**\n",
        "\n",
        "I'm using a single illustrative prompt so that the code path is easy to follow. In any research or production setting you must feed hundreds or thousands of diverse prompts before deciding which layers to deactivate"
      ],
      "metadata": {
        "id": "wyI5XiqYyBnc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HvhSYyE2Rk1H"
      },
      "outputs": [],
      "source": [
        "# Using multiple prompts for calibration\n",
        "calibration_prompts = [\n",
        "    \"Hi I'm a sample text, used to calculate the cosine difference between input and output.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Machine learning models can be optimized through various techniques, explain the principals.\",\n",
        "    \"2+2=\",\n",
        "    \"What is the meaning of life, the universe, and everything?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SIMPLE AAB CALIBRATION - MINIMAL CODE\n",
        "# =============================================================================\n",
        "def measure_layer_importance_simple(model, tokenizer, prompts):\n",
        "    \"\"\"Simple layer importance measurement - FIXED using original notebook pattern\"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    total_layers = len(model.model.layers)\n",
        "\n",
        "    # Accumulate importance scores across all prompts\n",
        "    importance_acc = {idx: 0.0 for idx in range(total_layers)}\n",
        "\n",
        "    print(f\"📊 Processing {len(prompts)} prompts across {total_layers} layers...\")\n",
        "\n",
        "    for prompt_idx, prompt in enumerate(prompts):\n",
        "        print(f\"   Processing prompt {prompt_idx + 1}/{len(prompts)}\")\n",
        "\n",
        "        # Tokenize input (following original notebook pattern)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Storage for this prompt's layer inputs/outputs\n",
        "        layer_inputs = {}\n",
        "        layer_outputs = {}\n",
        "\n",
        "        # Create hooks (EXACTLY like the original function)\n",
        "        def q_proj_input_hook(layer_idx):\n",
        "            def _hook(module, module_input):\n",
        "                # Handle tuple input (following original pattern)\n",
        "                inp = module_input[0] if isinstance(module_input, tuple) else module_input\n",
        "                layer_inputs[layer_idx] = inp.detach().clone()\n",
        "            return _hook\n",
        "\n",
        "        def o_proj_output_hook(layer_idx):\n",
        "            def _hook(module, module_input, module_output):\n",
        "                # Handle tuple output (following original pattern)\n",
        "                out = module_output[0] if isinstance(module_output, tuple) else module_output\n",
        "                layer_outputs[layer_idx] = out.detach().clone()\n",
        "            return _hook\n",
        "\n",
        "        # Register hooks for ALL layers (not just unpruned ones)\n",
        "        handles = []\n",
        "        for idx in range(total_layers):\n",
        "            layer = model.model.layers[idx]\n",
        "            handles.append(layer.self_attn.q_proj.register_forward_pre_hook(q_proj_input_hook(idx)))\n",
        "            handles.append(layer.self_attn.o_proj.register_forward_hook(o_proj_output_hook(idx)))\n",
        "\n",
        "        # Forward pass (following original pattern)\n",
        "        with torch.no_grad():\n",
        "            _ = model(**inputs)\n",
        "\n",
        "        # Remove hooks (following original pattern)\n",
        "        for h in handles:\n",
        "            h.remove()\n",
        "\n",
        "        # Calculate importance for each layer (EXACTLY like original)\n",
        "        for idx in range(total_layers):\n",
        "            if idx in layer_inputs and idx in layer_outputs:\n",
        "                inp = layer_inputs[idx]\n",
        "                out = layer_outputs[idx]\n",
        "\n",
        "                # Flatten tensors (following original pattern)\n",
        "                inp_flat = inp.view(inp.size(0), -1)\n",
        "                out_flat = out.view(out.size(0), -1)\n",
        "\n",
        "                # Calculate similarity and importance (following original pattern)\n",
        "                similarity = F.cosine_similarity(inp_flat, out_flat, dim=1).mean().item()\n",
        "                importance_score = 1 - similarity\n",
        "                importance_acc[idx] += importance_score\n",
        "\n",
        "    # Average across all prompts\n",
        "    avg_importance = {idx: importance_acc[idx] / len(prompts) for idx in range(total_layers)}\n",
        "\n",
        "    print(\"✅ Layer importance measurement complete!\")\n",
        "    return avg_importance\n",
        "\n",
        "\n",
        "def create_adaptive_config_simple(model, tokenizer, prompts):\n",
        "    \"\"\"Create OPTIMIZED adaptive config - ultra-simple format for efficient inference\"\"\"\n",
        "    print(\"🚀 Creating optimized adaptive config...\")\n",
        "\n",
        "    # Step 1: Analyze model\n",
        "    model_size_category = detect_model_size_category(model)\n",
        "    total_layers = count_attention_layers_correctly(model)\n",
        "    context_window = get_context_window_size(model, tokenizer)\n",
        "\n",
        "    # Step 2: Measure importance\n",
        "    print(\"📊 Measuring layer importance...\")\n",
        "    importance_scores = measure_layer_importance_simple(model, tokenizer, prompts)\n",
        "\n",
        "    # Step 3: Create layers_by_importance (sorted list)\n",
        "    print(\"🏆 Creating layers_by_importance list...\")\n",
        "    sorted_layers = sorted(importance_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    layers_by_importance = [layer_idx for layer_idx, _ in sorted_layers]\n",
        "\n",
        "    # Step 4: Calculate complexity thresholds using existing notebook functions\n",
        "    print(\"🎯 Calculating complexity thresholds...\")\n",
        "    complexity_scores = GLOBAL_COMPLEXITIES\n",
        "    complexity_thresholds = {}\n",
        "\n",
        "    print(\"📊 Using notebook functions to get exact layer counts:\")\n",
        "    for score in complexity_scores:\n",
        "        active_layers_count, _, _, _, _ = calculate_active_layers(\n",
        "            total_layers, model_size_category, score\n",
        "        )\n",
        "        complexity_thresholds[score] = active_layers_count\n",
        "        level_name = classify_complexity_level(score)\n",
        "        print(f\"   Score {score:3.1f} ({level_name:12}) → {active_layers_count:2d}/{total_layers} layers\")\n",
        "\n",
        "    # Step 5: Build OPTIMIZED config\n",
        "    print(\"⚙️ Building optimized configuration...\")\n",
        "    config = {\n",
        "        \"model_info\": {\n",
        "            \"name\": getattr(model.config, '_name_or_path', 'unknown'),\n",
        "            \"total_parameters\": f\"{sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\",\n",
        "            \"size_category\": model_size_category,\n",
        "            \"total_layers\": total_layers,\n",
        "            \"context_window\": context_window,\n",
        "            \"architecture\": type(model).__name__\n",
        "        },\n",
        "        \"layers_by_importance\": layers_by_importance,\n",
        "        \"complexity_thresholds\": complexity_thresholds,\n",
        "        \"complexity_weights\": COMPLEXITY_WEIGHTS\n",
        "    }\n",
        "\n",
        "    # Step 6: Save optimized config\n",
        "    with open(\"adaptive_config.json\", \"w\") as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    print(\"✅ OPTIMIZED adaptive_config.json created!\")\n",
        "\n",
        "    # Show optimized results\n",
        "    print(f\"📊 Model: {total_layers} layers, {model_size_category}\")\n",
        "    print(f\"🏆 Layers by importance: {layers_by_importance[:5]}... (showing first 5)\")\n",
        "    print(\"🎯 Complexity thresholds:\")\n",
        "    for threshold, count in complexity_thresholds.items():\n",
        "        percentage = (count / total_layers) * 100\n",
        "        level = classify_complexity_level(threshold)\n",
        "        print(f\"   {threshold:3.1f} ({level:12}): {count:2d} layers ({percentage:4.1f}%)\")\n",
        "\n",
        "    print(\"\\n🚀 ULTRA-EFFICIENT RUNTIME FORMAT:\")\n",
        "    print(\"   • No redundant data\")\n",
        "    print(\"   • Direct score → layer count lookup\")\n",
        "    print(\"   • Minimal JSON size\")\n",
        "    print(\"   • Fastest possible inference decisions\")\n",
        "\n",
        "    return config\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6yrcseaPSBpb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SIMPLE EXECUTION - OPTIMIZED VERSION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"🚀 CREATING ULTRA-EFFICIENT ADAPTIVE CONFIG\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create the OPTIMIZED adaptive config using existing calibration_prompts\n",
        "adaptive_config = create_adaptive_config_simple(model, tokenizer, calibration_prompts)\n",
        "\n",
        "print(f\"\\n🎉 DONE! Optimized adaptive_config.json ready for AAB!\")\n",
        "print(\"=\" * 50)\n",
        "print(\"📋 What's in your optimized config:\")\n",
        "print(\"   ✅ model_info - Essential model metadata\")\n",
        "print(\"   ✅ layers_by_importance - Ordered list [most → least important]\")\n",
        "print(\"   ✅ complexity_thresholds - Direct score → layer count mapping\")\n",
        "print(\"   ❌ NO redundant data (groups, rankings, ratios)\")\n",
        "print(\"   ❌ NO unnecessary metadata\")\n",
        "print(f\"\\n💡 Runtime efficiency:\")\n",
        "print(\"   • JSON size: ~80% smaller\")\n",
        "print(\"   • Lookup time: O(1) direct access\")\n",
        "print(\"   • Memory usage: Minimal\")\n",
        "print(\"   • Perfect for production inference!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRuB-fZ6VK5H",
        "outputId": "16e5554e-a909-4625-c176-241214ea970e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 CREATING ULTRA-EFFICIENT ADAPTIVE CONFIG\n",
            "==================================================\n",
            "🚀 Creating optimized adaptive config...\n",
            "🔍 Detected model size: 1.24B parameters\n",
            "📊 Measuring layer importance...\n",
            "📊 Processing 5 prompts across 16 layers...\n",
            "   Processing prompt 1/5\n",
            "   Processing prompt 2/5\n",
            "   Processing prompt 3/5\n",
            "   Processing prompt 4/5\n",
            "   Processing prompt 5/5\n",
            "✅ Layer importance measurement complete!\n",
            "🏆 Creating layers_by_importance list...\n",
            "🎯 Calculating complexity thresholds...\n",
            "📊 Using notebook functions to get exact layer counts:\n",
            "   Score 0.1 (trivial     ) → 12/16 layers\n",
            "   Score 0.3 (simple      ) → 13/16 layers\n",
            "   Score 0.5 (medium      ) → 14/16 layers\n",
            "   Score 0.7 (complex     ) → 15/16 layers\n",
            "   Score 0.9 (very_complex) → 16/16 layers\n",
            "⚙️ Building optimized configuration...\n",
            "✅ OPTIMIZED adaptive_config.json created!\n",
            "📊 Model: 16 layers, <2B\n",
            "🏆 Layers by importance: [15, 10, 8, 6, 0]... (showing first 5)\n",
            "🎯 Complexity thresholds:\n",
            "   0.1 (trivial     ): 12 layers (75.0%)\n",
            "   0.3 (simple      ): 13 layers (81.2%)\n",
            "   0.5 (medium      ): 14 layers (87.5%)\n",
            "   0.7 (complex     ): 15 layers (93.8%)\n",
            "   0.9 (very_complex): 16 layers (100.0%)\n",
            "\n",
            "🚀 ULTRA-EFFICIENT RUNTIME FORMAT:\n",
            "   • No redundant data\n",
            "   • Direct score → layer count lookup\n",
            "   • Minimal JSON size\n",
            "   • Fastest possible inference decisions\n",
            "\n",
            "🎉 DONE! Optimized adaptive_config.json ready for AAB!\n",
            "==================================================\n",
            "📋 What's in your optimized config:\n",
            "   ✅ model_info - Essential model metadata\n",
            "   ✅ layers_by_importance - Ordered list [most → least important]\n",
            "   ✅ complexity_thresholds - Direct score → layer count mapping\n",
            "   ❌ NO redundant data (groups, rankings, ratios)\n",
            "   ❌ NO unnecessary metadata\n",
            "\n",
            "💡 Runtime efficiency:\n",
            "   • JSON size: ~80% smaller\n",
            "   • Lookup time: O(1) direct access\n",
            "   • Memory usage: Minimal\n",
            "   • Perfect for production inference!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adaptive_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExBmu0i5AMvx",
        "outputId": "c050fb19-dc2d-4ba6-b76a-e45bcf66a1f5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_info': {'name': 'meta-llama/Llama-3.2-1B',\n",
              "  'total_parameters': '1.24B',\n",
              "  'size_category': '<2B',\n",
              "  'total_layers': 16,\n",
              "  'context_window': 131072,\n",
              "  'architecture': 'LlamaForCausalLM'},\n",
              " 'layers_by_importance': [15,\n",
              "  10,\n",
              "  8,\n",
              "  6,\n",
              "  0,\n",
              "  7,\n",
              "  5,\n",
              "  3,\n",
              "  4,\n",
              "  9,\n",
              "  2,\n",
              "  1,\n",
              "  11,\n",
              "  12,\n",
              "  13,\n",
              "  14],\n",
              " 'complexity_thresholds': {0.1: 12, 0.3: 13, 0.5: 14, 0.7: 15, 0.9: 16},\n",
              " 'complexity_weights': {'token_count': 0.65, 'embedding_variance': 0.35}}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test prompt complexity"
      ],
      "metadata": {
        "id": "X7OMW6WzMCyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using multiple prompts for calibration\n",
        "calibration_prompts = [\n",
        "    # Trivial (0.0-0.2) - Short, simple\n",
        "    \"Hi\",\n",
        "    \"2+2=\",\n",
        "    \"Hello.\",\n",
        "    \"What is 2+2?\",\n",
        "\n",
        "    # Simple (0.2-0.4) - Basic questions\n",
        "    \"What is the capital of France?\",\n",
        "    \"Tell me a joke.\",\n",
        "    \"Name the capital of Catalonia.\",\n",
        "    \"Who wrote 'To Kill a Mockingbird'?\",\n",
        "\n",
        "    # Medium (0.4-0.6) - Knowledge retrieval, moderate length\n",
        "    \"Explain the basic principles of machine learning and how neural networks work.\",\n",
        "    \"What are the main causes of climate change and what can individuals do to help?\",\n",
        "    \"Summarize the plot of 'The Matrix' in one sentence.\",\n",
        "    \"List three benefits of regular exercise.\",\n",
        "\n",
        "    # Complex (0.6-0.8) - Multi-step reasoning, analysis\n",
        "    \"Compare and contrast the economic policies of Keynesian and Austrian schools of thought, analyzing their effectiveness during different historical periods and explaining which approach would be most suitable for addressing current global economic challenges.\",\n",
        "    \"Design a comprehensive strategy for a small tech startup to compete against established giants like Google and Microsoft in the cloud computing market, considering market positioning, technological differentiation, partnerships, and funding requirements.\",\n",
        "    \"Explain why the sky appears blue during the day.\",\n",
        "    \"Describe how a neural network learns from data.\",\n",
        "\n",
        "    # Very Complex (0.8-1.0) - Deep analysis, creativity, long form\n",
        "    \"Write a detailed philosophical essay examining the ethical implications of artificial intelligence consciousness, incorporating perspectives from utilitarian, deontological, and virtue ethics frameworks, while addressing counterarguments and proposing a novel ethical framework for AI development that balances technological progress with human values and societal well-being.\",\n",
        "    \"Develop a multidisciplinary research proposal that integrates quantum computing, biotechnology, and environmental science to address food security challenges in the context of climate change, including methodology, timeline, budget considerations, potential collaborations, risk assessment, and expected societal impact over the next two decades.\"\n",
        "    \"Given current economic trends, predict one challenge global markets may face in the next decade.\",\n",
        "    \"Write a short poem about the experience of learning something new.\",\n",
        "    \"As an expert in global macroeconomics, geopolitical risk assessment, and artificial intelligence ethics, write an in-depth policy advisory report for a coalition of G20 nations facing simultaneous systemic challenges, including post-pandemic inflation volatility, supply chain reconfiguration due to AI-driven automation, increasing regional instability in energy markets, and declining trust in democratic institutions. Your report should propose a coordinated strategy that balances fiscal stimulus with monetary restraint, integrates quantum-secure blockchain for supply chain transparency, and includes AI oversight frameworks aligned with both utilitarian and deontological ethical models. Additionally, evaluate how international institutions like the IMF and the World Bank could modernize their governance structures to reflect multipolar power dynamics, and assess the feasibility of adopting an intergovernmental AI alignment charter inspired by the Paris Agreement model. Your recommendations must be actionable, globally inclusive, and anticipate sociopolitical backlash from both populist and nationalist movements.\",\n",
        "    \"\"\"\n",
        "    Draft Integrated Strategic White-Paper for Inter-Agency Review—\n",
        "\n",
        "Executive Overview:\n",
        "This document synthesises cutting-edge research in climate science, planetary boundaries, quantum-enhanced computation, synthetic bio-manufacturing, neuro-symbolic artificial intelligence, behavioural economics, geopolitics, space-based energy infrastructure, and post-growth macro-finance. It is intended for cabinet-level policymakers across the G20, the African Union, and APEC, as well as multilateral lenders, sovereign wealth funds, philanthropic megadonors, and fourth-sector cooperative alliances.\n",
        "\n",
        "Section 1 – Macroeconomic Volatility & Post-Pandemic Debt Overhang\n",
        "1.1 Analyse the persistence of stagflationary pressures under divergent monetary regimes.\n",
        "1.2 Model cascading default scenarios using agent-based stress tests that incorporate climate-induced supply-chain interruptions, semiconductor chokepoints in Taiwan and the Netherlands, and maritime bottlenecks in the Suez and Panama Canals.\n",
        "1.3 Propose a menu of fiscal-monetary coordination instruments—helicopter stabilisation bonds, biodiversity-linked debt swaps, and anti-fragile carbon border adjustments—scaled to emerging-market liquidity traps.\n",
        "\n",
        "Section 2 – Planetary Health & Regenerative Bio-Economy\n",
        "2.1 Summarise findings from IPCC AR7 draft chapters on irreversible cryosphere tipping points.\n",
        "2.2 Evaluate next-generation direct air capture catalysis that leverages metal-organic frameworks seeded by engineered extremophilic microbes.\n",
        "2.3 Draft a governance blueprint for a Global Soil Microbiome Commons, incorporating indigenous data sovereignty protocols, fair-benefit-sharing algorithms, and quantum-secured telemetry for real-time biodiversity crediting.\n",
        "\n",
        "Section 3 – Quantum-Classical Hybrid Infrastructure\n",
        "3.1 Detail a phased roadmap for 1 000-qubit photonic processors coupled to error-mitigated superconducting qubits for combinatorial optimisation in logistics, drug-discovery, and lattice-QCD.\n",
        "3.2 Define open-standard interfaces that allow sovereign cloud providers to interoperate with NATO-grade zero-trust enclaves and NIST-post-quantum cryptographic suites.\n",
        "3.3 Recommend incentives for talent-mobility corridors bridging quantum start-up clusters in Toronto, Delft, Shenzhen, Sydney, and Kigali.\n",
        "\n",
        "Section 4 – Neuro-Symbolic AI & Alignment Governance\n",
        "4.1 Compare scaling-law extrapolations for transformers, mixture-of-experts, retrieval-augmented decoders, and recursive reasoning agents.\n",
        "4.2 Propose a multi-layer safety stack: interpretability probes, causal influence diagrams, counterfactual policy evaluation, and cooperative inverse-reinforcement architectures monitored by open-weight red-team sandboxes.\n",
        "4.3 Outline a treaty-grade AI Alignment Accord modelled after the Paris Agreement, featuring dynamic capability thresholds, compute-cluster registration, differential privacy audits, and a tiered sanctions regime enforced via programmable CBDCs.\n",
        "\n",
        "Section 5 – Security, Geopolitics & Space-Based Energy\n",
        "5.1 Assess escalation risks stemming from fractional-orbital bombardment systems, low-cost hypersonic glide vehicles, and AI-directed drone swarms.\n",
        "5.2 Present techno-economic viability of kilometre-scale solar power satellites in sun-synchronous orbit, with microwave beaming arrays utilising adaptive phased-conjugate mirrors.\n",
        "5.3 Recommend confidence-building measures: reciprocal on-site inspection, open telemetry APIs, catastrophe-bond insurance pools, and an International Orbital Commons Authority.\n",
        "\n",
        "Section 6 – Behavioural & Cultural Dynamics\n",
        "6.1 Integrate behavioural-nudge frameworks, narrative foresight, and social-network epistemic resilience analytics to counter disinformation loops.\n",
        "6.2 Design outcome-oriented citizen deliberation platforms that leverage quadratic voting, verifiable credentials, and language-agnostic dialogue agents with embedded bias-mitigation layers.\n",
        "\n",
        "Section 7 – Financing Mechanisms & Implementation Timeline\n",
        "7.1 Catalogue blended-finance instruments: catalytic first-loss capital, sovereign green sukuk, resilience impact derivatives, and decentralized autonomous project bonds.\n",
        "7.2 Map a ten-year Gantt chart with critical path analysis, specifying TRL-milestones, regulatory sandboxes, and adaptive procurement clauses.\n",
        "\n",
        "Call to Action:\n",
        "Conclude by articulating how cooperative mission-oriented investment, science-diplomacy trust architecture, and inclusive technology governance can converge to safeguard planetary health while enabling equitable prosperity within the safe-and-just operating space for humanity.\n",
        "    \"\"\",\n",
        "]"
      ],
      "metadata": {
        "id": "aHCrf_ivPK5A"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def sigmoid_scaling(n_tokens, context_window):\n",
        "    return 1 / (1 + math.exp(-10 * (n_tokens / context_window - 0.5)))\n",
        "\n",
        "def exponential_scaling(n_tokens, context_window):\n",
        "    return (math.exp(n_tokens / context_window) - 1) / (math.e - 1)\n",
        "\n",
        "def sqrt_scaling(n_tokens, context_window):\n",
        "    return math.sqrt(n_tokens / context_window)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9qgRLW1SZtc3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_prompt_complexity(prompts, config, model, tokenizer, verbose: bool = True):\n",
        "    \"\"\"\n",
        "    Compute a complexity score in [0, 1] for each prompt.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prompts : list[str]\n",
        "        The text prompts to score.\n",
        "    config : dict\n",
        "        adaptive_config.json already loaded as dict.\n",
        "    model : transformers.PreTrainedModel\n",
        "        The HF model (on CPU or GPU).\n",
        "    tokenizer : transformers.PreTrainedTokenizer\n",
        "        Matching tokenizer.\n",
        "    verbose : bool\n",
        "        If True, print a per-prompt breakdown.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list[tuple[str, float]]\n",
        "        (prompt, complexity_score) for each input string.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- model-specific constants ----------------------------------\n",
        "    context_window = config[\"model_info\"].get(\"context_window\", 2048)\n",
        "    hidden_size    = model.config.hidden_size\n",
        "\n",
        "    # --- weights (fallback to defaults if not in config) -----------\n",
        "    weights = config.get(\"complexity_weights\", {\n",
        "    \"token_count\": 0.65,\n",
        "    \"embedding_variance\": 0.35\n",
        "})\n",
        "\n",
        "    results = []\n",
        "    device  = next(model.parameters()).device\n",
        "\n",
        "    for p in prompts:\n",
        "        # --- Tokenise on the model's device ------------------------\n",
        "        ids = tokenizer(p, return_tensors=\"pt\")[\"input_ids\"][0].to(device)\n",
        "        n_tokens = ids.size(0)\n",
        "\n",
        "        # A) LENGTH  — log-scaled so it grows smoothly up to ctx-window\n",
        "        k = 32                        # start here\n",
        "        floor_ctx = 2048             # never go below this\n",
        "        effective_ctx = max(context_window // k, floor_ctx)\n",
        "        token_score = math.log1p(n_tokens) / math.log1p(effective_ctx)\n",
        "        token_score = min(token_score, 1.0)\n",
        "        #token_score = math.log1p(n_tokens) / math.log1p(context_window)\n",
        "        #token_score = sqrt_scaling(n_tokens, context_window)\n",
        "\n",
        "        # C) EMBEDDING VARIANCE  — std/√d clamped to 1\n",
        "        #with torch.no_grad():\n",
        "        #    emb = model.get_input_embeddings()(ids.unsqueeze(0)).squeeze(0)\n",
        "        #    emb = torch.nn.functional.normalize(emb, p=2, dim=1)\n",
        "        #    cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
        "        #    emb_var = 1 - torch.mean(cosine_sim(emb.unsqueeze(1), emb.unsqueeze(0)))\n",
        "\n",
        "        #emb_var_norm = min(float(emb_var), 1.0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          emb = model.get_input_embeddings()(ids.unsqueeze(0)).squeeze(0).float()\n",
        "          n = emb.size(0)\n",
        "          if n < 3:                       # 1- or 2-token prompt → no diversity\n",
        "              emb_var_norm = 0.0\n",
        "          else:\n",
        "              # 1.  ℓ2-normalise each embedding vector\n",
        "              norm_emb = torch.nn.functional.normalize(emb, p=2, dim=1)    # (n, d)\n",
        "\n",
        "              # 2.  Full cosine-similarity matrix\n",
        "              sim = torch.matmul(norm_emb, norm_emb.t())                   # (n, n)\n",
        "\n",
        "              # 3.  Remove self-similarities (diagonal) and compute mean\n",
        "              off_diag = sim[~torch.eye(n, dtype=bool, device=sim.device)]  # (n²-n,)\n",
        "              base_var = 1.0 - off_diag.mean().item()   # 0 … 1   (0 = identical, 1 = orthogonal)\n",
        "\n",
        "              # 4.  Length factor: 0 → 1 across full context window\n",
        "              len_fac = math.log1p(n) / math.log1p(context_window)         # 0 … 1\n",
        "\n",
        "              # 5.  Combine & clamp\n",
        "              emb_var_norm = min(base_var * len_fac, 1.0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # --- Weighted combination ---------------------------------\n",
        "        score = (\n",
        "            weights[\"token_count\"]        * token_score   +\n",
        "            weights[\"embedding_variance\"] * emb_var_norm\n",
        "        )\n",
        "        score = max(0.0, min(score, 1.0))  # clamp for safety\n",
        "\n",
        "        if verbose:\n",
        "            head = (p[:57] + \"…\") if len(p) > 60 else p\n",
        "            print(f\"{head:<60} | \"\n",
        "                  f\"score={score:.3f}  \"\n",
        "                  f\"| length={n_tokens} \"\n",
        "                  f\"[tok {token_score:.3f}  var {emb_var_norm:.3f}]\")\n",
        "\n",
        "        results.append((p, round(score, 4)))\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "DF9Xw_oJMIM8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_prompt_complexity(calibration_prompts, adaptive_config, model,  tokenizer)"
      ],
      "metadata": {
        "id": "f8_F7kJqOVbK",
        "outputId": "0be7d624-6fb0-4909-e5ba-45cf93bf9306",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi                                                           | score=0.086  | length=2 [tok 0.132  var 0.000]\n",
            "2+2=                                                         | score=0.180  | length=5 [tok 0.215  var 0.115]\n",
            "Hello.                                                       | score=0.151  | length=3 [tok 0.167  var 0.121]\n",
            "What is 2+2?                                                 | score=0.225  | length=8 [tok 0.264  var 0.153]\n",
            "What is the capital of France?                               | score=0.232  | length=8 [tok 0.264  var 0.172]\n",
            "Tell me a joke.                                              | score=0.209  | length=6 [tok 0.234  var 0.163]\n",
            "Name the capital of Catalonia.                               | score=0.222  | length=7 [tok 0.250  var 0.171]\n",
            "Who wrote 'To Kill a Mockingbird'?                           | score=0.267  | length=11 [tok 0.299  var 0.208]\n",
            "Explain the basic principles of machine learning and how …   | score=0.295  | length=15 [tok 0.333  var 0.225]\n",
            "What are the main causes of climate change and what can i…   | score=0.304  | length=17 [tok 0.347  var 0.222]\n",
            "Summarize the plot of 'The Matrix' in one sentence.          | score=0.294  | length=15 [tok 0.333  var 0.221]\n",
            "List three benefits of regular exercise.                     | score=0.235  | length=8 [tok 0.264  var 0.181]\n",
            "Compare and contrast the economic policies of Keynesian a…   | score=0.389  | length=38 [tok 0.440  var 0.294]\n",
            "Design a comprehensive strategy for a small tech startup …   | score=0.388  | length=38 [tok 0.440  var 0.291]\n",
            "Explain why the sky appears blue during the day.             | score=0.273  | length=12 [tok 0.308  var 0.208]\n",
            "Describe how a neural network learns from data.              | score=0.256  | length=10 [tok 0.288  var 0.197]\n",
            "Write a detailed philosophical essay examining the ethica…   | score=0.429  | length=55 [tok 0.484  var 0.326]\n",
            "Develop a multidisciplinary research proposal that integr…   | score=0.455  | length=72 [tok 0.516  var 0.342]\n",
            "Write a short poem about the experience of learning somet…   | score=0.279  | length=13 [tok 0.317  var 0.209]\n",
            "As an expert in global macroeconomics, geopolitical risk …   | score=0.554  | length=180 [tok 0.625  var 0.421]\n",
            "\n",
            "    Draft Integrated Strategic White-Paper for Inter-Age…   | score=0.727  | length=904 [tok 0.818  var 0.559]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Hi', 0.0858),\n",
              " ('2+2=', 0.1804),\n",
              " ('Hello.', 0.1509),\n",
              " ('What is 2+2?', 0.2252),\n",
              " ('What is the capital of France?', 0.2321),\n",
              " ('Tell me a joke.', 0.2089),\n",
              " ('Name the capital of Catalonia.', 0.2223),\n",
              " (\"Who wrote 'To Kill a Mockingbird'?\", 0.2669),\n",
              " ('Explain the basic principles of machine learning and how neural networks work.',\n",
              "  0.2954),\n",
              " ('What are the main causes of climate change and what can individuals do to help?',\n",
              "  0.3035),\n",
              " (\"Summarize the plot of 'The Matrix' in one sentence.\", 0.2939),\n",
              " ('List three benefits of regular exercise.', 0.235),\n",
              " ('Compare and contrast the economic policies of Keynesian and Austrian schools of thought, analyzing their effectiveness during different historical periods and explaining which approach would be most suitable for addressing current global economic challenges.',\n",
              "  0.3892),\n",
              " ('Design a comprehensive strategy for a small tech startup to compete against established giants like Google and Microsoft in the cloud computing market, considering market positioning, technological differentiation, partnerships, and funding requirements.',\n",
              "  0.3881),\n",
              " ('Explain why the sky appears blue during the day.', 0.2732),\n",
              " ('Describe how a neural network learns from data.', 0.2563),\n",
              " ('Write a detailed philosophical essay examining the ethical implications of artificial intelligence consciousness, incorporating perspectives from utilitarian, deontological, and virtue ethics frameworks, while addressing counterarguments and proposing a novel ethical framework for AI development that balances technological progress with human values and societal well-being.',\n",
              "  0.4286),\n",
              " ('Develop a multidisciplinary research proposal that integrates quantum computing, biotechnology, and environmental science to address food security challenges in the context of climate change, including methodology, timeline, budget considerations, potential collaborations, risk assessment, and expected societal impact over the next two decades.Given current economic trends, predict one challenge global markets may face in the next decade.',\n",
              "  0.4548),\n",
              " ('Write a short poem about the experience of learning something new.',\n",
              "  0.2793),\n",
              " ('As an expert in global macroeconomics, geopolitical risk assessment, and artificial intelligence ethics, write an in-depth policy advisory report for a coalition of G20 nations facing simultaneous systemic challenges, including post-pandemic inflation volatility, supply chain reconfiguration due to AI-driven automation, increasing regional instability in energy markets, and declining trust in democratic institutions. Your report should propose a coordinated strategy that balances fiscal stimulus with monetary restraint, integrates quantum-secure blockchain for supply chain transparency, and includes AI oversight frameworks aligned with both utilitarian and deontological ethical models. Additionally, evaluate how international institutions like the IMF and the World Bank could modernize their governance structures to reflect multipolar power dynamics, and assess the feasibility of adopting an intergovernmental AI alignment charter inspired by the Paris Agreement model. Your recommendations must be actionable, globally inclusive, and anticipate sociopolitical backlash from both populist and nationalist movements.',\n",
              "  0.5535),\n",
              " ('\\n    Draft Integrated Strategic White-Paper for Inter-Agency Review—\\n\\nExecutive Overview:\\nThis document synthesises cutting-edge research in climate science, planetary boundaries, quantum-enhanced computation, synthetic bio-manufacturing, neuro-symbolic artificial intelligence, behavioural economics, geopolitics, space-based energy infrastructure, and post-growth macro-finance. It is intended for cabinet-level policymakers across the G20, the African Union, and APEC, as well as multilateral lenders, sovereign wealth funds, philanthropic megadonors, and fourth-sector cooperative alliances.\\n\\nSection 1 – Macroeconomic Volatility & Post-Pandemic Debt Overhang\\n1.1\\u2003Analyse the persistence of stagflationary pressures under divergent monetary regimes.\\n1.2\\u2003Model cascading default scenarios using agent-based stress tests that incorporate climate-induced supply-chain interruptions, semiconductor chokepoints in Taiwan and the Netherlands, and maritime bottlenecks in the Suez and Panama Canals.\\n1.3\\u2003Propose a menu of fiscal-monetary coordination instruments—helicopter stabilisation bonds, biodiversity-linked debt swaps, and anti-fragile carbon border adjustments—scaled to emerging-market liquidity traps.\\n\\nSection 2 – Planetary Health & Regenerative Bio-Economy\\n2.1\\u2003Summarise findings from IPCC AR7 draft chapters on irreversible cryosphere tipping points.\\n2.2\\u2003Evaluate next-generation direct air capture catalysis that leverages metal-organic frameworks seeded by engineered extremophilic microbes.\\n2.3\\u2003Draft a governance blueprint for a Global Soil Microbiome Commons, incorporating indigenous data sovereignty protocols, fair-benefit-sharing algorithms, and quantum-secured telemetry for real-time biodiversity crediting.\\n\\nSection 3 – Quantum-Classical Hybrid Infrastructure\\n3.1\\u2003Detail a phased roadmap for 1 000-qubit photonic processors coupled to error-mitigated superconducting qubits for combinatorial optimisation in logistics, drug-discovery, and lattice-QCD.\\n3.2\\u2003Define open-standard interfaces that allow sovereign cloud providers to interoperate with NATO-grade zero-trust enclaves and NIST-post-quantum cryptographic suites.\\n3.3\\u2003Recommend incentives for talent-mobility corridors bridging quantum start-up clusters in Toronto, Delft, Shenzhen, Sydney, and Kigali.\\n\\nSection 4 – Neuro-Symbolic AI & Alignment Governance\\n4.1\\u2003Compare scaling-law extrapolations for transformers, mixture-of-experts, retrieval-augmented decoders, and recursive reasoning agents.\\n4.2\\u2003Propose a multi-layer safety stack: interpretability probes, causal influence diagrams, counterfactual policy evaluation, and cooperative inverse-reinforcement architectures monitored by open-weight red-team sandboxes.\\n4.3\\u2003Outline a treaty-grade AI Alignment Accord modelled after the Paris Agreement, featuring dynamic capability thresholds, compute-cluster registration, differential privacy audits, and a tiered sanctions regime enforced via programmable CBDCs.\\n\\nSection 5 – Security, Geopolitics & Space-Based Energy\\n5.1\\u2003Assess escalation risks stemming from fractional-orbital bombardment systems, low-cost hypersonic glide vehicles, and AI-directed drone swarms.\\n5.2\\u2003Present techno-economic viability of kilometre-scale solar power satellites in sun-synchronous orbit, with microwave beaming arrays utilising adaptive phased-conjugate mirrors.\\n5.3\\u2003Recommend confidence-building measures: reciprocal on-site inspection, open telemetry APIs, catastrophe-bond insurance pools, and an International Orbital Commons Authority.\\n\\nSection 6 – Behavioural & Cultural Dynamics\\n6.1\\u2003Integrate behavioural-nudge frameworks, narrative foresight, and social-network epistemic resilience analytics to counter disinformation loops.\\n6.2\\u2003Design outcome-oriented citizen deliberation platforms that leverage quadratic voting, verifiable credentials, and language-agnostic dialogue agents with embedded bias-mitigation layers.\\n\\nSection 7 – Financing Mechanisms & Implementation Timeline\\n7.1\\u2003Catalogue blended-finance instruments: catalytic first-loss capital, sovereign green sukuk, resilience impact derivatives, and decentralized autonomous project bonds.\\n7.2\\u2003Map a ten-year Gantt chart with critical path analysis, specifying TRL-milestones, regulatory sandboxes, and adaptive procurement clauses.\\n\\nCall to Action:\\nConclude by articulating how cooperative mission-oriented investment, science-diplomacy trust architecture, and inclusive technology governance can converge to safeguard planetary health while enabling equitable prosperity within the safe-and-just operating space for humanity.\\n    ',\n",
              "  0.7275)]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46_tIN1brKZW"
      },
      "source": [
        "# Test Pruned Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFdCesVcnnrM"
      },
      "source": [
        "Now, let's test the pruned model, which is a Llama-3.2-3B model where I have marked 4 Attention layers to be bypassed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oCJ8EYLhJg-",
        "outputId": "8ecaa3b1-5d72-44ba-b033-d82d70eb097f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run 1:\n",
            "Tokenization time: 0.46 ms\n",
            "Generation time: 1251.89 ms\n",
            "Decoding time: 0.23 ms\n",
            "Total time: 1252.68 ms\n",
            "\n",
            "Run 2:\n",
            "Tokenization time: 0.59 ms\n",
            "Generation time: 1245.58 ms\n",
            "Decoding time: 0.21 ms\n",
            "Total time: 1246.48 ms\n",
            "\n",
            "Average time over 2 runs: 1249.48 ms\n",
            "Generated text: ['Paris is the capital of France and/or world-wide fame for its beautiful skyline skyline-top-top-bottom-bottomside-side-side sidesidednessnessNESSNESSnessinessinessnessesivenessivenessfulnessnessfulnessfulnessinessesenessesinesssinessESness', 'Paris is the capital of France and/or world-wide fame for its beautiful skyline skyline-top-top-bottom-bottomside-side-side sidesidednessnessNESSNESSnessinessinessnessesivenessivenessfulnessnessfulnessfulnessinessesenessesinesssinessESness']\n"
          ]
        }
      ],
      "source": [
        "# Test the pruned model\n",
        "pruned_model = pruned_model.to(device) #Move the model to GPU again.\n",
        "generated = get_output(prompt, pruned_model, num_runs=2)\n",
        "print(f\"Generated text: {generated}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ic4ux7goGu_"
      },
      "source": [
        "\n",
        "The execution of this second model is slightly faster than that of the base model, and the generated text is fairly accurate, although some repetition can be noticed towards the end of the sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Store the Model.\n"
      ],
      "metadata": {
        "id": "H7R_VaIPTQhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_model_name = 'attnprun-llama-3.2-3B'\n",
        "output_dir = './'+new_model_name\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "pruned_model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "#new_config.save_pretrained(output_dir)\n",
        "print(f\"Pruned model saved to {output_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERXYQsy5TUtn",
        "outputId": "50e669d9-04ed-40bd-835e-463ed60a3d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned model saved to ./attnprun-llama-3.2-3B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Check that config contains layers to skip\n",
        "from transformers import AutoConfig\n",
        "config = AutoConfig.from_pretrained(output_dir)\n",
        "\n",
        "if hasattr(config, \"drop_attn_list\"):\n",
        "    print(f\"drop_attn_list stored: {config.drop_attn_list}\")\n",
        "else:\n",
        "    print(\"drop_attn_list isn't present.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdE7o0PETggr",
        "outputId": "66718550-f72e-47d2-b2d3-a96d1cb04c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drop_attn_list stored: [14, 13, 12, 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload to Hugging Face."
      ],
      "metadata": {
        "id": "hG63t8jqVdOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El proceso de subida de este modelo a Hugging es ligeramente más complejo por que se debe almacenar no tan solo el modelo en si, sino tambien el código de la función _bypass_single_layer. Que como recordarás es la función que se encarga de decidir cuando ejecutar o simplemente bypasear una capa de atención.  "
      ],
      "metadata": {
        "id": "YRDoj5bYKWVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, upload_folder, whoami"
      ],
      "metadata": {
        "id": "0bi3zX7FVjwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Get your HF username from the current token\n",
        "username = whoami()[\"name\"]  # Returns a dict like {'name': 'your_username', 'email': ...}\n",
        "username"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_AnSaJPuWffg",
        "outputId": "01f84a9a-6d3c-469f-9cf2-67225de8a998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'oopere'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define repo name\n",
        "repo_id = f\"{username}/{new_model_name}\""
      ],
      "metadata": {
        "id": "KDuY1p8zWjax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define path to your model\n",
        "output_dir = \"./\"+new_model_name\n"
      ],
      "metadata": {
        "id": "-MNgDvImWmFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function must be saved in a .py file, but since this notebook runs on Colab, I’ve decided the best approach is to create a cell that generates the file to be uploaded.\n",
        "\n",
        "The file contains the custom class PrunedLlamaForCausalLM, which extends Hugging Face’s LlamaForCausalLM.\n",
        "\n",
        "This custom class calls the base constructor, ensuring that the model's configuration file includes the drop_attn_list, which specifies the layers that should be skipped.\n",
        "\n",
        "The forward function is modified only for the layers that need to be skipped; the rest continue executing their standard forward function.\n"
      ],
      "metadata": {
        "id": "dJSb5z2nLRS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_model_code = '''\n",
        "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
        "\n",
        "class PrunedLlamaForCausalLM(LlamaForCausalLM):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        if not hasattr(config, \"drop_attn_list\"):\n",
        "            config.drop_attn_list = []\n",
        "\n",
        "        for idx in config.drop_attn_list:\n",
        "            self._bypass_single_layer(idx)\n",
        "\n",
        "    def _bypass_single_layer(self, layer_idx):\n",
        "        \"\"\"\n",
        "        Modifies the specified layer's forward method so that attention is bypassed.\n",
        "        \"\"\"\n",
        "        layer = self.model.layers[layer_idx]\n",
        "        if not hasattr(layer.self_attn, \"_original_forward\"):\n",
        "            layer.self_attn._original_forward = layer.self_attn.forward\n",
        "\n",
        "        def new_attention_forward(self, hidden_states, attention_mask=None, position_ids=None,\n",
        "                                  past_key_value=None, output_attentions=False, use_cache=False,\n",
        "                                  **kwargs):\n",
        "            if getattr(self, \"layer_idx\", -1) in self.config.drop_attn_list:\n",
        "                if use_cache:\n",
        "                    return hidden_states, None\n",
        "                else:\n",
        "                    return hidden_states, None\n",
        "            return self._original_forward(hidden_states, attention_mask, position_ids,\n",
        "                                          past_key_value, output_attentions, use_cache, **kwargs)\n",
        "\n",
        "        layer.self_attn.layer_idx = layer_idx\n",
        "        layer.self_attn.forward = new_attention_forward.__get__(layer.self_attn, type(layer.self_attn))\n",
        "\n",
        "'''\n",
        "\n",
        "# Define path and write the file\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "with open(os.path.join(output_dir, \"modeling_attnprun_llama.py\"), \"w\") as f:\n",
        "    f.write(custom_model_code.strip())\n",
        "\n",
        "print(\"Custom model script modeling_attnprun_llama.py created successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_xaG0FDXojc",
        "outputId": "f79b2cf2-59fb-48c6-fece-b2410f91b7cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom model script modeling_attnprun_llama.py created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the model's configuration file is updated by adding the `auto_map` field, which tells the Transformers library which class to use to construct the model: `modeling_attnprun_llama.PrunedLlamaForCausalLM.`\n"
      ],
      "metadata": {
        "id": "r59Yae3jVMpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Path to the config file\n",
        "config_path = os.path.join(output_dir, \"config.json\")\n",
        "\n",
        "# Load the existing config\n",
        "with open(config_path, \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Add or update the auto_map section\n",
        "config[\"auto_map\"] = {\n",
        "    \"AutoModelForCausalLM\": \"modeling_attnprun_llama.PrunedLlamaForCausalLM\"\n",
        "}\n",
        "\n",
        "# Optional: ensure the architecture field is aligned\n",
        "config[\"architectures\"] = [\"PrunedLlamaForCausalLM\"]\n",
        "\n",
        "# Save the updated config\n",
        "with open(config_path, \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"config.json updated with auto_map and architecture.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBbxbBbmbeRl",
        "outputId": "1105e204-17e1-4cd5-b93e-733cad7b00ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json updated with auto_map and architecture.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time to upload the folder containing the weights, the config file and the new function to HF."
      ],
      "metadata": {
        "id": "0ciXxElsWsQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Upload the folder to the Hub\n",
        "upload_folder(\n",
        "    folder_path=output_dir,\n",
        "    path_in_repo=\"\",  # Upload everything to root\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "print(f\"Model uploaded successfully to https://huggingface.co/{repo_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCUToiTmWp9M",
        "outputId": "e0c99d01-b250-471e-91ef-39959ab662a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model uploaded successfully to https://huggingface.co/oopere/attnprun-llama-3.2-3B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download model from Hugging Face."
      ],
      "metadata": {
        "id": "8Lp3bWMOehLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "del pruned_model\n",
        "del tokenizer\n",
        "del model\n",
        "\n",
        "# 2. Libera la caché de la GPU\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()  # Opcional, ayuda en Colab\n",
        "\n",
        "# 3. Forza recolección de basura en Python\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2eDLdnEmapS",
        "outputId": "66087f86-ec21-4fa5-a240-a4d78ff549e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "186"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is downloaded normally from Hugging Face, but you must remember to set `trust_remote_code=True` since the model includes the custom code you previously created and uploaded.\n"
      ],
      "metadata": {
        "id": "UgHQ5-2MW6q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_model_name=\"oopere/attnprun-llama-3.2-3B\"\n",
        "\n",
        "model_hf = AutoModelForCausalLM.from_pretrained(\n",
        "    pruned_model_name,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pruned_model_name)"
      ],
      "metadata": {
        "id": "d4AB09pNemii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_hf = model_hf.to(device) #Move the model to GPU again.\n",
        "generated = get_output(prompt, model_hf, num_runs=2)\n",
        "print(f\"Generated text: {generated}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2feKDCrPll0E",
        "outputId": "79f8010b-6fed-4460-c2ea-5442036f7a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run 1:\n",
            "Tokenization time: 0.77 ms\n",
            "Generation time: 1245.88 ms\n",
            "Decoding time: 0.23 ms\n",
            "Total time: 1246.97 ms\n",
            "\n",
            "Run 2:\n",
            "Tokenization time: 0.49 ms\n",
            "Generation time: 1239.63 ms\n",
            "Decoding time: 0.19 ms\n",
            "Total time: 1240.41 ms\n",
            "\n",
            "Average time over 2 runs: 1243.60 ms\n",
            "Generated text: ['Paris is the capital of France and/or world-wide fame for its beautiful skyline skyline-top-top-bottom-bottomside-side-side sidesidednessnessNESSNESSnessinessinessnessesivenessivenessfulnessnessfulnessfulnessinessesenessesinesssinessESness', 'Paris is the capital of France and/or world-wide fame for its beautiful skyline skyline-top-top-bottom-bottomside-side-side sidesidednessnessNESSNESSnessinessinessnessesivenessivenessfulnessnessfulnessfulnessinessesenessesinesssinessESness']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CsaXViPtmft"
      },
      "source": [
        "# Conclusion.\n",
        "Based on the findings in the paper and the results obtained, I believe this type of pruning may work better with larger models where attention layers tend to have redundancy.\n",
        "\n",
        "Since this type of pruning does not alter the model's structure, it does not result in a reduction in its size or the memory required to load it. The main advantage of using this pruning approach is the reduction of computational load during inference, leading to a more efficient model with faster responses and lower resource consumption.\n",
        "\n",
        "Unlike the original paper, which describes \"removing\" selected attention layers but provides limited implementation details, this implementation takes a transparent functional approach by explicitly overriding the `forward` method only in the specified layers. As a result, the model retains its full architecture and parameter set, but selectively skips computations at runtime. This makes the method reversible, modular, and fully compatible with the Hugging Face ecosystem using `trust_remote_code=True`. While both approaches achieve similar computational savings, this one emphasizes clarity, portability, and practical integration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XDtwh-dwMHh"
      },
      "source": [
        "# Authors Note.\n",
        "\n",
        "In addition to creating content like this notebook and offering it under the MIT license, I have also contributed to repositories such as those of Hugging Face and Google Gemini.\n",
        "\n",
        "I am especially proud of my book: [Large Language Models: Apply and Implement Strategies for Large Language Models (Apress)(https://amzn.to/3DSepLb).\n",
        "\n",
        "You can find it on both [Amazon](https://amzn.to/3DSepLb) and [Springer](https://link.springer.com/book/10.1007/979-8-8688-0515-8), where they often have good deals on the purchase price.\n",
        "\n",
        "If you take a look and end up purchasing it, keep in mind that you can reach out with any questions via the Discussions section of this same repository or on any of my social media channels. I’ll do my best to respond as quickly as possible."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPaFuIyw/FU+aroiDNa0E/I",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}