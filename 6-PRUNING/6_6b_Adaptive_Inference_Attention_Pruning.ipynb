{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Large-Language-Model-Notebooks-Course/blob/inference-adaptative-attention-pruning/6-PRUNING/6_6b_Adaptive_Inference_Attention_Pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV5KPbfseL8S"
      },
      "source": [
        "<div>\n",
        "    <h1>Large Language Models Projects</a></h1>\n",
        "    <h3>Apply and Implement Strategies for Large Language Models</h3>\n",
        "    <h2>Pruning Attention Layers</h2>\n",
        "    <h3>Not All Attention is needed</h3>\n",
        "</div>\n",
        "\n",
        "by [Pere Martra](https://www.linkedin.com/in/pere-martra/)\n",
        "\n",
        "_______\n",
        "Models: meta-llama/Llama-3.2\n",
        "\n",
        "Colab Environment: GPU L4 for 3B Models\n",
        "\n",
        "T4 for 1B Model.\n",
        "\n",
        "Keys:\n",
        "* Pruning\n",
        "* Attention\n",
        "\n",
        "References:\n",
        "* [Resource-Efficient Transformer Pruning for Finetuning of Large Models](https://openaccess.thecvf.com/content/CVPR2024/html/Ilhan_Resource-Efficient_Transformer_Pruning_for_Finetuning_of_Large_Models_CVPR_2024_paper.html)\n",
        "\n",
        "_______\n",
        "**disclaimer: The pruning / knowledge distillation section has been created after the first edition of the book was published. They are not included in the book’s original content but are intended to supplement and expand on the topics covered.**\n",
        "\n",
        "This is the unofficial repository for the book:\n",
        "        <a href=\"https://amzn.to/4eanT1g\"> <b>Large Language Models:</b> Apply and Implement Strategies for Large Language Models</a> (Apress).\n",
        "        The book is based on the content of this repository, but the notebooks are being updated, and I am incorporating new examples and chapters.\n",
        "        If you are looking for the official repository for the book, with the original notebooks, you should visit the\n",
        "        <a href=\"https://github.com/Apress/Large-Language-Models-Projects\">Apress repository</a>, where you can find all the notebooks in their original format as they appear in the book.\n",
        "\n",
        "______\n",
        "# Introduction\n",
        "\n",
        "\n",
        "# Methodology.\n",
        "\n",
        "______"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwYeKwswnkTG"
      },
      "source": [
        "# Install libraries & Configure variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PblPrYCiYTl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "633d1c58-29bc-48bd-f058-3d0d2b1ba027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m126.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hf_xet\n",
            "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
            "Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf_xet\n",
            "Successfully installed hf_xet-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch==2.6.0\n",
        "!pip install -q torchvision==0.21.0\n",
        "!pip install -q transformers==4.51.3\n",
        "!pip install -q datasets==3.6.0\n",
        "!pip install -q lm-eval==0.4.8\n",
        "\n",
        "!pip install hf_xet #To speed up downloads from HF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6qN0mu6IHqpy"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)"
      ],
      "metadata": {
        "id": "1k0hnzPg302j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptative Configuration."
      ],
      "metadata": {
        "id": "hYdRIDoV8R3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ADAPTIVE ATTENTION BYPASS (AAB) CONFIGURATION - CORRECTED SCALING TO 100%\n",
        "# =============================================================================\n",
        "\n",
        "GLOBAL_COMPLEXITIES = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "COMPLEXITY_WEIGHTS = {\n",
        "    \"token_count\": 0.75,\n",
        "    \"embedding_variance\": 0.25\n",
        "}\n",
        "\n",
        "ADAPTIVE_CONFIG = {\n",
        "    # Model size-based ratios with proportional scaling to 100%\n",
        "    \"model_size_ratios\": {\n",
        "        \"70B+\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.15, \"scaling_factor\": 0.85},\n",
        "            \"simple\": {\"min_ratio\": 0.35, \"scaling_factor\": 0.65},\n",
        "            \"medium\": {\"min_ratio\": 0.55, \"scaling_factor\": 0.45},\n",
        "            \"complex\": {\"min_ratio\": 0.75, \"scaling_factor\": 0.25},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"30B-70B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.25, \"scaling_factor\": 0.75},\n",
        "            \"simple\": {\"min_ratio\": 0.40, \"scaling_factor\": 0.60},\n",
        "            \"medium\": {\"min_ratio\": 0.60, \"scaling_factor\": 0.40},\n",
        "            \"complex\": {\"min_ratio\": 0.80, \"scaling_factor\": 0.20},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"10B-30B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.30, \"scaling_factor\": 0.75},\n",
        "            \"simple\": {\"min_ratio\": 0.45, \"scaling_factor\": 0.55},\n",
        "            \"medium\": {\"min_ratio\": 0.65, \"scaling_factor\": 0.35},\n",
        "            \"complex\": {\"min_ratio\": 0.82, \"scaling_factor\": 0.18},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"5B-10B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.45, \"scaling_factor\": 0.60},\n",
        "            \"simple\": {\"min_ratio\": 0.55, \"scaling_factor\": 0.45},\n",
        "            \"medium\": {\"min_ratio\": 0.75, \"scaling_factor\": 0.25},\n",
        "            \"complex\": {\"min_ratio\": 0.87, \"scaling_factor\": 0.13},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"2B-5B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.80, \"scaling_factor\": 0.55},\n",
        "            \"simple\": {\"min_ratio\": 0.85, \"scaling_factor\": 0.55},\n",
        "            \"medium\": {\"min_ratio\": 0.90, \"scaling_factor\": 0.30},\n",
        "            \"complex\": {\"min_ratio\": 0.95, \"scaling_factor\": 0.10},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"<2B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.85, \"scaling_factor\": 0.50},\n",
        "            \"simple\": {\"min_ratio\": 0.90, \"scaling_factor\": 0.35},\n",
        "            \"medium\": {\"min_ratio\": 0.93, \"scaling_factor\": 0.35},\n",
        "            \"complex\": {\"min_ratio\": 0.97, \"scaling_factor\": 0.05},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # 5-level complexity thresholds and descriptions\n",
        "    \"complexity_levels\": {\n",
        "        \"trivial\": {\n",
        "            \"range\": [0.0, 0.2],\n",
        "        },\n",
        "        \"simple\": {\n",
        "            \"range\": [0.2, 0.4],\n",
        "        },\n",
        "        \"medium\": {\n",
        "            \"range\": [0.4, 0.6],\n",
        "        },\n",
        "        \"complex\": {\n",
        "            \"range\": [0.6, 0.8],\n",
        "        },\n",
        "        \"very_complex\": {\n",
        "            \"range\": [0.8, 1.0],\n",
        "        }\n",
        "    },\n",
        "}\n"
      ],
      "metadata": {
        "id": "feRLhLsw8Voz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support & calculate functions"
      ],
      "metadata": {
        "id": "tlPEOEaj8b7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_model_size_category(model):\n",
        "    \"\"\"\n",
        "    Used only for information.\n",
        "    Automatically detect model size category from model parameters\n",
        "    \"\"\"\n",
        "    try:\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        size_billion = total_params / 1e9\n",
        "\n",
        "        print(f\"🔍 Detected model size: {size_billion:.2f}B parameters\")\n",
        "\n",
        "        if size_billion >= 70:\n",
        "            return \"70B+\"\n",
        "        elif size_billion >= 30:\n",
        "            return \"30B-70B\"\n",
        "        elif size_billion >= 10:\n",
        "            return \"10B-30B\"\n",
        "        elif size_billion >= 5:\n",
        "            return \"5B-10B\"\n",
        "        elif size_billion >= 2:\n",
        "            return \"2B-5B\"\n",
        "        else:\n",
        "            return \"<2B\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error detecting model size: {e}\")\n",
        "        return \"1B-3B\"\n",
        "\n",
        "\n",
        "def count_attention_layers_correctly(model):\n",
        "    \"\"\"\n",
        "    Correctly count attention layers by finding main decoder/transformer layers\n",
        "    \"\"\"\n",
        "    # Method 1: Count main decoder layers directly (most reliable)\n",
        "    decoder_layer_count = 0\n",
        "    for name, module in model.named_modules():\n",
        "        module_type = type(module).__name__\n",
        "        # Look for main transformer/decoder layers\n",
        "        if any(layer_type in module_type for layer_type in\n",
        "               ['DecoderLayer', 'TransformerBlock', 'Block', 'Layer']) and \\\n",
        "           any(exclude not in module_type for exclude in\n",
        "               ['Embedding', 'Norm', 'Linear', 'MLP', 'Attention']):\n",
        "            # Make sure it's a numbered layer (e.g., layers.0, layers.1, etc.)\n",
        "            if '.layers.' in name and name.count('.') == 2:  # e.g., \"model.layers.0\"\n",
        "                decoder_layer_count += 1\n",
        "\n",
        "    if decoder_layer_count > 0:\n",
        "        return decoder_layer_count\n",
        "\n",
        "    # Method 2: Use model config as fallback\n",
        "    try:\n",
        "        if hasattr(model, 'config'):\n",
        "            config_attrs = ['num_hidden_layers', 'n_layer', 'num_layers', 'n_layers']\n",
        "            for attr in config_attrs:\n",
        "                if hasattr(model.config, attr):\n",
        "                    return getattr(model.config, attr)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Method 3: Direct access to layers ModuleList\n",
        "    try:\n",
        "        if hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
        "            return len(model.model.layers)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return 16  # Conservative fallback\n",
        "\n",
        "\n",
        "def classify_complexity_level(complexity_score):\n",
        "    \"\"\"\n",
        "    Classify complexity score into one of 5 levels\n",
        "\n",
        "    Args:\n",
        "        complexity_score (float): Complexity score (0.0-1.0)\n",
        "\n",
        "    Returns:\n",
        "        str: Complexity level (\"trivial\", \"simple\", \"medium\", \"complex\", \"very_complex\")\n",
        "    \"\"\"\n",
        "    levels = ADAPTIVE_CONFIG[\"complexity_levels\"]\n",
        "\n",
        "    for level_name, level_config in levels.items():\n",
        "        min_val, max_val = level_config[\"range\"]\n",
        "        if min_val <= complexity_score < max_val:\n",
        "            return level_name\n",
        "\n",
        "    # Handle edge case for exactly 1.0\n",
        "    if complexity_score >= 0.8:\n",
        "        return \"very_complex\"\n",
        "\n",
        "    return \"trivial\"  # Fallback\n",
        "\n",
        "\n",
        "def calculate_active_layers(total_layers, model_size_category, complexity_score):\n",
        "    \"\"\"\n",
        "    Calculate number of active layers based on complexity and model size\n",
        "\n",
        "    Args:\n",
        "        total_layers (int): Total number of attention layers\n",
        "        model_size_category (str): Model size category\n",
        "        complexity_score (float): Complexity score (0.0-1.0)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (active_layers_count, complexity_level, layer_groups_used, min_guaranteed, max_possible)\n",
        "    \"\"\"\n",
        "    # Classify complexity level\n",
        "    complexity_level = classify_complexity_level(complexity_score)\n",
        "\n",
        "    # Get configuration for this model size and complexity\n",
        "    config = ADAPTIVE_CONFIG[\"model_size_ratios\"][model_size_category][complexity_level]\n",
        "    min_ratio = config[\"min_ratio\"]\n",
        "    scaling_factor = config[\"scaling_factor\"]\n",
        "\n",
        "    # Calculate layer counts\n",
        "    min_guaranteed = int(total_layers * min_ratio)\n",
        "    remaining_layers = total_layers - min_guaranteed\n",
        "    additional_layers = int(complexity_score * scaling_factor * remaining_layers)\n",
        "    active_layers = min_guaranteed + additional_layers\n",
        "\n",
        "    # Ensure we don't exceed total layers\n",
        "    active_layers = min(active_layers, total_layers)\n",
        "    max_possible = total_layers  # Always can reach 100%\n",
        "\n",
        "\n",
        "    return active_layers, complexity_level,  min_guaranteed, max_possible\n",
        "\n",
        "\n",
        "\n",
        "print(\"✅ Enhanced 5-level calculation functions loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n-xWD2o8hmA",
        "outputId": "63026dfa-0529-493b-d303-fa74524ad3f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Enhanced 5-level calculation functions loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8iRr5iapy5q"
      },
      "source": [
        "# Download the Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9IFTYa9P6Zy",
        "outputId": "03001c43-3494-4998-ff4f-03bb4a6bcc56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sbnDW_tZRTGp",
        "outputId": "90515ceb-5204-4e79-ad4e-093dc2674707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "ed048f20f57f4a78bac5614b69091364",
            "a49bf4fe7b4d4b5b90f0bbe93835913b",
            "9ef94cd53439420eafc890152c34a5bd",
            "5214eac789784b09bf4aa1805481fcde",
            "71c5d0f5acfc49f8981a902d5f818da5",
            "4ce64f6d8f9e44eea165bd1ac0061f54",
            "35f5d7e08f674e868e0458cbf6631d08",
            "52729eb860a64aa4a311c2e27483d622",
            "ee5359a655594d45b38a926a5655a854",
            "f4e627d3959c48d080ba621d2c56b0b2",
            "2f5e185dd6cc4cdba9ea183a64666f2a",
            "64937842c8464a02b9ba7ded491c90ed",
            "3e4e05164ddf4f96a90c1ba9fd4f72ef",
            "df6e928928b74cabacca4eb87cecf951",
            "a29e9debeda54cf2878b2874bf92690b",
            "3224fdf91ef54bea8a6af2cc27ccce9c",
            "ac3d2ef2a4c740c98c1119039395cd1a",
            "70bfad28ac184978bfc148cffa68a15a",
            "ed253ea3fe5f4330be7b963e69496f02",
            "f10fc071c47a409aae7fa6eba8e85751",
            "5c3a627e696d4106a2fa48145a4f03a8",
            "543860f9b187473b875d1a370b25d3ae",
            "f4880bca6c204fdb8c4a2de74f456981",
            "ea0e702086334424aca2b833775dc5f0",
            "737122a53d6d448c8a986ed6c345d6d1",
            "399601d0a0f9495faa0454a49ca0f903",
            "210715dba0bb43918a0676df2a2953cc",
            "dcde46654d76489bb8c05fbbdc2bf8ed",
            "763c179aee784c708d1854a37b33689a",
            "09677d68436e4dacaaed1e457a7eff08",
            "a1f33c6eebf84999b0a8ffb9f8d534b9",
            "e4edc11164344eeebd7c544df3a40984",
            "3f0193272c644e94b526e7ad2086bc3c",
            "58131c98f90e4cb98164d7e6973972c9",
            "45a07be96ad24b4f82ac65a8ae30b2c8",
            "b4660425744b4d968cbb7f0e70e44a79",
            "4c9606d971c347e7a2fffa82fdc425cf",
            "f73eb82d060e4b0ea47b49a532f375e6",
            "1121fdb2fe044a4e9bedfd88bd5108e3",
            "84b2b84bbf5644cda2ba872a48d463c4",
            "4d475a0bfa8645f5b7ce9be1131aa815",
            "2c82fa09a07048a6b07d34cac5a34929",
            "50cbee306cd540a198dc0da3fb29c1f7",
            "bf85cc61649b4efbbf50ee174d4b6f88",
            "462b94ad53f044c989ab8013570d72ce",
            "b078ed813d23486ca2c15275e92d8f8c",
            "46b143107d924085b8a5d0d61da9ee3a",
            "b3c1fe672d864454bc108c93d343c064",
            "472aaec536f747a7afa2a845d1b58055",
            "71664fe690e64526bd943492b9612baf",
            "1ac198f661634c7b8026a0695a8ccd45",
            "ece7f18184334e338a17b54adf8357cb",
            "b76ae2dca55e44a7ae875772b0f0df1f",
            "4d78800476064b3b959677c39dc33319",
            "61af0d860f8847b782819a08fde17d3f",
            "d9b61d4bdf5c4c069ead9d11352db706",
            "eff34fe93fba4994b1d5a0792e72c77f",
            "2b1a6491c33c4c29a3388ee3b696da14",
            "762cd7f670f14030a3407f326380299b",
            "9a7177b3ccff4e4e8e3b0c578ad9a7e3",
            "927a3ecf1d8d4c41b01c035b0cf4d083",
            "ca354c7f5b854f65b0532a1e07ff49e8",
            "aa8343d725924fcebad159e31c492667",
            "d3fcd116dbad4cdbab99d046c3e5206a",
            "0301b4a59acf43499b95d53ec44f0988",
            "50a33a132f434c78a05a59beadd80e44",
            "201460dafe95486fb8641e1c0a34b4ba",
            "6b2ae29912714c12847d7085214f6d8e",
            "5edcb7f768c6473da74b2de175f6440e",
            "3c29fab5a72b4848aadffdc5b086519f",
            "4ea4331781484308b3710f8658dc6dc8",
            "1381136601244ec197ec556345ff7937",
            "52572e9a631c4fdda5678029b955df18",
            "d4a9483c29ff4bc5ac9fc9336fb9a818",
            "beed8970dc3b43fe9750c43368c962c4",
            "8e77b231ee1a4b96878bd71bb28f5e2f",
            "5584646b81444eedb6dc64159d1e54dd",
            "c6d4232f54964adab7c8a481fdeb2bfc",
            "888117fa708846cda5cf8045a66f9e00",
            "03089edcc4f74e6cb9e79e9618ef4fa5",
            "8a1a52f1145e42f0985d2909d5ed4d38",
            "8dc6f798f81045afb298415894ebaa2d",
            "3ea4805a1fcc4b819d9708510d41d588",
            "dbe84ffe4f8444d4b118a200ab3a4863",
            "6a3d3d25df93417abce11554e9405dd5",
            "f9871c0d1b734a5489d5160db3e45cbf",
            "6dab9e9024344daba5d693d4abd1d108",
            "44bee7b273b743358f5624166df1272a",
            "dbf5ba2cf5bc46b7a31ea8b607094a44",
            "de9fef56974f473e8a19f296514ef774",
            "467d0b4a4e244f38b73d4fdf5326da71",
            "dd92ab4a0a0748f89c4f57fa34900806",
            "81768475fdfd4eae88a4f18d771cb568",
            "03dbdaa422734e5fbe3997053d54559a",
            "2f7f01ed09af42109c9dc1c7778c2bc4",
            "494ee21fc7e244c4bdc5b91eba8b7f3d",
            "8f84580d900e4014b65c8452ec795040",
            "a8f678b2b7214e1691379934eea19cc9",
            "adb660a666cb4edaa516c98181f91e50",
            "23a5f13bcbef403d893149a5522e847c",
            "2d0724f750424a409a66bdbaf80ac658",
            "84fb76436f5c46c4917719f09a1c984b",
            "aa7d835428094168bfc67e94a5e8d2d2",
            "87f91558ca9446b48e33bbd2ebcfb382",
            "00372be8fc694339a0ff04e137c43c76",
            "6135ea9cc8714408bba914d4025fe87c",
            "09c986f0f8da4b99919a14c1897e7daf",
            "75a8b99bbf2b48df83af0d24b772bf1a",
            "3a9b50368c2f4aabacf4460984b1728d",
            "5f1d0bc5300a4f48800567ae28560562"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed048f20f57f4a78bac5614b69091364"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64937842c8464a02b9ba7ded491c90ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4880bca6c204fdb8c4a2de74f456981"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58131c98f90e4cb98164d7e6973972c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "462b94ad53f044c989ab8013570d72ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9b61d4bdf5c4c069ead9d11352db706"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "201460dafe95486fb8641e1c0a34b4ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6d4232f54964adab7c8a481fdeb2bfc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbf5ba2cf5bc46b7a31ea8b607094a44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23a5f13bcbef403d893149a5522e847c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#model_name = 'meta-llama/Llama-3.2-1B'\n",
        "model_name = 'meta-llama/Llama-3.2-3B'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#tokenizer.pad_token = tokenizer.eos_token  # Set pad token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "TkQnkc3U8l-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the configuration with clean, simplified output\n",
        "if 'model' in locals():\n",
        "    # Get model information with improved detection\n",
        "    total_attention_layers = count_attention_layers_correctly(model)\n",
        "    model_category = detect_model_size_category(model)\n",
        "\n",
        "    print(f\"\\n🏗️ Model Analysis:\")\n",
        "    print(f\"   Attention layers: {total_attention_layers}\")\n",
        "    print(f\"   Size category: {model_category}\")\n",
        "    print(f\"   Architecture: {type(model).__name__}\")\n",
        "\n",
        "    # Show layer detection verification\n",
        "    print(f\"\\n🔍 Layer Detection Verification:\")\n",
        "    decoder_layers = [name for name, module in model.named_modules()\n",
        "                     if 'DecoderLayer' in type(module).__name__ and '.layers.' in name]\n",
        "    print(f\"   Found DecoderLayers: {len(decoder_layers)}\")\n",
        "\n",
        "    # Test all 5 complexity levels with simplified table\n",
        "    test_complexities = GLOBAL_COMPLEXITIES\n",
        "\n",
        "    print(\"\\n🧪 Layer Activation by Complexity Level:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"{'Level':<12} {'Active Layers':<15} {'Usage Ratio':<12}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for complexity in test_complexities:\n",
        "        active, level, min_guaranteed, max_possible = calculate_active_layers(\n",
        "            total_attention_layers, model_category, complexity\n",
        "        )\n",
        "        ratio = active / total_attention_layers\n",
        "\n",
        "        print(f\"{level.capitalize():<12} {active:<15} {ratio:<12.1%}\")\n",
        "\n",
        "    print(f\"\\n📊 Summary for {model_category} model:\")\n",
        "    trivial_config = ADAPTIVE_CONFIG['model_size_ratios'][model_category]['trivial']\n",
        "    trivial_min = int(total_attention_layers * trivial_config['min_ratio'])\n",
        "    print(f\"   • Range: {trivial_min}-{total_attention_layers} layers ({trivial_min/total_attention_layers:.1%}-100%)\")\n",
        "    print(f\"   • All complexity levels can reach 100% layer usage\")\n",
        "\n",
        "else:\n",
        "    print(\"⏳ Load your model first to test the configuration\")\n",
        "    print(\"\\nTo test, make sure you have:\")\n",
        "    print(\"1. model = ... (your loaded model)\")\n",
        "    print(\"2. tokenizer = ... (optional, your tokenizer)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rUlhopw8s63",
        "outputId": "5b172aa9-193d-42fa-97b6-599b05699051"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Detected model size: 3.21B parameters\n",
            "\n",
            "🏗️ Model Analysis:\n",
            "   Attention layers: 28\n",
            "   Size category: 2B-5B\n",
            "   Architecture: LlamaForCausalLM\n",
            "\n",
            "🔍 Layer Detection Verification:\n",
            "   Found DecoderLayers: 28\n",
            "\n",
            "🧪 Layer Activation by Complexity Level:\n",
            "==================================================\n",
            "Level        Active Layers   Usage Ratio \n",
            "--------------------------------------------------\n",
            "Trivial      22              78.6%       \n",
            "Simple       23              82.1%       \n",
            "Medium       25              89.3%       \n",
            "Complex      26              92.9%       \n",
            "Very_complex 28              100.0%      \n",
            "\n",
            "📊 Summary for 2B-5B model:\n",
            "   • Range: 22-28 layers (78.6%-100%)\n",
            "   • All complexity levels can reach 100% layer usage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0ifeungrbHw"
      },
      "source": [
        "## Study the structure.\n",
        "* Llama-3.2-1B\n",
        "```\n",
        "LlamaForCausalLM(\n",
        "  (model): LlamaModel(\n",
        "    (embed_tokens): Embedding(128256, 2048)\n",
        "    (layers): ModuleList(\n",
        "      (0-15): 16 x LlamaDecoderLayer(\n",
        "        (self_attn): LlamaSdpaAttention(\n",
        "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
        "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
        "          (rotary_emb): LlamaRotaryEmbedding()\n",
        "        )\n",
        "        (mlp): LlamaMLP(\n",
        "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
        "          (act_fn): SiLU()\n",
        "        )\n",
        "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "      )\n",
        "    )\n",
        "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "    (rotary_emb): LlamaRotaryEmbedding()\n",
        "  )\n",
        "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "The model follows the typical structure of modern Llama models, consisting of blocks made up of an Attention layer and an MLP layer with a GLU structure.\n",
        "\n",
        "> If you want to see an example of how to perform pruning on the MLP layers of the model, you can check out the notebook:[Pruning Llama 3.2.](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/6_3_pruning_structured_llama3.2-1b_OK.ipynb) y leer el paper [Exploring GLU expansion ratios: Structured pruning in Llama-3.2 models](https://osf.io/preprints/osf/qgxea)\n",
        "\n",
        "\n",
        "Since the layers form a block, the attention layer cannot be removed without also removing the accompanying MLP layer. For this reason, the decision was made to bypass their execution during inference.\n",
        "\n",
        "The 1B model has 16 layers, as shown in the structure above, while the 3B model has 28 layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference function & Test Base Model\n",
        "\n",
        "The `get_output` function is designed to generate text  and measure the time taken for different stages of the generation process.\n",
        "\n",
        "It provides insights into the performance of the model and can be used to evaluate the efficiency of text generation."
      ],
      "metadata": {
        "id": "vF4cHUb_rICs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2igvy4z6rGgy"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def get_output(prompt, model=model, tokenizer=tokenizer, num_runs=1, max_length=50):\n",
        "    print(f\"--- get_output ENTERED. Prompt (first 30 chars): '{prompt[:30]}...' ---\") # New log\n",
        "\n",
        "    total_time = 0\n",
        "    generated_outputs = []\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        # Start timing\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Tokenization time\n",
        "        token_start = time.time()\n",
        "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "        token_time = time.time() - token_start\n",
        "\n",
        "        # Generation time\n",
        "        gen_start = time.time()\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            temperature=None,\n",
        "            top_p=None,\n",
        "            do_sample=False,  # Disable sampling\n",
        "            num_beams=5,      # Use beam search\n",
        "            early_stopping=True,  # Stop when end-of-sequence token is generated\n",
        "            no_repeat_ngram_size=2  # Prevent repetition of 2-grams\n",
        "        )\n",
        "        gen_time = time.time() - gen_start\n",
        "\n",
        "        # Decoding time\n",
        "        decode_start = time.time()\n",
        "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        decode_time = time.time() - decode_start\n",
        "\n",
        "        # Total time for this run\n",
        "        total_time += time.time() - start_time\n",
        "        generated_outputs.append(generated)\n",
        "\n",
        "        if num_runs > 1:\n",
        "            print(f\"\\nRun {run + 1}:\")\n",
        "        print(f\"Tokenization time: {token_time*1000:.2f} ms\")\n",
        "        print(f\"Generation time: {gen_time*1000:.2f} ms\")\n",
        "        print(f\"Decoding time: {decode_time*1000:.2f} ms\")\n",
        "        print(f\"Total time: {(time.time() - start_time)*1000:.2f} ms\")\n",
        "\n",
        "    if num_runs > 1:\n",
        "        avg_time = total_time / num_runs\n",
        "        print(f\"\\nAverage time over {num_runs} runs: {avg_time*1000:.2f} ms\")\n",
        "\n",
        "    return generated_outputs[0] if num_runs == 1 else generated_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b05a468c-6f8c-4579-cb4d-c480790e1c9d",
        "id": "lH7cotAxrhO3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- get_output ENTERED. Prompt (first 30 chars): 'Paris is the capital of...' ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run 1:\n",
            "Tokenization time: 1.64 ms\n",
            "Generation time: 3983.88 ms\n",
            "Decoding time: 0.30 ms\n",
            "Total time: 3985.92 ms\n",
            "\n",
            "Run 2:\n",
            "Tokenization time: 0.60 ms\n",
            "Generation time: 2992.87 ms\n",
            "Decoding time: 0.21 ms\n",
            "Total time: 2993.76 ms\n",
            "\n",
            "Average time over 2 runs: 3489.75 ms\n",
            "Generated text: ['Paris is the capital of France. It is located in the north-central part of the country, on the river Seine. The city has a population of over 2 million people, making it the largest city in France and the second-largest city', 'Paris is the capital of France. It is located in the north-central part of the country, on the river Seine. The city has a population of over 2 million people, making it the largest city in France and the second-largest city']\n"
          ]
        }
      ],
      "source": [
        "# Test the original model\n",
        "prompt = \"Paris is the capital of\"\n",
        "generated = get_output(prompt, num_runs=2)\n",
        "print(f\"Generated text: {generated}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text generation of the original model, as expected, works perfectly and returns a correct and meaningful sentence."
      ],
      "metadata": {
        "id": "mo4IjOYGry0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cpu\")               # actual data moves ↙\n",
        "torch.cuda.empty_cache()      # allocator drops cached blocks"
      ],
      "metadata": {
        "id": "bLN1_gLdt7Rx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjgf4WA_vF3B"
      },
      "source": [
        "# Pruning the Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TQDCkoL6RW3C"
      },
      "outputs": [],
      "source": [
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#from torch.nn import functional as F\n",
        "#from copy import deepcopy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOuKyH0MjyJM"
      },
      "source": [
        "## Execute Pruning."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disclaimer**\n",
        "\n",
        "I'm using a single illustrative prompt so that the code path is easy to follow. In any research or production setting you must feed hundreds or thousands of diverse prompts before deciding which layers to deactivate"
      ],
      "metadata": {
        "id": "wyI5XiqYyBnc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HvhSYyE2Rk1H"
      },
      "outputs": [],
      "source": [
        "# Using multiple prompts for calibration\n",
        "# Using multiple prompts for calibration\n",
        "calibration_prompts = [\n",
        "    # Trivial (0.0-0.2) - Short, simple\n",
        "    \"Hi\",\n",
        "    \"2+2=\",\n",
        "    \"Hello.\",\n",
        "    \"What is 2+2?\",\n",
        "\n",
        "    # Simple (0.2-0.4) - Basic questions\n",
        "    \"What is the capital of France?\",\n",
        "    \"Paris is the capital of \"\n",
        "    \"Tell me a joke.\",\n",
        "    \"Name the capital of Catalonia.\",\n",
        "    \"Who wrote 'To Kill a Mockingbird'?\",\n",
        "\n",
        "    # Medium (0.4-0.6) - Knowledge retrieval, moderate length\n",
        "    \"Explain the basic principles of machine learning and how neural networks work.\",\n",
        "    \"What are the main causes of climate change and what can individuals do to help?\",\n",
        "    \"Summarize the plot of 'The Matrix' in one sentence.\",\n",
        "    \"List three benefits of regular exercise.\",\n",
        "\n",
        "    # Complex (0.6-0.8) - Multi-step reasoning, analysis\n",
        "    \"Compare and contrast the economic policies of Keynesian and Austrian schools of thought, analyzing their effectiveness during different historical periods and explaining which approach would be most suitable for addressing current global economic challenges.\",\n",
        "    \"Design a comprehensive strategy for a small tech startup to compete against established giants like Google and Microsoft in the cloud computing market, considering market positioning, technological differentiation, partnerships, and funding requirements.\",\n",
        "    \"Explain why the sky appears blue during the day.\",\n",
        "    \"Describe how a neural network learns from data.\",\n",
        "\n",
        "    # Very Complex (0.8-1.0) - Deep analysis, creativity, long form\n",
        "    \"Write a detailed philosophical essay examining the ethical implications of artificial intelligence consciousness, incorporating perspectives from utilitarian, deontological, and virtue ethics frameworks, while addressing counterarguments and proposing a novel ethical framework for AI development that balances technological progress with human values and societal well-being.\",\n",
        "    \"Develop a multidisciplinary research proposal that integrates quantum computing, biotechnology, and environmental science to address food security challenges in the context of climate change, including methodology, timeline, budget considerations, potential collaborations, risk assessment, and expected societal impact over the next two decades.\"\n",
        "    \"Given current economic trends, predict one challenge global markets may face in the next decade.\",\n",
        "    \"Write a short poem about the experience of learning something new.\",\n",
        "    \"Produce a 450-word technical tutorial that walks through implementing a transformer-based language model from scratch in NumPy, including positional encoding and scaled-dot-product attention.\"\n",
        "    \"As an expert in global macroeconomics, geopolitical risk assessment, and artificial intelligence ethics, write an in-depth policy advisory report for a coalition of G20 nations facing simultaneous systemic challenges, including post-pandemic inflation volatility, supply chain reconfiguration due to AI-driven automation, increasing regional instability in energy markets, and declining trust in democratic institutions. Your report should propose a coordinated strategy that balances fiscal stimulus with monetary restraint, integrates quantum-secure blockchain for supply chain transparency, and includes AI oversight frameworks aligned with both utilitarian and deontological ethical models. Additionally, evaluate how international institutions like the IMF and the World Bank could modernize their governance structures to reflect multipolar power dynamics, and assess the feasibility of adopting an intergovernmental AI alignment charter inspired by the Paris Agreement model. Your recommendations must be actionable, globally inclusive, and anticipate sociopolitical backlash from both populist and nationalist movements.\",\n",
        "    \"\"\"\n",
        "    Draft Integrated Strategic White-Paper for Inter-Agency Review—\n",
        "\n",
        "Executive Overview:\n",
        "This document synthesises cutting-edge research in climate science, planetary boundaries, quantum-enhanced computation, synthetic bio-manufacturing, neuro-symbolic artificial intelligence, behavioural economics, geopolitics, space-based energy infrastructure, and post-growth macro-finance. It is intended for cabinet-level policymakers across the G20, the African Union, and APEC, as well as multilateral lenders, sovereign wealth funds, philanthropic megadonors, and fourth-sector cooperative alliances.\n",
        "\n",
        "Section 1 – Macroeconomic Volatility & Post-Pandemic Debt Overhang\n",
        "1.1 Analyse the persistence of stagflationary pressures under divergent monetary regimes.\n",
        "1.2 Model cascading default scenarios using agent-based stress tests that incorporate climate-induced supply-chain interruptions, semiconductor chokepoints in Taiwan and the Netherlands, and maritime bottlenecks in the Suez and Panama Canals.\n",
        "1.3 Propose a menu of fiscal-monetary coordination instruments—helicopter stabilisation bonds, biodiversity-linked debt swaps, and anti-fragile carbon border adjustments—scaled to emerging-market liquidity traps.\n",
        "\n",
        "Section 2 – Planetary Health & Regenerative Bio-Economy\n",
        "2.1 Summarise findings from IPCC AR7 draft chapters on irreversible cryosphere tipping points.\n",
        "2.2 Evaluate next-generation direct air capture catalysis that leverages metal-organic frameworks seeded by engineered extremophilic microbes.\n",
        "2.3 Draft a governance blueprint for a Global Soil Microbiome Commons, incorporating indigenous data sovereignty protocols, fair-benefit-sharing algorithms, and quantum-secured telemetry for real-time biodiversity crediting.\n",
        "\n",
        "Section 3 – Quantum-Classical Hybrid Infrastructure\n",
        "3.1 Detail a phased roadmap for 1 000-qubit photonic processors coupled to error-mitigated superconducting qubits for combinatorial optimisation in logistics, drug-discovery, and lattice-QCD.\n",
        "3.2 Define open-standard interfaces that allow sovereign cloud providers to interoperate with NATO-grade zero-trust enclaves and NIST-post-quantum cryptographic suites.\n",
        "3.3 Recommend incentives for talent-mobility corridors bridging quantum start-up clusters in Toronto, Delft, Shenzhen, Sydney, and Kigali.\n",
        "\n",
        "Section 4 – Neuro-Symbolic AI & Alignment Governance\n",
        "4.1 Compare scaling-law extrapolations for transformers, mixture-of-experts, retrieval-augmented decoders, and recursive reasoning agents.\n",
        "4.2 Propose a multi-layer safety stack: interpretability probes, causal influence diagrams, counterfactual policy evaluation, and cooperative inverse-reinforcement architectures monitored by open-weight red-team sandboxes.\n",
        "4.3 Outline a treaty-grade AI Alignment Accord modelled after the Paris Agreement, featuring dynamic capability thresholds, compute-cluster registration, differential privacy audits, and a tiered sanctions regime enforced via programmable CBDCs.\n",
        "\n",
        "Section 5 – Security, Geopolitics & Space-Based Energy\n",
        "5.1 Assess escalation risks stemming from fractional-orbital bombardment systems, low-cost hypersonic glide vehicles, and AI-directed drone swarms.\n",
        "5.2 Present techno-economic viability of kilometre-scale solar power satellites in sun-synchronous orbit, with microwave beaming arrays utilising adaptive phased-conjugate mirrors.\n",
        "5.3 Recommend confidence-building measures: reciprocal on-site inspection, open telemetry APIs, catastrophe-bond insurance pools, and an International Orbital Commons Authority.\n",
        "\n",
        "Section 6 – Behavioural & Cultural Dynamics\n",
        "6.1 Integrate behavioural-nudge frameworks, narrative foresight, and social-network epistemic resilience analytics to counter disinformation loops.\n",
        "6.2 Design outcome-oriented citizen deliberation platforms that leverage quadratic voting, verifiable credentials, and language-agnostic dialogue agents with embedded bias-mitigation layers.\n",
        "\n",
        "Section 7 – Financing Mechanisms & Implementation Timeline\n",
        "7.1 Catalogue blended-finance instruments: catalytic first-loss capital, sovereign green sukuk, resilience impact derivatives, and decentralized autonomous project bonds.\n",
        "7.2 Map a ten-year Gantt chart with critical path analysis, specifying TRL-milestones, regulatory sandboxes, and adaptive procurement clauses.\n",
        "\n",
        "Call to Action:\n",
        "Conclude by articulating how cooperative mission-oriented investment, science-diplomacy trust architecture, and inclusive technology governance can converge to safeguard planetary health while enabling equitable prosperity within the safe-and-just operating space for humanity.\n",
        "    \"\"\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SIMPLE AAB CALIBRATION - MINIMAL CODE\n",
        "# =============================================================================\n",
        "def measure_layer_importance_simple(model, tokenizer, prompts):\n",
        "    \"\"\"Simple layer importance measurement - FIXED using original notebook pattern\"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    total_layers = len(model.model.layers)\n",
        "\n",
        "    # Accumulate importance scores across all prompts\n",
        "    importance_acc = {idx: 0.0 for idx in range(total_layers)}\n",
        "\n",
        "    print(f\"📊 Processing {len(prompts)} prompts across {total_layers} layers...\")\n",
        "\n",
        "    for prompt_idx, prompt in enumerate(prompts):\n",
        "        print(f\"   Processing prompt {prompt_idx + 1}/{len(prompts)}\")\n",
        "\n",
        "        # Tokenize input (following original notebook pattern)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Storage for this prompt's layer inputs/outputs\n",
        "        layer_inputs = {}\n",
        "        layer_outputs = {}\n",
        "\n",
        "        # Create hooks (EXACTLY like the original function)\n",
        "        def q_proj_input_hook(layer_idx):\n",
        "            def _hook(module, module_input):\n",
        "                # Handle tuple input (following original pattern)\n",
        "                inp = module_input[0] if isinstance(module_input, tuple) else module_input\n",
        "                layer_inputs[layer_idx] = inp.detach().clone()\n",
        "            return _hook\n",
        "\n",
        "        def o_proj_output_hook(layer_idx):\n",
        "            def _hook(module, module_input, module_output):\n",
        "                # Handle tuple output (following original pattern)\n",
        "                out = module_output[0] if isinstance(module_output, tuple) else module_output\n",
        "                layer_outputs[layer_idx] = out.detach().clone()\n",
        "            return _hook\n",
        "\n",
        "        # Register hooks for ALL layers (not just unpruned ones)\n",
        "        handles = []\n",
        "        for idx in range(total_layers):\n",
        "            layer = model.model.layers[idx]\n",
        "            handles.append(layer.self_attn.q_proj.register_forward_pre_hook(q_proj_input_hook(idx)))\n",
        "            handles.append(layer.self_attn.o_proj.register_forward_hook(o_proj_output_hook(idx)))\n",
        "\n",
        "        # Forward pass (following original pattern)\n",
        "        with torch.no_grad():\n",
        "            _ = model(**inputs)\n",
        "\n",
        "        # Remove hooks (following original pattern)\n",
        "        for h in handles:\n",
        "            h.remove()\n",
        "\n",
        "        # Calculate importance for each layer (EXACTLY like original)\n",
        "        for idx in range(total_layers):\n",
        "            if idx in layer_inputs and idx in layer_outputs:\n",
        "                inp = layer_inputs[idx]\n",
        "                out = layer_outputs[idx]\n",
        "\n",
        "                # Flatten tensors (following original pattern)\n",
        "                inp_flat = inp.view(inp.size(0), -1)\n",
        "                out_flat = out.view(out.size(0), -1)\n",
        "\n",
        "                # Calculate similarity and importance (following original pattern)\n",
        "                similarity = F.cosine_similarity(inp_flat, out_flat, dim=1).mean().item()\n",
        "                importance_score = 1 - similarity\n",
        "                importance_acc[idx] += importance_score\n",
        "\n",
        "    # Average across all prompts\n",
        "    avg_importance = {idx: importance_acc[idx] / len(prompts) for idx in range(total_layers)}\n",
        "\n",
        "    print(\"✅ Layer importance measurement complete!\")\n",
        "    return avg_importance\n",
        "\n",
        "\n",
        "def create_adaptive_config_simple(model, tokenizer, prompts):\n",
        "    \"\"\"Create OPTIMIZED adaptive config - ultra-simple format for efficient inference\"\"\"\n",
        "    print(\"🚀 Creating optimized adaptive config...\")\n",
        "\n",
        "    # Step 1: Analyze model\n",
        "    model_size_category = detect_model_size_category(model)\n",
        "    total_layers = count_attention_layers_correctly(model)\n",
        "\n",
        "    # Step 2: Measure importance\n",
        "    print(\"📊 Measuring layer importance...\")\n",
        "    importance_scores = measure_layer_importance_simple(model, tokenizer, prompts)\n",
        "\n",
        "    # Step 3: Create layers_by_importance (sorted list)\n",
        "    print(\"🏆 Creating layers_by_importance list...\")\n",
        "    sorted_layers = sorted(importance_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    layers_by_importance = [layer_idx for layer_idx, _ in sorted_layers]\n",
        "\n",
        "    # Step 4: Calculate complexity thresholds using existing notebook functions\n",
        "    print(\"🎯 Calculating complexity thresholds...\")\n",
        "    complexity_scores = GLOBAL_COMPLEXITIES\n",
        "    complexity_thresholds = {}\n",
        "\n",
        "    print(\"📊 Using notebook functions to get exact layer counts:\")\n",
        "    for score in complexity_scores:\n",
        "        active_layers_count, _, _, _ = calculate_active_layers(\n",
        "            total_layers, model_size_category, score\n",
        "        )\n",
        "        complexity_thresholds[score] = active_layers_count\n",
        "        level_name = classify_complexity_level(score)\n",
        "        print(f\"   Score {score:3.1f} ({level_name:12}) → {active_layers_count:2d}/{total_layers} layers\")\n",
        "\n",
        "    # Step 5: Build OPTIMIZED config\n",
        "    print(\"⚙️ Building optimized configuration...\")\n",
        "    config = {\n",
        "        \"model_info\": {\n",
        "            \"name\": getattr(model.config, '_name_or_path', 'unknown'),\n",
        "            \"total_parameters\": f\"{sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\",\n",
        "            \"size_category\": model_size_category,\n",
        "            \"total_layers\": total_layers,\n",
        "            \"architecture\": type(model).__name__\n",
        "        },\n",
        "        \"layers_by_importance\": layers_by_importance,\n",
        "        \"complexity_thresholds\": complexity_thresholds,\n",
        "        \"complexity_weights\": COMPLEXITY_WEIGHTS\n",
        "    }\n",
        "\n",
        "    # Step 6: Save optimized config\n",
        "    with open(\"adaptive_config.json\", \"w\") as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    print(\"✅ OPTIMIZED adaptive_config.json created!\")\n",
        "\n",
        "    # Show optimized results\n",
        "    print(f\"📊 Model: {total_layers} layers, {model_size_category}\")\n",
        "    print(f\"🏆 Layers by importance: {layers_by_importance[:5]}... (showing first 5)\")\n",
        "    print(\"🎯 Complexity thresholds:\")\n",
        "    for threshold, count in complexity_thresholds.items():\n",
        "        percentage = (count / total_layers) * 100\n",
        "        level = classify_complexity_level(threshold)\n",
        "        print(f\"   {threshold:3.1f} ({level:12}): {count:2d} layers ({percentage:4.1f}%)\")\n",
        "\n",
        "    print(\"\\n🚀 ULTRA-EFFICIENT RUNTIME FORMAT:\")\n",
        "\n",
        "    return config\n"
      ],
      "metadata": {
        "id": "6yrcseaPSBpb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SIMPLE EXECUTION - OPTIMIZED VERSION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"🚀 CREATING ULTRA-EFFICIENT ADAPTIVE CONFIG\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create the OPTIMIZED adaptive config using existing calibration_prompts\n",
        "adaptive_config = create_adaptive_config_simple(model, tokenizer, calibration_prompts)\n",
        "\n",
        "print(f\"\\n🎉 DONE! Optimized adaptive_config.json ready for AAB!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRuB-fZ6VK5H",
        "outputId": "b253fad0-1d09-4766-899b-35da96aa8bd2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 CREATING ULTRA-EFFICIENT ADAPTIVE CONFIG\n",
            "==================================================\n",
            "🚀 Creating optimized adaptive config...\n",
            "🔍 Detected model size: 3.21B parameters\n",
            "📊 Measuring layer importance...\n",
            "📊 Processing 21 prompts across 28 layers...\n",
            "   Processing prompt 1/21\n",
            "   Processing prompt 2/21\n",
            "   Processing prompt 3/21\n",
            "   Processing prompt 4/21\n",
            "   Processing prompt 5/21\n",
            "   Processing prompt 6/21\n",
            "   Processing prompt 7/21\n",
            "   Processing prompt 8/21\n",
            "   Processing prompt 9/21\n",
            "   Processing prompt 10/21\n",
            "   Processing prompt 11/21\n",
            "   Processing prompt 12/21\n",
            "   Processing prompt 13/21\n",
            "   Processing prompt 14/21\n",
            "   Processing prompt 15/21\n",
            "   Processing prompt 16/21\n",
            "   Processing prompt 17/21\n",
            "   Processing prompt 18/21\n",
            "   Processing prompt 19/21\n",
            "   Processing prompt 20/21\n",
            "   Processing prompt 21/21\n",
            "✅ Layer importance measurement complete!\n",
            "🏆 Creating layers_by_importance list...\n",
            "🎯 Calculating complexity thresholds...\n",
            "📊 Using notebook functions to get exact layer counts:\n",
            "   Score 0.1 (trivial     ) → 22/28 layers\n",
            "   Score 0.3 (simple      ) → 23/28 layers\n",
            "   Score 0.5 (medium      ) → 25/28 layers\n",
            "   Score 0.7 (complex     ) → 26/28 layers\n",
            "   Score 0.9 (very_complex) → 28/28 layers\n",
            "⚙️ Building optimized configuration...\n",
            "✅ OPTIMIZED adaptive_config.json created!\n",
            "📊 Model: 28 layers, 2B-5B\n",
            "🏆 Layers by importance: [8, 9, 12, 10, 7]... (showing first 5)\n",
            "🎯 Complexity thresholds:\n",
            "   0.1 (trivial     ): 22 layers (78.6%)\n",
            "   0.3 (simple      ): 23 layers (82.1%)\n",
            "   0.5 (medium      ): 25 layers (89.3%)\n",
            "   0.7 (complex     ): 26 layers (92.9%)\n",
            "   0.9 (very_complex): 28 layers (100.0%)\n",
            "\n",
            "🚀 ULTRA-EFFICIENT RUNTIME FORMAT:\n",
            "\n",
            "🎉 DONE! Optimized adaptive_config.json ready for AAB!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adaptive_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExBmu0i5AMvx",
        "outputId": "ae296c35-321b-4eab-ef62-4e5f947c863d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_info': {'name': 'meta-llama/Llama-3.2-3B',\n",
              "  'total_parameters': '3.21B',\n",
              "  'size_category': '2B-5B',\n",
              "  'total_layers': 28,\n",
              "  'architecture': 'LlamaForCausalLM'},\n",
              " 'layers_by_importance': [8,\n",
              "  9,\n",
              "  12,\n",
              "  10,\n",
              "  7,\n",
              "  0,\n",
              "  6,\n",
              "  27,\n",
              "  13,\n",
              "  5,\n",
              "  11,\n",
              "  14,\n",
              "  18,\n",
              "  4,\n",
              "  3,\n",
              "  15,\n",
              "  2,\n",
              "  1,\n",
              "  17,\n",
              "  21,\n",
              "  25,\n",
              "  24,\n",
              "  16,\n",
              "  22,\n",
              "  20,\n",
              "  26,\n",
              "  23,\n",
              "  19],\n",
              " 'complexity_thresholds': {0.1: 22, 0.3: 23, 0.5: 25, 0.7: 26, 0.9: 28},\n",
              " 'complexity_weights': {'token_count': 0.75, 'embedding_variance': 0.25}}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test prompt complexity"
      ],
      "metadata": {
        "id": "X7OMW6WzMCyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_prompt_complexity(prompts, config, model, tokenizer, verbose: bool = True):\n",
        "    \"\"\"\n",
        "    Compute a complexity score in [0, 1] for each prompt.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prompts : list[str]\n",
        "        The text prompts to score.\n",
        "    config : dict\n",
        "        adaptive_config.json already loaded as dict.\n",
        "    model : transformers.PreTrainedModel\n",
        "        The HF model (on CPU or GPU).\n",
        "    tokenizer : transformers.PreTrainedTokenizer\n",
        "        Matching tokenizer.\n",
        "    verbose : bool\n",
        "        If True, print a per-prompt breakdown.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list[tuple[str, float]]\n",
        "        (prompt, complexity_score) for each input string.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get model size and device\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    size_billion = total_params / 1e9\n",
        "    MIN_TOKENS = 4\n",
        "    # Unified size adjustment factor\n",
        "    # Small models (< 2B) get boost, large models (> 10B) get dampening\n",
        "    size_factor = 1.0 + (2.0 - size_billion) * 0.1\n",
        "    size_factor = max(0.5, min(2.0, size_factor))  # Clamp between 0.5 and 2.0\n",
        "\n",
        "\n",
        "    # Length reference scaled by model size\n",
        "    # Smaller models reach max complexity with shorter prompts\n",
        "    base_length = 2000\n",
        "    length_reference = base_length / size_factor\n",
        "    variance_saturation = length_reference / 15\n",
        "\n",
        "    # Get weights from config\n",
        "    weights = config.get(\"complexity_weights\", {\n",
        "        \"token_count\": 0.65,\n",
        "        \"embedding_variance\": 0.35\n",
        "    })\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for prompt in prompts:\n",
        "        # Tokenize\n",
        "        ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0].to(device)\n",
        "        n_tokens = ids.size(0)\n",
        "\n",
        "        # 1. TOKEN SCORE - Simple logarithmic scaling\n",
        "        # Maps token count to [0, 1] with smooth growth\n",
        "        token_score = math.log1p(n_tokens) / math.log1p(length_reference)\n",
        "        token_score = min(token_score * size_factor, 1.0)\n",
        "        if n_tokens < MIN_TOKENS:\n",
        "          dampening = (n_tokens / MIN_TOKENS) ** 2  # Quadratic dampening\n",
        "          token_score = token_score * dampening\n",
        "\n",
        "        # 2. EMBEDDING VARIANCE - Semantic diversity\n",
        "        with torch.no_grad():\n",
        "            emb = model.get_input_embeddings()(ids.unsqueeze(0)).squeeze(0).float()\n",
        "            n = emb.size(0)\n",
        "\n",
        "            if n < 3:\n",
        "                # Too few tokens for meaningful variance\n",
        "                emb_variance = 0.0\n",
        "            else:\n",
        "                # Normalize embeddings\n",
        "                norm_emb = torch.nn.functional.normalize(emb, p=2, dim=1)\n",
        "\n",
        "                # Compute pairwise cosine similarities\n",
        "                sim_matrix = torch.matmul(norm_emb, norm_emb.t())\n",
        "\n",
        "                # Get off-diagonal elements (exclude self-similarity)\n",
        "                mask = ~torch.eye(n, dtype=bool, device=device)\n",
        "                off_diag_sim = sim_matrix[mask]\n",
        "\n",
        "                # Variance = 1 - mean similarity\n",
        "                # Higher variance = more diverse embeddings\n",
        "                emb_variance = 1.0 - off_diag_sim.mean().item()\n",
        "\n",
        "                # Scale by length (longer prompts naturally have more variance)\n",
        "                length_scale = min(n_tokens / variance_saturation, 1.0)\n",
        "                emb_variance = emb_variance * length_scale\n",
        "\n",
        "        # 3. FINAL SCORE - Weighted combination\n",
        "        complexity_score = (\n",
        "            weights[\"token_count\"] * token_score +\n",
        "            weights[\"embedding_variance\"] * emb_variance\n",
        "        )\n",
        "        complexity_score = max(0.0, min(complexity_score, 1.0))\n",
        "\n",
        "        if verbose:\n",
        "            prompt_preview = (prompt[:57] + \"…\") if len(prompt) > 60 else prompt\n",
        "            print(f\"{prompt_preview:<60} | \"\n",
        "                  f\"score={complexity_score:.3f} | \"\n",
        "                  f\"tokens={n_tokens} \"\n",
        "                  f\"[tok={token_score:.3f} var={emb_variance:.3f}]\")\n",
        "\n",
        "        results.append((prompt, round(complexity_score, 4)))\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "P_C8-flogsDJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_prompt_complexity(calibration_prompts, adaptive_config, model,  tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8_F7kJqOVbK",
        "outputId": "e6269999-0e7d-4168-e9f8-8c27fcb4b9a0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi                                                           | score=0.023 | tokens=2 [tok=0.031 var=0.000]\n",
            "2+2=                                                         | score=0.159 | tokens=5 [tok=0.204 var=0.026]\n",
            "Hello.                                                       | score=0.072 | tokens=3 [tok=0.089 var=0.021]\n",
            "What is 2+2?                                                 | score=0.199 | tokens=8 [tok=0.250 var=0.046]\n",
            "What is the capital of France?                               | score=0.200 | tokens=8 [tok=0.250 var=0.050]\n",
            "Paris is the capital of Tell me a joke.                      | score=0.229 | tokens=11 [tok=0.282 var=0.069]\n",
            "Name the capital of Catalonia.                               | score=0.189 | tokens=7 [tok=0.236 var=0.046]\n",
            "Who wrote 'To Kill a Mockingbird'?                           | score=0.230 | tokens=11 [tok=0.282 var=0.072]\n",
            "Explain the basic principles of machine learning and how …   | score=0.260 | tokens=15 [tok=0.315 var=0.096]\n",
            "What are the main causes of climate change and what can i…   | score=0.272 | tokens=17 [tok=0.329 var=0.104]\n",
            "Summarize the plot of 'The Matrix' in one sentence.          | score=0.260 | tokens=15 [tok=0.315 var=0.095]\n",
            "List three benefits of regular exercise.                     | score=0.200 | tokens=8 [tok=0.250 var=0.052]\n",
            "Compare and contrast the economic policies of Keynesian a…   | score=0.372 | tokens=38 [tok=0.416 var=0.240]\n",
            "Design a comprehensive strategy for a small tech startup …   | score=0.372 | tokens=38 [tok=0.416 var=0.238]\n",
            "Explain why the sky appears blue during the day.             | score=0.238 | tokens=12 [tok=0.292 var=0.077]\n",
            "Describe how a neural network learns from data.              | score=0.221 | tokens=10 [tok=0.273 var=0.065]\n",
            "Write a detailed philosophical essay examining the ethica…   | score=0.431 | tokens=55 [tok=0.458 var=0.350]\n",
            "Develop a multidisciplinary research proposal that integr…   | score=0.478 | tokens=72 [tok=0.488 var=0.451]\n",
            "Write a short poem about the experience of learning somet…   | score=0.245 | tokens=13 [tok=0.300 var=0.082]\n",
            "Produce a 450-word technical tutorial that walks through …   | score=0.699 | tokens=211 [tok=0.609 var=0.968]\n",
            "\n",
            "    Draft Integrated Strategic White-Paper for Inter-Age…   | score=0.825 | tokens=904 [tok=0.774 var=0.978]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Hi', 0.0234),\n",
              " ('2+2=', 0.1594),\n",
              " ('Hello.', 0.0716),\n",
              " ('What is 2+2?', 0.1987),\n",
              " ('What is the capital of France?', 0.1999),\n",
              " ('Paris is the capital of Tell me a joke.', 0.2292),\n",
              " ('Name the capital of Catalonia.', 0.1888),\n",
              " (\"Who wrote 'To Kill a Mockingbird'?\", 0.2298),\n",
              " ('Explain the basic principles of machine learning and how neural networks work.',\n",
              "  0.2603),\n",
              " ('What are the main causes of climate change and what can individuals do to help?',\n",
              "  0.2723),\n",
              " (\"Summarize the plot of 'The Matrix' in one sentence.\", 0.2601),\n",
              " ('List three benefits of regular exercise.', 0.2003),\n",
              " ('Compare and contrast the economic policies of Keynesian and Austrian schools of thought, analyzing their effectiveness during different historical periods and explaining which approach would be most suitable for addressing current global economic challenges.',\n",
              "  0.3723),\n",
              " ('Design a comprehensive strategy for a small tech startup to compete against established giants like Google and Microsoft in the cloud computing market, considering market positioning, technological differentiation, partnerships, and funding requirements.',\n",
              "  0.3717),\n",
              " ('Explain why the sky appears blue during the day.', 0.2379),\n",
              " ('Describe how a neural network learns from data.', 0.2206),\n",
              " ('Write a detailed philosophical essay examining the ethical implications of artificial intelligence consciousness, incorporating perspectives from utilitarian, deontological, and virtue ethics frameworks, while addressing counterarguments and proposing a novel ethical framework for AI development that balances technological progress with human values and societal well-being.',\n",
              "  0.4307),\n",
              " ('Develop a multidisciplinary research proposal that integrates quantum computing, biotechnology, and environmental science to address food security challenges in the context of climate change, including methodology, timeline, budget considerations, potential collaborations, risk assessment, and expected societal impact over the next two decades.Given current economic trends, predict one challenge global markets may face in the next decade.',\n",
              "  0.4785),\n",
              " ('Write a short poem about the experience of learning something new.',\n",
              "  0.2454),\n",
              " ('Produce a 450-word technical tutorial that walks through implementing a transformer-based language model from scratch in NumPy, including positional encoding and scaled-dot-product attention.As an expert in global macroeconomics, geopolitical risk assessment, and artificial intelligence ethics, write an in-depth policy advisory report for a coalition of G20 nations facing simultaneous systemic challenges, including post-pandemic inflation volatility, supply chain reconfiguration due to AI-driven automation, increasing regional instability in energy markets, and declining trust in democratic institutions. Your report should propose a coordinated strategy that balances fiscal stimulus with monetary restraint, integrates quantum-secure blockchain for supply chain transparency, and includes AI oversight frameworks aligned with both utilitarian and deontological ethical models. Additionally, evaluate how international institutions like the IMF and the World Bank could modernize their governance structures to reflect multipolar power dynamics, and assess the feasibility of adopting an intergovernmental AI alignment charter inspired by the Paris Agreement model. Your recommendations must be actionable, globally inclusive, and anticipate sociopolitical backlash from both populist and nationalist movements.',\n",
              "  0.6988),\n",
              " ('\\n    Draft Integrated Strategic White-Paper for Inter-Agency Review—\\n\\nExecutive Overview:\\nThis document synthesises cutting-edge research in climate science, planetary boundaries, quantum-enhanced computation, synthetic bio-manufacturing, neuro-symbolic artificial intelligence, behavioural economics, geopolitics, space-based energy infrastructure, and post-growth macro-finance. It is intended for cabinet-level policymakers across the G20, the African Union, and APEC, as well as multilateral lenders, sovereign wealth funds, philanthropic megadonors, and fourth-sector cooperative alliances.\\n\\nSection 1 – Macroeconomic Volatility & Post-Pandemic Debt Overhang\\n1.1\\u2003Analyse the persistence of stagflationary pressures under divergent monetary regimes.\\n1.2\\u2003Model cascading default scenarios using agent-based stress tests that incorporate climate-induced supply-chain interruptions, semiconductor chokepoints in Taiwan and the Netherlands, and maritime bottlenecks in the Suez and Panama Canals.\\n1.3\\u2003Propose a menu of fiscal-monetary coordination instruments—helicopter stabilisation bonds, biodiversity-linked debt swaps, and anti-fragile carbon border adjustments—scaled to emerging-market liquidity traps.\\n\\nSection 2 – Planetary Health & Regenerative Bio-Economy\\n2.1\\u2003Summarise findings from IPCC AR7 draft chapters on irreversible cryosphere tipping points.\\n2.2\\u2003Evaluate next-generation direct air capture catalysis that leverages metal-organic frameworks seeded by engineered extremophilic microbes.\\n2.3\\u2003Draft a governance blueprint for a Global Soil Microbiome Commons, incorporating indigenous data sovereignty protocols, fair-benefit-sharing algorithms, and quantum-secured telemetry for real-time biodiversity crediting.\\n\\nSection 3 – Quantum-Classical Hybrid Infrastructure\\n3.1\\u2003Detail a phased roadmap for 1 000-qubit photonic processors coupled to error-mitigated superconducting qubits for combinatorial optimisation in logistics, drug-discovery, and lattice-QCD.\\n3.2\\u2003Define open-standard interfaces that allow sovereign cloud providers to interoperate with NATO-grade zero-trust enclaves and NIST-post-quantum cryptographic suites.\\n3.3\\u2003Recommend incentives for talent-mobility corridors bridging quantum start-up clusters in Toronto, Delft, Shenzhen, Sydney, and Kigali.\\n\\nSection 4 – Neuro-Symbolic AI & Alignment Governance\\n4.1\\u2003Compare scaling-law extrapolations for transformers, mixture-of-experts, retrieval-augmented decoders, and recursive reasoning agents.\\n4.2\\u2003Propose a multi-layer safety stack: interpretability probes, causal influence diagrams, counterfactual policy evaluation, and cooperative inverse-reinforcement architectures monitored by open-weight red-team sandboxes.\\n4.3\\u2003Outline a treaty-grade AI Alignment Accord modelled after the Paris Agreement, featuring dynamic capability thresholds, compute-cluster registration, differential privacy audits, and a tiered sanctions regime enforced via programmable CBDCs.\\n\\nSection 5 – Security, Geopolitics & Space-Based Energy\\n5.1\\u2003Assess escalation risks stemming from fractional-orbital bombardment systems, low-cost hypersonic glide vehicles, and AI-directed drone swarms.\\n5.2\\u2003Present techno-economic viability of kilometre-scale solar power satellites in sun-synchronous orbit, with microwave beaming arrays utilising adaptive phased-conjugate mirrors.\\n5.3\\u2003Recommend confidence-building measures: reciprocal on-site inspection, open telemetry APIs, catastrophe-bond insurance pools, and an International Orbital Commons Authority.\\n\\nSection 6 – Behavioural & Cultural Dynamics\\n6.1\\u2003Integrate behavioural-nudge frameworks, narrative foresight, and social-network epistemic resilience analytics to counter disinformation loops.\\n6.2\\u2003Design outcome-oriented citizen deliberation platforms that leverage quadratic voting, verifiable credentials, and language-agnostic dialogue agents with embedded bias-mitigation layers.\\n\\nSection 7 – Financing Mechanisms & Implementation Timeline\\n7.1\\u2003Catalogue blended-finance instruments: catalytic first-loss capital, sovereign green sukuk, resilience impact derivatives, and decentralized autonomous project bonds.\\n7.2\\u2003Map a ten-year Gantt chart with critical path analysis, specifying TRL-milestones, regulatory sandboxes, and adaptive procurement clauses.\\n\\nCall to Action:\\nConclude by articulating how cooperative mission-oriented investment, science-diplomacy trust architecture, and inclusive technology governance can converge to safeguard planetary health while enabling equitable prosperity within the safe-and-just operating space for humanity.\\n    ',\n",
              "  0.8249)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AAB Implementation"
      ],
      "metadata": {
        "id": "fu6QaAE0iK9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "import logging"
      ],
      "metadata": {
        "id": "x1Ep1jh64LP6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "sPSj27_Imhm8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerActivationMask:\n",
        "    \"\"\"\n",
        "    External mask system to control which layers are active.\n",
        "    This avoids modifying model state and provides clean separation.\n",
        "    \"\"\"\n",
        "    def __init__(self, total_layers: int):\n",
        "        self.total_layers = total_layers\n",
        "        self.active_mask = [True] * total_layers\n",
        "        self.current_complexity = None\n",
        "        self.current_active_count = total_layers\n",
        "\n",
        "        # NEW: Debug tracking\n",
        "        self.execution_log = []\n",
        "        self.current_inference_id = 0\n",
        "        self.last_sequence_length = 0  # For layer 0 trigger detection\n",
        "\n",
        "    def update_for_prompt(self, active_layer_indices: List[int], complexity_score: float):\n",
        "        \"\"\"Update mask based on active layer indices for current prompt\"\"\"\n",
        "        self.active_mask = [i in active_layer_indices for i in range(self.total_layers)]\n",
        "        self.current_complexity = complexity_score\n",
        "        self.current_active_count = len(active_layer_indices)\n",
        "\n",
        "        # Reset execution log for new inference\n",
        "        self.execution_log = []\n",
        "        self.current_inference_id += 1\n",
        "\n",
        "    def is_layer_active(self, layer_idx: int) -> bool:\n",
        "        \"\"\"Check if a specific layer should be active\"\"\"\n",
        "        return self.active_mask[layer_idx]\n",
        "\n",
        "    def get_stats(self) -> Dict:\n",
        "        \"\"\"Get current mask statistics\"\"\"\n",
        "        return {\n",
        "            'complexity_score': self.current_complexity,\n",
        "            'active_layers': self.current_active_count,\n",
        "            'total_layers': self.total_layers,\n",
        "            'usage_ratio': self.current_active_count / self.total_layers if self.current_active_count else 0,\n",
        "            'initialized': self.current_complexity is not None\n",
        "        }\n",
        "\n",
        "    def log_layer_execution(self, layer_idx: int, executed: bool):\n",
        "        \"\"\"DEBUG: Log whether a layer was executed or bypassed\"\"\"\n",
        "        self.execution_log.append({\n",
        "            'inference_id': self.current_inference_id,\n",
        "            'layer_idx': layer_idx,\n",
        "            'executed': executed,\n",
        "            'expected_active': self.active_mask[layer_idx]\n",
        "        })\n",
        "\n",
        "    def get_execution_stats(self) -> Dict:\n",
        "        \"\"\"DEBUG: Get detailed execution statistics\"\"\"\n",
        "        if not self.execution_log:\n",
        "            return {\n",
        "                'inference_id': self.current_inference_id,\n",
        "                'layers_executed': [],\n",
        "                'layers_bypassed': [],\n",
        "                'total_calls': 0,\n",
        "                'execution_matches_mask': True\n",
        "            }\n",
        "\n",
        "        executed = [log['layer_idx'] for log in self.execution_log if log['executed']]\n",
        "        bypassed = [log['layer_idx'] for log in self.execution_log if not log['executed']]\n",
        "\n",
        "        # Check if execution matches expected mask\n",
        "        execution_matches = True\n",
        "        for log in self.execution_log:\n",
        "            if log['executed'] != log['expected_active']:\n",
        "                execution_matches = False\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'inference_id': self.current_inference_id,\n",
        "            'layers_executed': sorted(executed),\n",
        "            'layers_bypassed': sorted(bypassed),\n",
        "            'total_calls': len(self.execution_log),\n",
        "            'execution_matches_mask': execution_matches,\n",
        "            'expected_active': [i for i, active in enumerate(self.active_mask) if active],\n",
        "            'expected_bypassed': [i for i, active in enumerate(self.active_mask) if not active]\n",
        "        }"
      ],
      "metadata": {
        "id": "qNjXffvpmkEg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_model_architecture(model) -> str:\n",
        "    \"\"\"\n",
        "    Automatically detect model architecture for compatibility\n",
        "    \"\"\"\n",
        "    model_class = model.__class__.__name__.lower()\n",
        "    model_name = getattr(model.config, '_name_or_path', '').lower()\n",
        "\n",
        "    if 'llama' in model_class or 'llama' in model_name:\n",
        "        return 'llama'\n",
        "    elif 'mistral' in model_class or 'mistral' in model_name:\n",
        "        return 'mistral'\n",
        "    elif 'gpt2' in model_class or 'gpt2' in model_name:\n",
        "        return 'gpt2'\n",
        "    else:\n",
        "        # Default to generic transformer approach\n",
        "        return 'generic'\n",
        "\n"
      ],
      "metadata": {
        "id": "nAIBQr5qmqnZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attention_layers(model, architecture: str) -> List:\n",
        "    \"\"\"\n",
        "    Get attention layers based on architecture\n",
        "    \"\"\"\n",
        "    if architecture in ['llama', 'mistral']:\n",
        "        return model.model.layers\n",
        "    elif architecture == 'gpt2':\n",
        "        return model.transformer.h\n",
        "    else:\n",
        "        # Generic approach - try common patterns\n",
        "        if hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
        "            return model.model.layers\n",
        "        elif hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):\n",
        "            return model.transformer.h\n",
        "        else:\n",
        "            raise ValueError(f\"Cannot find attention layers for architecture: {architecture}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ZqaBxaCgmuAa"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attention_module(layer, architecture: str):\n",
        "    \"\"\"\n",
        "    Get the attention module from a layer based on architecture\n",
        "    \"\"\"\n",
        "    if architecture in ['llama', 'mistral']:\n",
        "        return layer.self_attn\n",
        "    elif architecture == 'gpt2':\n",
        "        return layer.attn\n",
        "    else:\n",
        "        # Generic approach\n",
        "        if hasattr(layer, 'self_attn'):\n",
        "            return layer.self_attn\n",
        "        elif hasattr(layer, 'attn'):\n",
        "            return layer.attn\n",
        "        else:\n",
        "            raise ValueError(f\"Cannot find attention module for architecture: {architecture}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "a0JMDO14mzeu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_prompt_complexity_runtime(prompt: str, model, tokenizer, config: Dict) -> float:\n",
        "    \"\"\"\n",
        "    EXACT copy of your analyze_prompt_complexity algorithm, adapted for single prompt runtime use.\n",
        "    Every calculation line is identical to your original function.\n",
        "    \"\"\"\n",
        "    # --- model-specific constants ---------------------------------- (EXACT COPY)\n",
        "    hidden_size = model.config.hidden_size\n",
        "\n",
        "    # --- weights (fallback to defaults if not in config) ----------- (EXACT COPY)\n",
        "    weights = config.get(\"complexity_weights\", {\n",
        "        \"token_count\": 0.65,\n",
        "        \"embedding_variance\": 0.35\n",
        "    })\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    param_str = config[\"model_info\"][\"total_parameters\"]  # e.g. \"3.21B\" (EXACT COPY)\n",
        "    param_count = float(param_str.rstrip(\"B\"))              # 3.21 (EXACT COPY)\n",
        "\n",
        "    # 1) Normalize to a 7 B reference and clamp (EXACT COPY)\n",
        "    param_factor = param_count / 7.0\n",
        "    param_factor = max(min(param_factor, 2.0), 0.5)\n",
        "\n",
        "    # 2) Compute a length budget scaled by model size (EXACT COPY)\n",
        "    #    (small models saturate sooner, large ones later)\n",
        "    base_length = 4000\n",
        "    length_reference = base_length * param_factor\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    size_billion = total_params / 1e9\n",
        "    token_multiplier = max(0.85, 1.3 - size_billion * 0.1)\n",
        "    variance_multiplier = max(0.9, 1.2 - size_billion * 0.08)\n",
        "\n",
        "    # --- Tokenise on the model's device ------------------------ (EXACT COPY)\n",
        "    ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0].to(device)\n",
        "    n_tokens = ids.size(0)\n",
        "\n",
        "    # 3) Raw length score (log-scaled) (EXACT COPY)\n",
        "    raw_score = math.log1p(n_tokens) / math.log1p(length_reference)\n",
        "    # 4) Size adjustment: boost small models, damp large ones (EXACT COPY)\n",
        "    size_adjust = 1.0 / param_factor\n",
        "    adj_score = raw_score * size_adjust\n",
        "    MIN_TOKS = 4\n",
        "    baseline_raw = math.log1p(MIN_TOKS) / math.log1p(length_reference)\n",
        "    baseline_adj = baseline_raw * size_adjust\n",
        "    # 5) Final token score (EXACT COPY)\n",
        "    token_score = (adj_score - baseline_adj) / (1.0 - baseline_adj)\n",
        "    raw_token_score = max(0.0, min(token_score, 1.0))\n",
        "    token_score = min(raw_token_score * token_multiplier, 1.0)\n",
        "\n",
        "    # C) EMBEDDING VARIANCE  — std/√d clamped to 1 (EXACT COPY)\n",
        "    with torch.no_grad():\n",
        "        emb = model.get_input_embeddings()(ids.unsqueeze(0)).squeeze(0).float()\n",
        "        n = emb.size(0)\n",
        "        if n < 3:                       # 1- or 2-token prompt → no diversity\n",
        "            emb_var_norm = 0.0\n",
        "        else:\n",
        "            # 1.  ℓ2-normalise each embedding vector (EXACT COPY)\n",
        "            norm_emb = torch.nn.functional.normalize(emb, p=2, dim=1)    # (n, d)\n",
        "            # 2.  Full cosine-similarity matrix (EXACT COPY)\n",
        "            sim = torch.matmul(norm_emb, norm_emb.t())                   # (n, n)\n",
        "            # 3.  Remove self-similarities (diagonal) and compute mean (EXACT COPY)\n",
        "            off_diag = sim[~torch.eye(n, dtype=bool, device=sim.device)]  # (n²-n,)\n",
        "            base_var = 1.0 - off_diag.mean().item()   # 0 … 1   (0 = identical, 1 = orthogonal)\n",
        "            # 4.  Length factor: 0 → 1 across full context window (EXACT COPY)\n",
        "            len_fac = math.log1p(n) / math.log1p(length_reference)         # 0 … 1\n",
        "            # 5.  Combine & clamp (EXACT COPY)\n",
        "            raw_emb_var_norm = min(base_var * len_fac, 1.0)\n",
        "            emb_var_norm = min(raw_emb_var_norm * variance_multiplier, 1.0)\n",
        "\n",
        "    # --- Weighted combination --------------------------------- (EXACT COPY)\n",
        "    score = (\n",
        "        weights[\"token_count\"] * token_score +\n",
        "        weights[\"embedding_variance\"] * emb_var_norm\n",
        "    )\n",
        "    score = max(0.0, min(score, 1.0))  # clamp for safety\n",
        "\n",
        "    return score\n",
        "\n"
      ],
      "metadata": {
        "id": "zR5_CEFxm4Tx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_active_layers_for_prompt(complexity_score: float, config: Dict) -> List[int]:\n",
        "    \"\"\"\n",
        "    Use your pre-computed complexity_thresholds instead of recalculating.\n",
        "    This respects your original calibration work exactly!\n",
        "    \"\"\"\n",
        "    layers_by_importance = config[\"layers_by_importance\"]\n",
        "    complexity_thresholds = config[\"complexity_thresholds\"]\n",
        "\n",
        "    # Convert string keys to float and sort (EXACT logic from your original design)\n",
        "    thresholds = [(float(k), v) for k, v in complexity_thresholds.items()]\n",
        "    thresholds.sort()\n",
        "\n",
        "    # Find the appropriate number of layers to activate (EXACT logic)\n",
        "    num_layers_to_activate = thresholds[-1][1]  # Default to max\n",
        "\n",
        "    for threshold, num_layers in thresholds:\n",
        "        if complexity_score <= threshold:\n",
        "            num_layers_to_activate = num_layers\n",
        "            break\n",
        "\n",
        "    # Return the most important N layers using your ranking (EXACT COPY)\n",
        "    return layers_by_importance[:num_layers_to_activate]\n",
        "\n"
      ],
      "metadata": {
        "id": "Mari1I8em8U4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_manual_complexity_methods(model, tokenizer, config: Dict):\n",
        "    \"\"\"\n",
        "    Add manual methods for complexity calculation and debugging.\n",
        "    These work independently of the automatic system.\n",
        "    \"\"\"\n",
        "    def manual_complexity_calculation(prompt: str) -> float:\n",
        "        \"\"\"Calculate exact prompt complexity manually\"\"\"\n",
        "        return compute_prompt_complexity_runtime(prompt, model, tokenizer, config)\n",
        "\n",
        "    def manual_mask_update(complexity_score: float):\n",
        "        \"\"\"Manually update the adaptive mask\"\"\"\n",
        "        active_layers = get_active_layers_for_prompt(complexity_score, config)\n",
        "        model._adaptive_mask.update_for_prompt(active_layers, complexity_score)\n",
        "        return model._adaptive_mask.get_stats()\n",
        "\n",
        "    def get_debug_info():\n",
        "        \"\"\"Get comprehensive debug information\"\"\"\n",
        "        stats = model._adaptive_mask.get_stats()\n",
        "        execution_stats = model._adaptive_mask.get_execution_stats()\n",
        "\n",
        "        return {\n",
        "            'mask_stats': stats,\n",
        "            'execution_stats': execution_stats,\n",
        "            'config_thresholds': config['complexity_thresholds'],\n",
        "            'layers_by_importance': config['layers_by_importance'][:10]  # First 10\n",
        "        }\n",
        "\n",
        "    def test_prompt_processing(prompt: str, verbose: bool = True):\n",
        "        \"\"\"Test end-to-end prompt processing\"\"\"\n",
        "        if verbose:\n",
        "            print(f\"🧪 Testing prompt: '{prompt[:50]}{'...' if len(prompt) > 50 else ''}'\")\n",
        "\n",
        "        # Step 1: Calculate complexity\n",
        "        complexity = manual_complexity_calculation(prompt)\n",
        "        if verbose:\n",
        "            print(f\"   Complexity: {complexity:.4f}\")\n",
        "\n",
        "        # Step 2: Update mask\n",
        "        stats = manual_mask_update(complexity)\n",
        "        if verbose:\n",
        "            print(f\"   Active layers: {stats['active_layers']}/{stats['total_layers']} \"\n",
        "                  f\"({stats['usage_ratio']:.1%})\")\n",
        "\n",
        "        # Step 3: Simulate inference (tokenize)\n",
        "        inputs = tokenizer(prompt, return_tensors='pt').to(next(model.parameters()).device)\n",
        "\n",
        "        # Step 4: Test forward pass\n",
        "        with torch.no_grad():\n",
        "            result = model.forward(input_ids=inputs['input_ids'])\n",
        "\n",
        "        # Step 5: Get execution stats\n",
        "        exec_stats = model._adaptive_mask.get_execution_stats()\n",
        "        if verbose:\n",
        "            print(f\"   Executed layers: {exec_stats['layers_executed']}\")\n",
        "            print(f\"   Bypassed layers: {exec_stats['layers_bypassed']}\")\n",
        "            print(f\"   Execution matches mask: {exec_stats['execution_matches_mask']}\")\n",
        "\n",
        "        return {\n",
        "            'complexity': complexity,\n",
        "            'mask_stats': stats,\n",
        "            'execution_stats': exec_stats\n",
        "        }\n",
        "\n",
        "    # Add methods to model\n",
        "    model.manual_complexity = manual_complexity_calculation\n",
        "    model.manual_mask_update = manual_mask_update\n",
        "    model.get_debug_info = get_debug_info\n",
        "    model.test_prompt = test_prompt_processing\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "EcgYMn2G4zCM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hook into model's forward method to automatically compute complexity\n",
        "def add_automatic_complexity_computation(model, tokenizer):\n",
        "    \"\"\"\n",
        "    Add automatic complexity computation to model's forward method.\n",
        "    This will automatically update the adaptive mask when new prompts are processed.\n",
        "    \"\"\"\n",
        "    print(\"--- add_automatic_complexity_computation PRINT: ENTERED ---\")\n",
        "\n",
        "    if not hasattr(model, '_adaptive_mask'):\n",
        "        print(\"add_automatic_complexity_computation PRINT: ERROR - Model must be created with create_adaptive_model() first\")\n",
        "        raise ValueError(\"Model must be created with create_adaptive_model() first\")\n",
        "\n",
        "    print(f\"add_automatic_complexity_computation PRINT: model.forward BEFORE change is: {model.forward}\")\n",
        "\n",
        "    # Store original forward\n",
        "    if not hasattr(model, '_original_forward'):\n",
        "        model._original_forward = model.forward # CORRECTED\n",
        "        print(f\"add_automatic_complexity_computation PRINT: Stored _original_forward: {model._original_forward}\")\n",
        "    else:\n",
        "        print(f\"add_automatic_complexity_computation PRINT: _original_forward already exists: {model._original_forward}\")\n",
        "\n",
        "    # This is the NESTED function adaptive_model_forward.\n",
        "    # It should be defined INSIDE add_automatic_complexity_computation(model, tokenizer):\n",
        "    def adaptive_model_forward(self, input_ids=None, **kwargs): # 'self' here is the model instance\n",
        "        print(\"--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\")\n",
        "\n",
        "        current_call_input_ids = input_ids\n",
        "        if current_call_input_ids is None and 'input_ids' in kwargs:\n",
        "            current_call_input_ids = kwargs['input_ids']\n",
        "            print(f\"adaptive_model_forward (nested in add_auto) PRINT: Using 'input_ids' from kwargs. Shape: {current_call_input_ids.shape if current_call_input_ids is not None else 'None'}\")\n",
        "        elif current_call_input_ids is not None:\n",
        "            print(f\"adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: {current_call_input_ids.shape}\")\n",
        "        else:\n",
        "            print(\"adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' is None and not in kwargs.\")\n",
        "\n",
        "        # Essential attributes check\n",
        "        if not hasattr(self, '_adaptive_config') or not hasattr(self, '_adaptive_mask'):\n",
        "            print(\"adaptive_model_forward (nested in add_auto) PRINT: ERROR - _adaptive_config or _adaptive_mask not found on model!\")\n",
        "            if hasattr(self, '_original_forward'):\n",
        "                return self._original_forward(input_ids=input_ids, **kwargs) # Pass original args\n",
        "            raise RuntimeError(\"Critical setup error: _original_forward missing and essential AAB attributes not found on model.\")\n",
        "\n",
        "        # Determine if this is effectively the first pass for generating a prompt.\n",
        "        past_key_values = kwargs.get('past_key_values')\n",
        "        is_effectively_first_pass = False\n",
        "\n",
        "        print(f\"adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: {past_key_values is None}\")\n",
        "\n",
        "        if past_key_values is None:\n",
        "            is_effectively_first_pass = True\n",
        "            print(\"adaptive_model_forward (nested in add_auto) PRINT: >>> past_key_values is None. is_effectively_first_pass = True.\")\n",
        "        elif hasattr(past_key_values, 'seen_tokens'): # Check for DynamicCache-like 'seen_tokens'\n",
        "            current_cache_seq_len = past_key_values.seen_tokens\n",
        "            print(f\"adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: {type(past_key_values)}. seen_tokens: {current_cache_seq_len}\")\n",
        "            if current_cache_seq_len == 0:\n",
        "                is_effectively_first_pass = True\n",
        "                print(f\"adaptive_model_forward (nested in add_auto) PRINT: >>> Cache 'seen_tokens' is 0. is_effectively_first_pass = True.\")\n",
        "        elif (isinstance(past_key_values, tuple) and # Fallback for standard tuple-based KV cache\n",
        "              len(past_key_values) > 0 and\n",
        "              isinstance(past_key_values[0], tuple) and len(past_key_values[0]) > 0 and\n",
        "              hasattr(past_key_values[0][0], 'shape') and past_key_values[0][0].shape[-2] == 0):\n",
        "            is_effectively_first_pass = True\n",
        "            print(f\"adaptive_model_forward (nested in add_auto) PRINT: >>> Tuple-based KV cache indicates empty state (Layer 0 Key_cache seq_len=0). is_effectively_first_pass = True. Shape: {past_key_values[0][0].shape}\")\n",
        "        else:\n",
        "            if past_key_values is not None:\n",
        "                 print(f\"adaptive_model_forward (nested in add_auto) PRINT: past_key_values is type {type(past_key_values)} and not recognized as empty by current checks.\")\n",
        "            # is_effectively_first_pass remains False if none of the above conditions are met\n",
        "\n",
        "        print(f\"adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: {is_effectively_first_pass}\")\n",
        "\n",
        "        # Check if we can get a prompt for complexity calculation\n",
        "        can_get_prompt_for_complexity = (current_call_input_ids is not None and\n",
        "                                         current_call_input_ids.ndim == 2 and\n",
        "                                         current_call_input_ids.shape[0] > 0 and # Batch size > 0\n",
        "                                         current_call_input_ids.shape[1] > 0)   # Sequence length > 0\n",
        "        print(f\"adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: {can_get_prompt_for_complexity}\")\n",
        "\n",
        "        # Main logic block trigger:\n",
        "        if is_effectively_first_pass and can_get_prompt_for_complexity:\n",
        "            print(\"adaptive_model_forward (nested in add_auto) PRINT: Effective first pass & valid inputs. Entering main logic block.\")\n",
        "            try:\n",
        "                # Decode from the first item in the batch (e.g., current_call_input_ids[0])\n",
        "                prompt_text = tokenizer.decode(current_call_input_ids[0], skip_special_tokens=True)\n",
        "                print(f\"adaptive_model_forward (nested in add_auto) PRINT: Decoded prompt for complexity (from batch item 0): '{prompt_text[:50]}...'\")\n",
        "\n",
        "                # Assuming compute_prompt_complexity_runtime and get_active_layers_for_prompt are accessible\n",
        "                # Also assuming self._adaptive_config and self._adaptive_mask are correctly set on the model instance\n",
        "                complexity_score = compute_prompt_complexity_runtime(\n",
        "                    prompt_text, self, tokenizer, self._adaptive_config\n",
        "                )\n",
        "                active_layers = get_active_layers_for_prompt(complexity_score, self._adaptive_config)\n",
        "                self._adaptive_mask.update_for_prompt(active_layers, complexity_score)\n",
        "\n",
        "                stats = self._adaptive_mask.get_stats()\n",
        "                print(f\"🎯 PRINT Prompt complexity: {complexity_score:.3f} | Active layers: {stats['active_layers']}/{stats['total_layers']} ({stats['usage_ratio']:.1%})\")\n",
        "                print(\"adaptive_model_forward (nested in add_auto) PRINT: Main logic block COMPLETED.\")\n",
        "            except Exception as e:\n",
        "                print(f\"adaptive_model_forward (nested in add_auto) PRINT: EXCEPTION in main logic block: {e}\")\n",
        "                import traceback # Import traceback here if not globally available\n",
        "                traceback.print_exc()\n",
        "        else:\n",
        "            print(\"adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\")\n",
        "\n",
        "        if not hasattr(self, '_original_forward'):\n",
        "            print(\"adaptive_model_forward (nested in add_auto) PRINT: ERROR _original_forward is MISSING on self!\")\n",
        "            raise RuntimeError(\"Cannot call missing _original_forward from adaptive_model_forward. Critical setup error.\")\n",
        "\n",
        "        # Always call _original_forward with the original arguments it received by the hook\n",
        "        return self._original_forward(input_ids=input_ids, **kwargs)\n",
        "    # CRITICAL: Assign the new forward method to the model instance\n",
        "    model.forward = adaptive_model_forward.__get__(model, type(model))\n",
        "    print(f\"add_automatic_complexity_computation PRINT: model.forward AFTER change is: {model.forward}\")\n",
        "    print(\"--- add_automatic_complexity_computation PRINT: EXITING ---\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "jnNs0YYX1yQz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_adaptive_attention_forward(original_forward, layer_idx: int, mask: LayerActivationMask,\n",
        "                                    architecture: str, model, tokenizer, config: Dict):\n",
        "    \"\"\"\n",
        "    Create a new forward method that respects the activation mask.\n",
        "    Layer 0 acts as automatic trigger for complexity calculation.\n",
        "    \"\"\"\n",
        "    def adaptive_forward(self, hidden_states, *args, **kwargs):\n",
        "        # Check if this layer should be active\n",
        "        is_active = mask.is_layer_active(layer_idx)\n",
        "        mask.log_layer_execution(layer_idx, is_active)\n",
        "\n",
        "        if is_active:\n",
        "            # Execute normal attention\n",
        "            result = original_forward(hidden_states, *args, **kwargs)\n",
        "            return result\n",
        "        else:\n",
        "            # Bypass attention\n",
        "            print(f\"--- Layer-level adaptive_forward PRINT: Bypassing Layer {layer_idx} ---\") # layer_idx from closure\n",
        "            use_cache_flag = kwargs.get('use_cache', False) # For logging\n",
        "            print(f\"Layer {layer_idx} bypass: use_cache={use_cache_flag}\")\n",
        "\n",
        "            # Always return a 2-tuple as per the ValueError (expected 2)\n",
        "            # and similar to potentially working static pruner.\n",
        "            print(f\"Layer {layer_idx} bypass: Now returning (hidden_states, None) (2-tuple)\")\n",
        "            return (hidden_states, None)\n",
        "\n",
        "    return adaptive_forward"
      ],
      "metadata": {
        "id": "EoWp630vnAnc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_adaptive_model(model, config: Dict, verbose: bool = True):\n",
        "    \"\"\"\n",
        "    Create an adaptive model that dynamically adjusts active layers based on prompt complexity.\n",
        "\n",
        "    Args:\n",
        "        model: The original transformer model\n",
        "        config: Adaptive configuration dictionary (from adaptive_config.json)\n",
        "        verbose: Whether to print setup information\n",
        "\n",
        "    Returns:\n",
        "        Modified model with adaptive attention bypass capability\n",
        "    \"\"\"\n",
        "    # Detect architecture\n",
        "    architecture = detect_model_architecture(model)\n",
        "    if verbose:\n",
        "        print(f\"🔍 Detected architecture: {architecture}\")\n",
        "\n",
        "    # Get attention layers\n",
        "    try:\n",
        "        attention_layers = get_attention_layers(model, architecture)\n",
        "        total_layers = len(attention_layers)\n",
        "        if verbose:\n",
        "            print(f\"📊 Found {total_layers} attention layers\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Failed to get attention layers: {e}\")\n",
        "\n",
        "    # Create activation mask\n",
        "    mask = LayerActivationMask(total_layers)\n",
        "\n",
        "    # Store references in model for access during inference\n",
        "    model._adaptive_mask = mask\n",
        "    model._adaptive_config = config\n",
        "    model._adaptive_architecture = architecture\n",
        "    model._adaptive_setup_complete = False\n",
        "\n",
        "    # Modify attention layers\n",
        "    modified_layers = 0\n",
        "    for layer_idx, layer in enumerate(attention_layers):\n",
        "        try:\n",
        "            attention_module = get_attention_module(layer, architecture)\n",
        "\n",
        "            # Store original forward if not already stored\n",
        "            if not hasattr(attention_module, '_original_forward'):\n",
        "                attention_module._original_forward = attention_module.forward\n",
        "\n",
        "            # Create adaptive forward method\n",
        "            adaptive_forward = create_adaptive_attention_forward(\n",
        "                attention_module._original_forward,\n",
        "                layer_idx,\n",
        "                mask,\n",
        "                architecture\n",
        "            )\n",
        "\n",
        "            # Replace forward method\n",
        "            attention_module.forward = adaptive_forward.__get__(attention_module, type(attention_module))\n",
        "            modified_layers += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to modify layer {layer_idx}: {e}\")\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"✅ Successfully modified {modified_layers}/{total_layers} attention layers\")\n",
        "        print(f\"🎯 Complexity thresholds: {config['complexity_thresholds']}\")\n",
        "        print(f\"⚡ Ready for adaptive inference!\")\n",
        "\n",
        "    # Add method to update mask (for debugging/manual control)\n",
        "    def update_adaptive_mask(complexity_score: float):\n",
        "        active_layers = get_active_layers_for_prompt(complexity_score, config)\n",
        "        mask.update_for_prompt(active_layers, complexity_score)\n",
        "        return mask.get_stats()\n",
        "\n",
        "    model.update_adaptive_mask = update_adaptive_mask\n",
        "    model.get_adaptive_stats = lambda: mask.get_stats()\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "6WZ1c--5nFvW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_adaptive_model_complete(model, tokenizer, config: Dict, verbose: bool = True):\n",
        "    \"\"\"\n",
        "    Complete setup of adaptive model with automatic complexity computation.\n",
        "    MODIFIED: Uses manual methods instead of automatic model.forward hooking.\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"🚀  Setting up Adaptive Attention Bypass (AAB) system...\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    # Step 1: Create adaptive model structure\n",
        "    adaptive_model = create_adaptive_model(model, config, verbose=verbose)\n",
        "\n",
        "\n",
        "    # Step 2: Add automatic complexity computation to hook into model.forward\n",
        "    adaptive_model = add_automatic_complexity_computation(adaptive_model, tokenizer)\n",
        "\n",
        "    # Step 2b: Add manual complexity methods, for testing.\n",
        "    #adaptive_model = add_manual_complexity_methods(adaptive_model, tokenizer, config)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"=\" * 60)\n",
        "        print(\"✅  AAB setup complete! Model ready for adaptive inference.\")\n",
        "        print(f\"📈  Usage will vary from {min(config['complexity_thresholds'].values())}\"\n",
        "              f\" to {max(config['complexity_thresholds'].values())} layers based on prompt complexity\")\n",
        "        print(\"\\n🔧  Available methods:\")\n",
        "        print(\"   • model.test_prompt(prompt) - Test end-to-end processing\")\n",
        "        print(\"   • model.get_debug_info() - Get comprehensive debug info\")\n",
        "        print(\"   • model.manual_complexity(prompt) - Calculate complexity manually\")\n",
        "        print(\"   • model.get_adaptive_stats() - Get current mask stats\")\n",
        "\n",
        "    return adaptive_model"
      ],
      "metadata": {
        "id": "3EPWU0uzsvIg"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_adaptive_model(model, config: Dict, verbose: bool = True):\n",
        "    \"\"\"\n",
        "    Create an adaptive model that dynamically adjusts active layers based on prompt complexity.\n",
        "    MODIFIED: Uses the new create_adaptive_attention_forward with layer 0 trigger.\n",
        "    \"\"\"\n",
        "    # Detect architecture\n",
        "    architecture = detect_model_architecture(model)\n",
        "    if verbose:\n",
        "        print(f\"🔍  Detected architecture: {architecture}\")\n",
        "\n",
        "    # Get attention layers\n",
        "    try:\n",
        "        attention_layers = get_attention_layers(model, architecture)\n",
        "        total_layers = len(attention_layers)\n",
        "        if verbose:\n",
        "            print(f\"📊  Found {total_layers} attention layers\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Failed to get attention layers: {e}\")\n",
        "\n",
        "    # Create activation mask\n",
        "    mask = LayerActivationMask(total_layers)\n",
        "\n",
        "    # Store references in model for access during inference\n",
        "    model._adaptive_mask = mask\n",
        "    model._adaptive_config = config\n",
        "    model._adaptive_architecture = architecture\n",
        "\n",
        "    # Modify attention layers (CHANGED to pass all parameters to create_adaptive_attention_forward)\n",
        "    modified_layers = 0\n",
        "    for layer_idx, layer in enumerate(attention_layers):\n",
        "        try:\n",
        "            attention_module = get_attention_module(layer, architecture)\n",
        "\n",
        "            # Store original forward if not already stored\n",
        "            if not hasattr(attention_module, '_original_forward'):\n",
        "                attention_module._original_forward = attention_module.forward\n",
        "\n",
        "            # Create adaptive forward method (CHANGED - now includes model, tokenizer, config)\n",
        "            adaptive_forward = create_adaptive_attention_forward(\n",
        "                attention_module._original_forward,\n",
        "                layer_idx,\n",
        "                mask,\n",
        "                architecture,\n",
        "                model,  # NEW\n",
        "                tokenizer,  # NEW (note: will be passed when called)\n",
        "                config  # NEW\n",
        "            )\n",
        "\n",
        "            # Replace forward method\n",
        "            attention_module.forward = adaptive_forward.__get__(attention_module, type(attention_module))\n",
        "            modified_layers += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to modify layer {layer_idx}: {e}\")\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"✅  Successfully modified {modified_layers}/{total_layers} attention layers\")\n",
        "        print(f\"🎯  Complexity thresholds: {config['complexity_thresholds']}\")\n",
        "        print(f\"⚡ Ready for adaptive inference with Layer 0 auto-trigger!\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "sIK5Co5J5Bs_"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: REPLACE your current setup function call\n",
        "print(\"🔄 Creating new adaptive model with Layer 0 trigger...\")\n",
        "adaptive_model = setup_adaptive_model_complete(model, tokenizer, adaptive_config, verbose=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f_PfgB0ninB",
        "outputId": "a8d98850-663e-44fc-c6be-e0fb889b61e9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Creating new adaptive model with Layer 0 trigger...\n",
            "🚀  Setting up Adaptive Attention Bypass (AAB) system...\n",
            "============================================================\n",
            "🔍  Detected architecture: llama\n",
            "📊  Found 28 attention layers\n",
            "✅  Successfully modified 28/28 attention layers\n",
            "🎯  Complexity thresholds: {0.1: 22, 0.3: 23, 0.5: 25, 0.7: 26, 0.9: 28}\n",
            "⚡ Ready for adaptive inference with Layer 0 auto-trigger!\n",
            "--- add_automatic_complexity_computation PRINT: ENTERED ---\n",
            "add_automatic_complexity_computation PRINT: model.forward BEFORE change is: <bound method LlamaForCausalLM.forward of LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 3072)\n",
            "    (layers): ModuleList(\n",
            "      (0-27): 28 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
            ")>\n",
            "add_automatic_complexity_computation PRINT: Stored _original_forward: <bound method LlamaForCausalLM.forward of LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 3072)\n",
            "    (layers): ModuleList(\n",
            "      (0-27): 28 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
            ")>\n",
            "add_automatic_complexity_computation PRINT: model.forward AFTER change is: <bound method add_automatic_complexity_computation.<locals>.adaptive_model_forward of LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 3072)\n",
            "    (layers): ModuleList(\n",
            "      (0-27): 28 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
            ")>\n",
            "--- add_automatic_complexity_computation PRINT: EXITING ---\n",
            "============================================================\n",
            "✅  AAB setup complete! Model ready for adaptive inference.\n",
            "📈  Usage will vary from 22 to 28 layers based on prompt complexity\n",
            "\n",
            "🔧  Available methods:\n",
            "   • model.test_prompt(prompt) - Test end-to-end processing\n",
            "   • model.get_debug_info() - Get comprehensive debug info\n",
            "   • model.manual_complexity(prompt) - Calculate complexity manually\n",
            "   • model.get_adaptive_stats() - Get current mask stats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"ID of adaptive_model after setup: {id(adaptive_model)}\")\n",
        "print(f\"adaptive_model.forward after setup: {adaptive_model.forward}\")"
      ],
      "metadata": {
        "id": "KY-B02hC_sw8",
        "outputId": "783a603b-b9b6-40ac-fa0a-2ed6b3ac38b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID of adaptive_model after setup: 138701811150416\n",
            "adaptive_model.forward after setup: <bound method add_automatic_complexity_computation.<locals>.adaptive_model_forward of LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 3072)\n",
            "    (layers): ModuleList(\n",
            "      (0-27): 28 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
            ")>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Move to device\n",
        "adaptive_model.to(device)"
      ],
      "metadata": {
        "id": "urM9Q1pB5TXk",
        "outputId": "f33f5495-30ac-439a-b69b-b9aa3406670a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 3072)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
              "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
              "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
              "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
              "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
              "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The model's forward method is: {adaptive_model.forward}\")\n",
        "if hasattr(adaptive_model, '_original_forward'):\n",
        "    print(f\"The model's _original_forward is: {adaptive_model._original_forward}\")\n",
        "else:\n",
        "    print(\"The model does NOT have an _original_forward attribute.\")"
      ],
      "metadata": {
        "id": "5ygZsk_K1oqB",
        "outputId": "03f094da-4de9-411b-f290-705f4529549c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model's forward method is: <bound method add_automatic_complexity_computation.<locals>.adaptive_model_forward of LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 3072)\n",
            "    (layers): ModuleList(\n",
            "      (0-27): 28 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
            ")>\n",
            "The model's _original_forward is: <bound method LlamaForCausalLM.forward of LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 3072)\n",
            "    (layers): ModuleList(\n",
            "      (0-27): 28 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
            ")>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n3️⃣ Testing original prompt with get_output:\")\n",
        "prompt = \"Paris is the capital of \"\n",
        "# The layer 0 trigger should work automatically during generate()\n",
        "print(f\"ID of adaptive_model before get_output: {id(adaptive_model)}\")\n",
        "print(f\"adaptive_model.forward before get_output: {adaptive_model.forward}\")\n",
        "generated = get_output(prompt, adaptive_model, num_runs=1)\n",
        "print(f\"Generated text: {generated}\")"
      ],
      "metadata": {
        "id": "OLL0gVF-5zIT",
        "outputId": "3997cfca-c57f-4317-fbc2-efa2374c6076",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3️⃣ Testing original prompt with get_output:\n",
            "ID of adaptive_model before get_output: 138701811150416\n",
            "adaptive_model.forward before get_output: <bound method add_automatic_complexity_computation.<locals>.adaptive_model_forward of LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 3072)\n",
            "    (layers): ModuleList(\n",
            "      (0-27): 28 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
            ")>\n",
            "--- get_output ENTERED. Prompt (first 30 chars): 'Paris is the capital of ...' ---\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 7])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 0\n",
            "adaptive_model_forward (nested in add_auto) PRINT: >>> Cache 'seen_tokens' is 0. is_effectively_first_pass = True.\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Effective first pass & valid inputs. Entering main logic block.\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Decoded prompt for complexity (from batch item 0): 'Paris is the capital of ...'\n",
            "🎯 PRINT Prompt complexity: 0.217 | Active layers: 23/28 (82.1%)\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Main logic block COMPLETED.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 7\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 8\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 9\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 10\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 11\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 12\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 13\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 14\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 15\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 16\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 17\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 18\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 19\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 20\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 21\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 22\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 23\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 24\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 25\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 26\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 27\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 28\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 29\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 30\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 31\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 32\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 33\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 34\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 35\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 36\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 37\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 38\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 39\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 40\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 41\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 42\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 43\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 44\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 45\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 46\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 47\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 48\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "Tokenization time: 0.71 ms\n",
            "Generation time: 2910.15 ms\n",
            "Decoding time: 0.20 ms\n",
            "Total time: 2911.15 ms\n",
            "Generated text: Paris is the capital of  France    france   France         capital city of France located in France's central-central region Paris         paris         capital city located on the river River River   france france France’s largest river\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n4 Testing a second prompt with get_output:\")\n",
        "prompt = \"Hello\"\n",
        "# The layer 0 trigger should work automatically during generate()\n",
        "print(f\"ID of adaptive_model before get_output: {id(adaptive_model)}\")\n",
        "print(f\"adaptive_model.forward before get_output: {adaptive_model.forward}\")\n",
        "generated = get_output(prompt, adaptive_model, num_runs=1)\n",
        "print(f\"Generated text: {generated}\")"
      ],
      "metadata": {
        "id": "Ze5wviiwGV-C",
        "outputId": "eaa342bd-040e-4fc7-d722-c61a5782174e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4 Testing a second prompt with get_output:\n",
            "ID of adaptive_model before get_output: 138701811150416\n",
            "adaptive_model.forward before get_output: <bound method add_automatic_complexity_computation.<locals>.adaptive_model_forward of LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 3072)\n",
            "    (layers): ModuleList(\n",
            "      (0-27): 28 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
            ")>\n",
            "--- get_output ENTERED. Prompt (first 30 chars): 'Hello...' ---\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 2])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 0\n",
            "adaptive_model_forward (nested in add_auto) PRINT: >>> Cache 'seen_tokens' is 0. is_effectively_first_pass = True.\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Effective first pass & valid inputs. Entering main logic block.\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Decoded prompt for complexity (from batch item 0): 'Hello...'\n",
            "🎯 PRINT Prompt complexity: 0.000 | Active layers: 22/28 (78.6%)\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Main logic block COMPLETED.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 2\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 3\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 4\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 5\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 6\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 7\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 8\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 9\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 10\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 11\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 12\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 13\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 14\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 15\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 16\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 17\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 18\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 19\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 20\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 21\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 22\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 23\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 24\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 25\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 26\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 27\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 28\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 29\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 30\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 31\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 32\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 33\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 34\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 35\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 36\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 37\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 38\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 39\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 40\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 41\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 42\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 43\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 44\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 45\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 46\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 47\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 48\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 16 ---\n",
            "Layer 16 bypass: use_cache=True\n",
            "Layer 16 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 19 ---\n",
            "Layer 19 bypass: use_cache=True\n",
            "Layer 19 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 20 ---\n",
            "Layer 20 bypass: use_cache=True\n",
            "Layer 20 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 22 ---\n",
            "Layer 22 bypass: use_cache=True\n",
            "Layer 22 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 23 ---\n",
            "Layer 23 bypass: use_cache=True\n",
            "Layer 23 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "--- Layer-level adaptive_forward PRINT: Bypassing Layer 26 ---\n",
            "Layer 26 bypass: use_cache=True\n",
            "Layer 26 bypass: Now returning (hidden_states, None) (2-tuple)\n",
            "Tokenization time: 0.97 ms\n",
            "Generation time: 3101.34 ms\n",
            "Decoding time: 0.21 ms\n",
            "Total time: 3102.55 ms\n",
            "Generated text: Helloooooooooooooooooo!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Helloooooooooo! hellooooooo helloooo hellooo Helloooo Hello hello hello welcome welcome Welcome Welcome welcomewelcome Welcomewelcomewelcome welcomeWelcome welcome hello Hello welcome\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n5 Testing a third prompt with get_output:\")\n",
        "prompt = \"Compare and contrast the economic policies of Keynesian and Austrian schools of thought, analyzing their effectiveness during different historical periods and explaining which approach would be most suitable for addressing current global economic challenges.\"\n",
        "# The layer 0 trigger should work automatically during generate()\n",
        "print(f\"ID of adaptive_model before get_output: {id(adaptive_model)}\")\n",
        "print(f\"adaptive_model.forward before get_output: {adaptive_model.forward}\")\n",
        "generated = get_output(prompt, adaptive_model, num_runs=1)\n",
        "print(f\"Generated text: {generated}\")"
      ],
      "metadata": {
        "id": "yeRwOAGoG-LM",
        "outputId": "1c1ae0d7-2b52-456d-b759-5d43e274c9e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5 Testing a third prompt with get_output:\n",
            "ID of adaptive_model before get_output: 138701811150416\n",
            "adaptive_model.forward before get_output: <bound method add_automatic_complexity_computation.<locals>.adaptive_model_forward of LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 3072)\n",
            "    (layers): ModuleList(\n",
            "      (0-27): 28 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
            ")>\n",
            "--- get_output ENTERED. Prompt (first 30 chars): 'Compare and contrast the econo...' ---\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 38])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 0\n",
            "adaptive_model_forward (nested in add_auto) PRINT: >>> Cache 'seen_tokens' is 0. is_effectively_first_pass = True.\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Effective first pass & valid inputs. Entering main logic block.\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Decoded prompt for complexity (from batch item 0): 'Compare and contrast the economic policies of Keyn...'\n",
            "🎯 PRINT Prompt complexity: 0.797 | Active layers: 28/28 (100.0%)\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Main logic block COMPLETED.\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 38\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 39\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 40\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 41\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 42\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 43\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 44\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 45\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 46\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 47\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "--- adaptive_model_forward (nested in add_auto) PRINT: ENTERED ---\n",
            "adaptive_model_forward (nested in add_auto) PRINT: 'input_ids' provided directly. Shape: torch.Size([5, 1])\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Initial past_key_values is None: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Detected Cache object with 'seen_tokens'. Type: <class 'transformers.cache_utils.DynamicCache'>. seen_tokens: 48\n",
            "adaptive_model_forward (nested in add_auto) PRINT: FINAL is_effectively_first_pass: False\n",
            "adaptive_model_forward (nested in add_auto) PRINT: can_get_prompt_for_complexity: True\n",
            "adaptive_model_forward (nested in add_auto) PRINT: Not effective first pass for complexity calc OR inputs invalid. Skipping main logic block.\n",
            "Tokenization time: 0.74 ms\n",
            "Generation time: 925.21 ms\n",
            "Decoding time: 0.21 ms\n",
            "Total time: 926.19 ms\n",
            "Generated text: Compare and contrast the economic policies of Keynesian and Austrian schools of thought, analyzing their effectiveness during different historical periods and explaining which approach would be most suitable for addressing current global economic challenges. Provide specific examples to support your argument.\n",
            "Keynesian economics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manual testing."
      ],
      "metadata": {
        "id": "ulydyCWk1cvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 1: Simple prompt (should use fewer layers)\n",
        "print(\"\\n1️⃣ Testing simple prompt:\")\n",
        "simple_result = adaptive_model.test_prompt(\"Hi\", verbose=True)\n"
      ],
      "metadata": {
        "id": "H9gnriZb5iPS",
        "outputId": "282f1e0e-f56f-4c89-8390-9ebb3bf8a687",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1️⃣ Testing simple prompt:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'LlamaForCausalLM' object has no attribute 'test_prompt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-863ff356565c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test 1: Simple prompt (should use fewer layers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n1️⃣ Testing simple prompt:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msimple_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madaptive_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1928\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1929\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'test_prompt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 1: Simple prompt (should use fewer layers)\n",
        "print(\"\\n1️⃣ Paris is the capital of \")\n",
        "simple_result = adaptive_model.test_prompt(\"Paris is the capital of \", verbose=True)\n"
      ],
      "metadata": {
        "id": "2ZdVC6RVBjhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 2: Complex prompt (should use more layers)\n",
        "print(\"\\n2️⃣ Testing complex prompt:\")\n",
        "complex_result = adaptive_model.test_prompt(\n",
        "    \"Analyze the geopolitical implications of quantum computing on global cybersecurity frameworks\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "8qeCXL085rfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what happened\n",
        "stats_after = adaptive_model.get_adaptive_stats()\n",
        "exec_stats = adaptive_model._adaptive_mask.get_execution_stats()"
      ],
      "metadata": {
        "id": "wqQOu_ya6CR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n📊 Automatic execution results:\")\n",
        "print(f\"   Active layers: {stats_after['active_layers']}/{stats_after['total_layers']}\")\n",
        "print(f\"   Usage: {stats_after['usage_ratio']:.1%}\")\n",
        "print(f\"   Layers executed: {exec_stats['layers_executed']}\")\n",
        "print(f\"   Layers bypassed: {exec_stats['layers_bypassed']}\")\n",
        "print(f\"   Execution matches mask: {exec_stats['execution_matches_mask']}\")\n"
      ],
      "metadata": {
        "id": "_F1qkAWt6G7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 4: Comprehensive debug info\n",
        "print(\"\\n4️⃣ Full debug info:\")\n",
        "debug_info = adaptive_model.get_debug_info()\n",
        "print(f\"   Mask stats: {debug_info['mask_stats']}\")\n",
        "print(f\"   Total execution calls: {debug_info['execution_stats']['total_calls']}\")\n",
        "print(f\"   Most important layers: {debug_info['layers_by_importance']}\")"
      ],
      "metadata": {
        "id": "2FjZSoLc6b4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 6: Verify different complexity levels work\n",
        "print(\"\\n6️⃣ Testing different complexity levels:\")\n",
        "test_prompts = [\n",
        "    (\"Simple\", \"2+2=\"),\n",
        "    (\"Medium\", \"Explain machine learning basics\"),\n",
        "    (\"Complex\", \"Write a comprehensive analysis of the economic implications of artificial intelligence\")\n",
        "]\n",
        "\n",
        "for level, test_prompt in test_prompts:\n",
        "    print(f\"\\n{level}: '{test_prompt[:50]}{'...' if len(test_prompt) > 50 else ''}'\")\n",
        "    result = adaptive_model.test_prompt(test_prompt, verbose=False)\n",
        "    print(f\"   Complexity: {result['complexity']:.3f}\")\n",
        "    print(f\"   Layers: {result['mask_stats']['active_layers']}/{result['mask_stats']['total_layers']} \"\n",
        "          f\"({result['mask_stats']['usage_ratio']:.1%})\")\n",
        "    print(f\"   Executed: {len(result['execution_stats']['layers_executed'])}, \"\n",
        "          f\"Bypassed: {len(result['execution_stats']['layers_bypassed'])}\")"
      ],
      "metadata": {
        "id": "xzENonZe6mRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MtXvjUS-57ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated = get_output(prompt, adaptive_model, num_runs=2)\n",
        "print(f\"Generated text: {generated}\")"
      ],
      "metadata": {
        "id": "6uT1_B5PvaQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar inmediatamente después\n",
        "stats_after_get_output = adaptive_model.get_adaptive_stats()\n",
        "print(\"Complexity after get_output:\", stats_after_get_output['complexity_score'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGlD4ZYRvdug",
        "outputId": "9fb5bea3-f51d-4791-9604-6989a1bd859d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complexity after get_output: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Check what happened\n",
        "stats = adaptive_model.get_adaptive_stats()\n",
        "print(f\"Complexity: {stats['complexity_score']:.3f}, Used {stats['active_layers']}/{stats['total_layers']} layers\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "QZLxgTMhnkAo",
        "outputId": "5d5d8231-0665-4ceb-ca11-91b53cadc338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported format string passed to NoneType.__format__",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-4ebb08d1743f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Optional: Check what happened\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madaptive_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_adaptive_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Complexity: {stats['complexity_score']:.3f}, Used {stats['active_layers']}/{stats['total_layers']} layers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to NoneType.__format__"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### complementary tests"
      ],
      "metadata": {
        "id": "rpQ1ykbeux1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_complexity = compute_prompt_complexity_runtime(\"Paris is the capital of\", adaptive_model, tokenizer, adaptive_config)\n",
        "print(f\"Manual complexity test: {test_complexity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfM6E-PKp33c",
        "outputId": "6250bef9-5f4e-4202-f018-9ade8c3e0c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual complexity test: 0.17870073644873735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original forward?\", hasattr(adaptive_model, '_original_forward'))\n",
        "print(\"Forward actual:\", type(adaptive_model.forward))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdoGcMM5tmUj",
        "outputId": "f975b84c-d396-4f54-f5bc-d44921eaf2b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original forward? True\n",
            "Forward actual: <class 'method'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Llamar directamente al forward (sin generate)\n",
        "inputs = tokenizer(\"Paris is the capital of\", return_tensors='pt').to(device)\n",
        "try:\n",
        "    result = adaptive_model.forward(input_ids=inputs['input_ids'])\n",
        "    print(\"Manual forward call worked\")\n",
        "except Exception as e:\n",
        "    print(f\"Manual forward failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjsXiQCMtuEr",
        "outputId": "f7c22dac-ca3c-47c3-9bd3-0090458b7f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual forward call worked\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver si generate() usa forward() o algo diferente\n",
        "print(\"Generate method:\", adaptive_model.generate.__func__.__name__)\n",
        "print(\"Model class:\", type(adaptive_model).__name__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmgrBBtptxzE",
        "outputId": "5c83e7d9-3230-45c8-e24a-e4a2f34959bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate method: generate\n",
            "Model class: LlamaForCausalLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Después de la llamada manual anterior, verificar si se calculó complejidad\n",
        "stats_after_manual = adaptive_model.get_adaptive_stats()\n",
        "print(\"Stats after manual forward:\", stats_after_manual)\n",
        "print(\"Complexity calculated?\", stats_after_manual['initialized'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "EwlEcsIfuQ6W",
        "outputId": "163953fb-4a4b-47be-d7b5-93a2f069feb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stats after manual forward: {'complexity_score': 0.17870073644873735, 'active_layers': 21, 'total_layers': 28, 'usage_ratio': 0.75}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'initialized'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-8c9996c33ac9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstats_after_manual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madaptive_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_adaptive_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Stats after manual forward:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_after_manual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Complexity calculated?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_after_manual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'initialized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'initialized'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resetear el sistema y probar manual de nuevo\n",
        "inputs = tokenizer(\"Paris is the capital of\", return_tensors='pt').to(device)\n",
        "print(\"=== Manual forward call ===\")\n",
        "result = adaptive_model.forward(input_ids=inputs['input_ids'])\n",
        "print(\"=== End manual call ===\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46F5MklsuZfd",
        "outputId": "de5ab7c1-b602-4b68-d1ca-681cc66edf56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Manual forward call ===\n",
            "=== End manual call ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"Paris is the capital of\", return_tensors='pt').to(device)\n",
        "result = adaptive_model.forward(input_ids=inputs['input_ids'])"
      ],
      "metadata": {
        "id": "-ppE-2IPurn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats_after_manual = adaptive_model.get_adaptive_stats()\n",
        "print(\"Complexity after manual:\", stats_after_manual['complexity_score'])\n",
        "print(\"Initialized?\", stats_after_manual['initialized'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "rb3SxMpqn2Mj",
        "outputId": "b5d1cf24-a112-4428-98f1-91c008d20e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complexity after manual: 0.17870073644873735\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'initialized'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-f9cde5088f55>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstats_after_manual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madaptive_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_adaptive_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Complexity after manual:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_after_manual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'complexity_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initialized?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_after_manual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'initialized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'initialized'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resetear el sistema para el test\n",
        "adaptive_model._adaptive_mask.current_complexity = None"
      ],
      "metadata": {
        "id": "ykZKBzG1u6e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_vWNC2YSvXB3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "authorship_tag": "ABX9TyNhc7TtN1M4mKPsUHghGZph",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ed048f20f57f4a78bac5614b69091364": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a49bf4fe7b4d4b5b90f0bbe93835913b",
              "IPY_MODEL_9ef94cd53439420eafc890152c34a5bd",
              "IPY_MODEL_5214eac789784b09bf4aa1805481fcde"
            ],
            "layout": "IPY_MODEL_71c5d0f5acfc49f8981a902d5f818da5"
          }
        },
        "a49bf4fe7b4d4b5b90f0bbe93835913b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ce64f6d8f9e44eea165bd1ac0061f54",
            "placeholder": "​",
            "style": "IPY_MODEL_35f5d7e08f674e868e0458cbf6631d08",
            "value": "config.json: 100%"
          }
        },
        "9ef94cd53439420eafc890152c34a5bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52729eb860a64aa4a311c2e27483d622",
            "max": 844,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee5359a655594d45b38a926a5655a854",
            "value": 844
          }
        },
        "5214eac789784b09bf4aa1805481fcde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4e627d3959c48d080ba621d2c56b0b2",
            "placeholder": "​",
            "style": "IPY_MODEL_2f5e185dd6cc4cdba9ea183a64666f2a",
            "value": " 844/844 [00:00&lt;00:00, 97.3kB/s]"
          }
        },
        "71c5d0f5acfc49f8981a902d5f818da5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ce64f6d8f9e44eea165bd1ac0061f54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35f5d7e08f674e868e0458cbf6631d08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52729eb860a64aa4a311c2e27483d622": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee5359a655594d45b38a926a5655a854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4e627d3959c48d080ba621d2c56b0b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f5e185dd6cc4cdba9ea183a64666f2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64937842c8464a02b9ba7ded491c90ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e4e05164ddf4f96a90c1ba9fd4f72ef",
              "IPY_MODEL_df6e928928b74cabacca4eb87cecf951",
              "IPY_MODEL_a29e9debeda54cf2878b2874bf92690b"
            ],
            "layout": "IPY_MODEL_3224fdf91ef54bea8a6af2cc27ccce9c"
          }
        },
        "3e4e05164ddf4f96a90c1ba9fd4f72ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac3d2ef2a4c740c98c1119039395cd1a",
            "placeholder": "​",
            "style": "IPY_MODEL_70bfad28ac184978bfc148cffa68a15a",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "df6e928928b74cabacca4eb87cecf951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed253ea3fe5f4330be7b963e69496f02",
            "max": 20919,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f10fc071c47a409aae7fa6eba8e85751",
            "value": 20919
          }
        },
        "a29e9debeda54cf2878b2874bf92690b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c3a627e696d4106a2fa48145a4f03a8",
            "placeholder": "​",
            "style": "IPY_MODEL_543860f9b187473b875d1a370b25d3ae",
            "value": " 20.9k/20.9k [00:00&lt;00:00, 2.26MB/s]"
          }
        },
        "3224fdf91ef54bea8a6af2cc27ccce9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac3d2ef2a4c740c98c1119039395cd1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70bfad28ac184978bfc148cffa68a15a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed253ea3fe5f4330be7b963e69496f02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f10fc071c47a409aae7fa6eba8e85751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c3a627e696d4106a2fa48145a4f03a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "543860f9b187473b875d1a370b25d3ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4880bca6c204fdb8c4a2de74f456981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea0e702086334424aca2b833775dc5f0",
              "IPY_MODEL_737122a53d6d448c8a986ed6c345d6d1",
              "IPY_MODEL_399601d0a0f9495faa0454a49ca0f903"
            ],
            "layout": "IPY_MODEL_210715dba0bb43918a0676df2a2953cc"
          }
        },
        "ea0e702086334424aca2b833775dc5f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcde46654d76489bb8c05fbbdc2bf8ed",
            "placeholder": "​",
            "style": "IPY_MODEL_763c179aee784c708d1854a37b33689a",
            "value": "Fetching 2 files: 100%"
          }
        },
        "737122a53d6d448c8a986ed6c345d6d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09677d68436e4dacaaed1e457a7eff08",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1f33c6eebf84999b0a8ffb9f8d534b9",
            "value": 2
          }
        },
        "399601d0a0f9495faa0454a49ca0f903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4edc11164344eeebd7c544df3a40984",
            "placeholder": "​",
            "style": "IPY_MODEL_3f0193272c644e94b526e7ad2086bc3c",
            "value": " 2/2 [00:43&lt;00:00, 43.42s/it]"
          }
        },
        "210715dba0bb43918a0676df2a2953cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcde46654d76489bb8c05fbbdc2bf8ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "763c179aee784c708d1854a37b33689a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09677d68436e4dacaaed1e457a7eff08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1f33c6eebf84999b0a8ffb9f8d534b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4edc11164344eeebd7c544df3a40984": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f0193272c644e94b526e7ad2086bc3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58131c98f90e4cb98164d7e6973972c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45a07be96ad24b4f82ac65a8ae30b2c8",
              "IPY_MODEL_b4660425744b4d968cbb7f0e70e44a79",
              "IPY_MODEL_4c9606d971c347e7a2fffa82fdc425cf"
            ],
            "layout": "IPY_MODEL_f73eb82d060e4b0ea47b49a532f375e6"
          }
        },
        "45a07be96ad24b4f82ac65a8ae30b2c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1121fdb2fe044a4e9bedfd88bd5108e3",
            "placeholder": "​",
            "style": "IPY_MODEL_84b2b84bbf5644cda2ba872a48d463c4",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "b4660425744b4d968cbb7f0e70e44a79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d475a0bfa8645f5b7ce9be1131aa815",
            "max": 4965799096,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c82fa09a07048a6b07d34cac5a34929",
            "value": 4965799096
          }
        },
        "4c9606d971c347e7a2fffa82fdc425cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50cbee306cd540a198dc0da3fb29c1f7",
            "placeholder": "​",
            "style": "IPY_MODEL_bf85cc61649b4efbbf50ee174d4b6f88",
            "value": " 4.97G/4.97G [00:42&lt;00:00, 287MB/s]"
          }
        },
        "f73eb82d060e4b0ea47b49a532f375e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1121fdb2fe044a4e9bedfd88bd5108e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84b2b84bbf5644cda2ba872a48d463c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d475a0bfa8645f5b7ce9be1131aa815": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c82fa09a07048a6b07d34cac5a34929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50cbee306cd540a198dc0da3fb29c1f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf85cc61649b4efbbf50ee174d4b6f88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "462b94ad53f044c989ab8013570d72ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b078ed813d23486ca2c15275e92d8f8c",
              "IPY_MODEL_46b143107d924085b8a5d0d61da9ee3a",
              "IPY_MODEL_b3c1fe672d864454bc108c93d343c064"
            ],
            "layout": "IPY_MODEL_472aaec536f747a7afa2a845d1b58055"
          }
        },
        "b078ed813d23486ca2c15275e92d8f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71664fe690e64526bd943492b9612baf",
            "placeholder": "​",
            "style": "IPY_MODEL_1ac198f661634c7b8026a0695a8ccd45",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "46b143107d924085b8a5d0d61da9ee3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ece7f18184334e338a17b54adf8357cb",
            "max": 1459729952,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b76ae2dca55e44a7ae875772b0f0df1f",
            "value": 1459729952
          }
        },
        "b3c1fe672d864454bc108c93d343c064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d78800476064b3b959677c39dc33319",
            "placeholder": "​",
            "style": "IPY_MODEL_61af0d860f8847b782819a08fde17d3f",
            "value": " 1.46G/1.46G [00:16&lt;00:00, 105MB/s]"
          }
        },
        "472aaec536f747a7afa2a845d1b58055": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71664fe690e64526bd943492b9612baf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ac198f661634c7b8026a0695a8ccd45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ece7f18184334e338a17b54adf8357cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b76ae2dca55e44a7ae875772b0f0df1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d78800476064b3b959677c39dc33319": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61af0d860f8847b782819a08fde17d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9b61d4bdf5c4c069ead9d11352db706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eff34fe93fba4994b1d5a0792e72c77f",
              "IPY_MODEL_2b1a6491c33c4c29a3388ee3b696da14",
              "IPY_MODEL_762cd7f670f14030a3407f326380299b"
            ],
            "layout": "IPY_MODEL_9a7177b3ccff4e4e8e3b0c578ad9a7e3"
          }
        },
        "eff34fe93fba4994b1d5a0792e72c77f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_927a3ecf1d8d4c41b01c035b0cf4d083",
            "placeholder": "​",
            "style": "IPY_MODEL_ca354c7f5b854f65b0532a1e07ff49e8",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2b1a6491c33c4c29a3388ee3b696da14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa8343d725924fcebad159e31c492667",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d3fcd116dbad4cdbab99d046c3e5206a",
            "value": 2
          }
        },
        "762cd7f670f14030a3407f326380299b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0301b4a59acf43499b95d53ec44f0988",
            "placeholder": "​",
            "style": "IPY_MODEL_50a33a132f434c78a05a59beadd80e44",
            "value": " 2/2 [00:02&lt;00:00,  1.07s/it]"
          }
        },
        "9a7177b3ccff4e4e8e3b0c578ad9a7e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "927a3ecf1d8d4c41b01c035b0cf4d083": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca354c7f5b854f65b0532a1e07ff49e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa8343d725924fcebad159e31c492667": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3fcd116dbad4cdbab99d046c3e5206a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0301b4a59acf43499b95d53ec44f0988": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50a33a132f434c78a05a59beadd80e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "201460dafe95486fb8641e1c0a34b4ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b2ae29912714c12847d7085214f6d8e",
              "IPY_MODEL_5edcb7f768c6473da74b2de175f6440e",
              "IPY_MODEL_3c29fab5a72b4848aadffdc5b086519f"
            ],
            "layout": "IPY_MODEL_4ea4331781484308b3710f8658dc6dc8"
          }
        },
        "6b2ae29912714c12847d7085214f6d8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1381136601244ec197ec556345ff7937",
            "placeholder": "​",
            "style": "IPY_MODEL_52572e9a631c4fdda5678029b955df18",
            "value": "generation_config.json: 100%"
          }
        },
        "5edcb7f768c6473da74b2de175f6440e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4a9483c29ff4bc5ac9fc9336fb9a818",
            "max": 185,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_beed8970dc3b43fe9750c43368c962c4",
            "value": 185
          }
        },
        "3c29fab5a72b4848aadffdc5b086519f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e77b231ee1a4b96878bd71bb28f5e2f",
            "placeholder": "​",
            "style": "IPY_MODEL_5584646b81444eedb6dc64159d1e54dd",
            "value": " 185/185 [00:00&lt;00:00, 23.7kB/s]"
          }
        },
        "4ea4331781484308b3710f8658dc6dc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1381136601244ec197ec556345ff7937": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52572e9a631c4fdda5678029b955df18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4a9483c29ff4bc5ac9fc9336fb9a818": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "beed8970dc3b43fe9750c43368c962c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e77b231ee1a4b96878bd71bb28f5e2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5584646b81444eedb6dc64159d1e54dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6d4232f54964adab7c8a481fdeb2bfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_888117fa708846cda5cf8045a66f9e00",
              "IPY_MODEL_03089edcc4f74e6cb9e79e9618ef4fa5",
              "IPY_MODEL_8a1a52f1145e42f0985d2909d5ed4d38"
            ],
            "layout": "IPY_MODEL_8dc6f798f81045afb298415894ebaa2d"
          }
        },
        "888117fa708846cda5cf8045a66f9e00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ea4805a1fcc4b819d9708510d41d588",
            "placeholder": "​",
            "style": "IPY_MODEL_dbe84ffe4f8444d4b118a200ab3a4863",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "03089edcc4f74e6cb9e79e9618ef4fa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a3d3d25df93417abce11554e9405dd5",
            "max": 50500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9871c0d1b734a5489d5160db3e45cbf",
            "value": 50500
          }
        },
        "8a1a52f1145e42f0985d2909d5ed4d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dab9e9024344daba5d693d4abd1d108",
            "placeholder": "​",
            "style": "IPY_MODEL_44bee7b273b743358f5624166df1272a",
            "value": " 50.5k/50.5k [00:00&lt;00:00, 5.88MB/s]"
          }
        },
        "8dc6f798f81045afb298415894ebaa2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ea4805a1fcc4b819d9708510d41d588": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbe84ffe4f8444d4b118a200ab3a4863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a3d3d25df93417abce11554e9405dd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9871c0d1b734a5489d5160db3e45cbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6dab9e9024344daba5d693d4abd1d108": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44bee7b273b743358f5624166df1272a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbf5ba2cf5bc46b7a31ea8b607094a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de9fef56974f473e8a19f296514ef774",
              "IPY_MODEL_467d0b4a4e244f38b73d4fdf5326da71",
              "IPY_MODEL_dd92ab4a0a0748f89c4f57fa34900806"
            ],
            "layout": "IPY_MODEL_81768475fdfd4eae88a4f18d771cb568"
          }
        },
        "de9fef56974f473e8a19f296514ef774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03dbdaa422734e5fbe3997053d54559a",
            "placeholder": "​",
            "style": "IPY_MODEL_2f7f01ed09af42109c9dc1c7778c2bc4",
            "value": "tokenizer.json: 100%"
          }
        },
        "467d0b4a4e244f38b73d4fdf5326da71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_494ee21fc7e244c4bdc5b91eba8b7f3d",
            "max": 9085657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f84580d900e4014b65c8452ec795040",
            "value": 9085657
          }
        },
        "dd92ab4a0a0748f89c4f57fa34900806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8f678b2b7214e1691379934eea19cc9",
            "placeholder": "​",
            "style": "IPY_MODEL_adb660a666cb4edaa516c98181f91e50",
            "value": " 9.09M/9.09M [00:01&lt;00:00, 5.84MB/s]"
          }
        },
        "81768475fdfd4eae88a4f18d771cb568": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03dbdaa422734e5fbe3997053d54559a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f7f01ed09af42109c9dc1c7778c2bc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "494ee21fc7e244c4bdc5b91eba8b7f3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f84580d900e4014b65c8452ec795040": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8f678b2b7214e1691379934eea19cc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adb660a666cb4edaa516c98181f91e50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23a5f13bcbef403d893149a5522e847c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d0724f750424a409a66bdbaf80ac658",
              "IPY_MODEL_84fb76436f5c46c4917719f09a1c984b",
              "IPY_MODEL_aa7d835428094168bfc67e94a5e8d2d2"
            ],
            "layout": "IPY_MODEL_87f91558ca9446b48e33bbd2ebcfb382"
          }
        },
        "2d0724f750424a409a66bdbaf80ac658": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00372be8fc694339a0ff04e137c43c76",
            "placeholder": "​",
            "style": "IPY_MODEL_6135ea9cc8714408bba914d4025fe87c",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "84fb76436f5c46c4917719f09a1c984b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09c986f0f8da4b99919a14c1897e7daf",
            "max": 301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75a8b99bbf2b48df83af0d24b772bf1a",
            "value": 301
          }
        },
        "aa7d835428094168bfc67e94a5e8d2d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a9b50368c2f4aabacf4460984b1728d",
            "placeholder": "​",
            "style": "IPY_MODEL_5f1d0bc5300a4f48800567ae28560562",
            "value": " 301/301 [00:00&lt;00:00, 39.9kB/s]"
          }
        },
        "87f91558ca9446b48e33bbd2ebcfb382": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00372be8fc694339a0ff04e137c43c76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6135ea9cc8714408bba914d4025fe87c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09c986f0f8da4b99919a14c1897e7daf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75a8b99bbf2b48df83af0d24b772bf1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a9b50368c2f4aabacf4460984b1728d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f1d0bc5300a4f48800567ae28560562": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}