{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Large-Language-Model-Notebooks-Course/blob/inference-adaptative-attention-pruning/6-PRUNING/6_6b_Adaptive_Inference_Attention_Pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV5KPbfseL8S"
      },
      "source": [
        "<div>\n",
        "    <h1>Large Language Models Projects</a></h1>\n",
        "    <h3>Apply and Implement Strategies for Large Language Models</h3>\n",
        "    <h2>Pruning Attention Layers</h2>\n",
        "    <h3>Not All Attention is needed</h3>\n",
        "</div>\n",
        "\n",
        "by [Pere Martra](https://www.linkedin.com/in/pere-martra/)\n",
        "\n",
        "_______\n",
        "Models: meta-llama/Llama-3.2\n",
        "\n",
        "Colab Environment: GPU L4 for 3B Models\n",
        "\n",
        "T4 for 1B Model.\n",
        "\n",
        "Keys:\n",
        "* Pruning\n",
        "* Attention\n",
        "\n",
        "References:\n",
        "* [What Matters in Transformers? Not All Attention is Needed](https://arxiv.org/abs/2406.15786)\n",
        "* [Resource-Efficient Transformer Pruning for Finetuning of Large Models](https://openaccess.thecvf.com/content/CVPR2024/html/Ilhan_Resource-Efficient_Transformer_Pruning_for_Finetuning_of_Large_Models_CVPR_2024_paper.html)\n",
        "\n",
        "_______\n",
        "**disclaimer: The pruning / knowledge distillation section has been created after the first edition of the book was published. They are not included in the book’s original content but are intended to supplement and expand on the topics covered.**\n",
        "\n",
        "This is the unofficial repository for the book:\n",
        "        <a href=\"https://amzn.to/4eanT1g\"> <b>Large Language Models:</b> Apply and Implement Strategies for Large Language Models</a> (Apress).\n",
        "        The book is based on the content of this repository, but the notebooks are being updated, and I am incorporating new examples and chapters.\n",
        "        If you are looking for the official repository for the book, with the original notebooks, you should visit the\n",
        "        <a href=\"https://github.com/Apress/Large-Language-Models-Projects\">Apress repository</a>, where you can find all the notebooks in their original format as they appear in the book.\n",
        "\n",
        "______\n",
        "# Introduction\n",
        "This notebook implements the paper: [What Matters in Transformers? Not all Attention is Needed](https://arxiv.org/abs/2406.15786).\n",
        "Although I followed the paper's guidelines, I made some adjustments to make the code clearer and easier to understand.\n",
        "\n",
        "The original paper demonstrates that larger models tend to have excessive redundancy in their attention layers. They achieved a 48.4% increase in inference performance for a Llama-2-70B model with only a minor 2.4% drop in response quality, **just bypassing the 50% of the Attention layers!**\n",
        "\n",
        "In this notebook, tests have been conducted using Llama-3.2-1B and 3B models. With these models, I found that removing 50% of the attention layers significantly impacted the model's functionality. However, the 3B model handled the removal of these layers much better. This suggests that redundancy may become more pronounced as model size increases.\n",
        "\n",
        "# Methodology.\n",
        "To identify which layers contribute the least, the cosine distance between the layer's input and output is measured. In the paper, this distance is calculated using a test dataset, while in the notebook, I used a simple prompt to activate the layers.  \n",
        "\n",
        "This method of measuring the importance of attention layers and their contribution to the model allows pruning to be tailored to a specific dataset. This approach can lead to the creation of more efficient models for specialized sectors such as healthcare or finance.\n",
        "\n",
        "\n",
        "Once the layer contributing the least to the final output is identified (the one with the smallest difference between input and output), it is added to a list included in the configuration file.\n",
        "\n",
        "This list is then referenced during inference by a new forward function that replaces the original one for attention layers. When this new function detects that a layer is in the list, it skips its execution and simply returns the input without modifications.\n",
        "\n",
        "The process of identifying layers to deactivate and marking them as non-executable is one-shot. In other words, it does  determine all the layers to skip in one go, as recommended in the paper.\n",
        "\n",
        "The iterative implementation has a significant drawback: the test dataset must be processed for each layer to be deactivated. The paper's authors note that while the iterative method may bring slight improvements, the added computational cost is not justified. However, since this is an example notebook, and there is no test dataset—just a small prompt—and the layer selection process takes only seconds, I chose the iterative approach.\n",
        "\n",
        "This pruning method does not produce a smaller model, as the layers are not physically removed. They remain in the model but are not executed, resulting in improved inference response times.\n",
        "______"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwYeKwswnkTG"
      },
      "source": [
        "# Install libraries & Configure variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PblPrYCiYTl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f8d8eb5-30c2-46f2-d97a-6c1a0dbe9223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hf_xet\n",
            "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
            "Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf_xet\n",
            "Successfully installed hf_xet-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch==2.6.0\n",
        "!pip install -q torchvision==0.21.0\n",
        "!pip install -q transformers==4.51.3\n",
        "!pip install -q datasets==3.6.0\n",
        "!pip install -q lm-eval==0.4.8\n",
        "\n",
        "!pip install hf_xet #To speed up downloads from HF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6qN0mu6IHqpy"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptative Configuration."
      ],
      "metadata": {
        "id": "hYdRIDoV8R3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ADAPTIVE ATTENTION BYPASS (AAB) CONFIGURATION - CORRECTED SCALING TO 100%\n",
        "# =============================================================================\n",
        "\n",
        "ADAPTIVE_CONFIG = {\n",
        "    # Model size-based ratios with proportional scaling to 100%\n",
        "    \"model_size_ratios\": {\n",
        "        \"70B+\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.15, \"scaling_factor\": 0.85},\n",
        "            \"simple\": {\"min_ratio\": 0.35, \"scaling_factor\": 0.65},\n",
        "            \"medium\": {\"min_ratio\": 0.55, \"scaling_factor\": 0.45},\n",
        "            \"complex\": {\"min_ratio\": 0.75, \"scaling_factor\": 0.25},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"30B-70B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.25, \"scaling_factor\": 0.75},\n",
        "            \"simple\": {\"min_ratio\": 0.40, \"scaling_factor\": 0.60},\n",
        "            \"medium\": {\"min_ratio\": 0.60, \"scaling_factor\": 0.40},\n",
        "            \"complex\": {\"min_ratio\": 0.80, \"scaling_factor\": 0.20},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"10B-30B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.30, \"scaling_factor\": 0.75},\n",
        "            \"simple\": {\"min_ratio\": 0.45, \"scaling_factor\": 0.55},\n",
        "            \"medium\": {\"min_ratio\": 0.65, \"scaling_factor\": 0.35},\n",
        "            \"complex\": {\"min_ratio\": 0.82, \"scaling_factor\": 0.18},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"5B-10B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.35, \"scaling_factor\": 0.65},\n",
        "            \"simple\": {\"min_ratio\": 0.55, \"scaling_factor\": 0.45},\n",
        "            \"medium\": {\"min_ratio\": 0.75, \"scaling_factor\": 0.25},\n",
        "            \"complex\": {\"min_ratio\": 0.87, \"scaling_factor\": 0.13},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"2B-5B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.60, \"scaling_factor\": 0.40},\n",
        "            \"simple\": {\"min_ratio\": 0.80, \"scaling_factor\": 0.20},\n",
        "            \"medium\": {\"min_ratio\": 0.90, \"scaling_factor\": 0.20},\n",
        "            \"complex\": {\"min_ratio\": 0.95, \"scaling_factor\": 0.10},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"<2B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.80, \"scaling_factor\": 0.20},\n",
        "            \"simple\": {\"min_ratio\": 0.85, \"scaling_factor\": 0.15},\n",
        "            \"medium\": {\"min_ratio\": 0.90, \"scaling_factor\": 0.10},\n",
        "            \"complex\": {\"min_ratio\": 0.95, \"scaling_factor\": 0.05},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # 5-level complexity thresholds and descriptions\n",
        "    \"complexity_levels\": {\n",
        "        \"trivial\": {\n",
        "            \"range\": [0.0, 0.2],\n",
        "            \"description\": \"Single word answers, basic arithmetic\",\n",
        "            \"layer_groups\": [\"imprescindibles\", \"critical\"]\n",
        "        },\n",
        "        \"simple\": {\n",
        "            \"range\": [0.2, 0.4],\n",
        "            \"description\": \"Simple factual questions\",\n",
        "            \"layer_groups\": [\"imprescindibles\", \"critical\", \"important\"]\n",
        "        },\n",
        "        \"medium\": {\n",
        "            \"range\": [0.4, 0.6],\n",
        "            \"description\": \"Knowledge retrieval, completion tasks\",\n",
        "            \"layer_groups\": [\"imprescindibles\", \"critical\", \"important\", \"standard\"]\n",
        "        },\n",
        "        \"complex\": {\n",
        "            \"range\": [0.6, 0.8],\n",
        "            \"description\": \"Reasoning, explanations\",\n",
        "            \"layer_groups\": [\"imprescindibles\", \"critical\", \"important\", \"standard\", \"optional\"]\n",
        "        },\n",
        "        \"very_complex\": {\n",
        "            \"range\": [0.8, 1.0],\n",
        "            \"description\": \"Deep reasoning, analysis, creativity\",\n",
        "            \"layer_groups\": [\"imprescindibles\", \"critical\", \"important\", \"standard\", \"optional\", \"dispensable\"]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # Layer activation thresholds (1:1 correspondence)\n",
        "    \"activation_thresholds\": {\n",
        "        \"critical\": 0.0,      # Always active above trivial level\n",
        "        \"important\": 0.2,     # Active from simple level\n",
        "        \"standard\": 0.4,      # Active from medium level\n",
        "        \"optional\": 0.6,      # Active from complex level\n",
        "        \"dispensable\": 0.8    # Active from very_complex level\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"✅ CORRECTED 5-level adaptive configuration loaded!\")\n",
        "print(f\"📊 All models can now reach 100% layers for very_complex prompts\")\n",
        "print(f\"🎯 5 complexity levels: {list(ADAPTIVE_CONFIG['complexity_levels'].keys())}\")"
      ],
      "metadata": {
        "id": "feRLhLsw8Voz",
        "outputId": "4b466094-9c0f-4b1c-f2ec-6443f6500de5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ CORRECTED 5-level adaptive configuration loaded!\n",
            "📊 All models can now reach 100% layers for very_complex prompts\n",
            "🎯 5 complexity levels: ['trivial', 'simple', 'medium', 'complex', 'very_complex']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detection and calculation functions."
      ],
      "metadata": {
        "id": "tlPEOEaj8b7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_model_size_category(model):\n",
        "    \"\"\"\n",
        "    Automatically detect model size category from model parameters\n",
        "    \"\"\"\n",
        "    try:\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        size_billion = total_params / 1e9\n",
        "\n",
        "        print(f\"🔍 Detected model size: {size_billion:.2f}B parameters\")\n",
        "\n",
        "        if size_billion >= 70:\n",
        "            return \"70B+\"\n",
        "        elif size_billion >= 30:\n",
        "            return \"30B-70B\"\n",
        "        elif size_billion >= 10:\n",
        "            return \"10B-30B\"\n",
        "        elif size_billion >= 5:\n",
        "            return \"5B-10B\"\n",
        "        elif size_billion >= 2:\n",
        "            return \"2B-5B\"\n",
        "        else:\n",
        "            return \"<2B\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error detecting model size: {e}\")\n",
        "        return \"1B-3B\"\n",
        "\n",
        "\n",
        "def get_context_window_size(model, tokenizer=None):\n",
        "    \"\"\"\n",
        "    Tries to detect the model's maximum context window size (in tokens).\n",
        "    \"\"\"\n",
        "    # Try model config first (most reliable)\n",
        "    if hasattr(model, 'config'):\n",
        "        config = model.config\n",
        "\n",
        "        # Common attribute names for context window\n",
        "        context_attrs = ['max_position_embeddings', 'n_positions', 'max_seq_len',\n",
        "                        'seq_length', 'max_sequence_length', 'context_length']\n",
        "\n",
        "        for attr in context_attrs:\n",
        "            if hasattr(config, attr):\n",
        "                value = getattr(config, attr)\n",
        "                if isinstance(value, int) and value > 0:\n",
        "                    return value\n",
        "\n",
        "    # Try tokenizer as fallback\n",
        "    if tokenizer:\n",
        "        if hasattr(tokenizer, 'model_max_length') and tokenizer.model_max_length != int(1e30):\n",
        "            return tokenizer.model_max_length\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def count_attention_layers_correctly(model):\n",
        "    \"\"\"\n",
        "    Correctly count attention layers by finding main decoder/transformer layers\n",
        "    \"\"\"\n",
        "    # Method 1: Count main decoder layers directly (most reliable)\n",
        "    decoder_layer_count = 0\n",
        "    for name, module in model.named_modules():\n",
        "        module_type = type(module).__name__\n",
        "        # Look for main transformer/decoder layers\n",
        "        if any(layer_type in module_type for layer_type in\n",
        "               ['DecoderLayer', 'TransformerBlock', 'Block', 'Layer']) and \\\n",
        "           any(exclude not in module_type for exclude in\n",
        "               ['Embedding', 'Norm', 'Linear', 'MLP', 'Attention']):\n",
        "            # Make sure it's a numbered layer (e.g., layers.0, layers.1, etc.)\n",
        "            if '.layers.' in name and name.count('.') == 2:  # e.g., \"model.layers.0\"\n",
        "                decoder_layer_count += 1\n",
        "\n",
        "    if decoder_layer_count > 0:\n",
        "        return decoder_layer_count\n",
        "\n",
        "    # Method 2: Use model config as fallback\n",
        "    try:\n",
        "        if hasattr(model, 'config'):\n",
        "            config_attrs = ['num_hidden_layers', 'n_layer', 'num_layers', 'n_layers']\n",
        "            for attr in config_attrs:\n",
        "                if hasattr(model.config, attr):\n",
        "                    return getattr(model.config, attr)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Method 3: Direct access to layers ModuleList\n",
        "    try:\n",
        "        if hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
        "            return len(model.model.layers)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return 16  # Conservative fallback\n",
        "\n",
        "\n",
        "def classify_complexity_level(complexity_score):\n",
        "    \"\"\"\n",
        "    Classify complexity score into one of 5 levels\n",
        "\n",
        "    Args:\n",
        "        complexity_score (float): Complexity score (0.0-1.0)\n",
        "\n",
        "    Returns:\n",
        "        str: Complexity level (\"trivial\", \"simple\", \"medium\", \"complex\", \"very_complex\")\n",
        "    \"\"\"\n",
        "    levels = ADAPTIVE_CONFIG[\"complexity_levels\"]\n",
        "\n",
        "    for level_name, level_config in levels.items():\n",
        "        min_val, max_val = level_config[\"range\"]\n",
        "        if min_val <= complexity_score < max_val:\n",
        "            return level_name\n",
        "\n",
        "    # Handle edge case for exactly 1.0\n",
        "    if complexity_score >= 0.8:\n",
        "        return \"very_complex\"\n",
        "\n",
        "    return \"trivial\"  # Fallback\n",
        "\n",
        "\n",
        "def calculate_active_layers(total_layers, model_size_category, complexity_score):\n",
        "    \"\"\"\n",
        "    Calculate number of active layers based on complexity and model size\n",
        "\n",
        "    Args:\n",
        "        total_layers (int): Total number of attention layers\n",
        "        model_size_category (str): Model size category\n",
        "        complexity_score (float): Complexity score (0.0-1.0)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (active_layers_count, complexity_level, layer_groups_used, min_guaranteed, max_possible)\n",
        "    \"\"\"\n",
        "    # Classify complexity level\n",
        "    complexity_level = classify_complexity_level(complexity_score)\n",
        "\n",
        "    # Get configuration for this model size and complexity\n",
        "    config = ADAPTIVE_CONFIG[\"model_size_ratios\"][model_size_category][complexity_level]\n",
        "    min_ratio = config[\"min_ratio\"]\n",
        "    scaling_factor = config[\"scaling_factor\"]\n",
        "\n",
        "    # Calculate layer counts\n",
        "    min_guaranteed = int(total_layers * min_ratio)\n",
        "    remaining_layers = total_layers - min_guaranteed\n",
        "    additional_layers = int(complexity_score * scaling_factor * remaining_layers)\n",
        "    active_layers = min_guaranteed + additional_layers\n",
        "\n",
        "    # Ensure we don't exceed total layers\n",
        "    active_layers = min(active_layers, total_layers)\n",
        "    max_possible = total_layers  # Always can reach 100%\n",
        "\n",
        "    # Get which layer groups should be used\n",
        "    layer_groups_used = ADAPTIVE_CONFIG[\"complexity_levels\"][complexity_level][\"layer_groups\"]\n",
        "\n",
        "    return active_layers, complexity_level, layer_groups_used, min_guaranteed, max_possible\n",
        "\n",
        "\n",
        "def get_complexity_info(complexity_level):\n",
        "    \"\"\"\n",
        "    Get detailed information about a complexity level\n",
        "    \"\"\"\n",
        "    return ADAPTIVE_CONFIG[\"complexity_levels\"][complexity_level]\n",
        "\n",
        "\n",
        "print(\"✅ Enhanced 5-level calculation functions loaded successfully!\")"
      ],
      "metadata": {
        "id": "7n-xWD2o8hmA",
        "outputId": "e1b3bb2c-200c-4c11-b85f-f9224d94a718",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Enhanced 5-level calculation functions loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H7MMdI1s8lez"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8iRr5iapy5q"
      },
      "source": [
        "# Download the Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9IFTYa9P6Zy",
        "outputId": "93c31d81-6a44-46f0-dfb7-792cfc0d73d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbnDW_tZRTGp"
      },
      "outputs": [],
      "source": [
        "#model_name = 'meta-llama/Llama-3.2-1B'\n",
        "model_name = 'meta-llama/Llama-3.2-3B'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#tokenizer.pad_token = tokenizer.eos_token  # Set pad token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "TkQnkc3U8l-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the configuration with clean, simplified output\n",
        "if 'model' in locals():\n",
        "    # Get model information with improved detection\n",
        "    total_attention_layers = count_attention_layers_correctly(model)\n",
        "    model_category = detect_model_size_category(model)\n",
        "    context_window = get_context_window_size(model, tokenizer if 'tokenizer' in locals() else None)\n",
        "\n",
        "    print(f\"\\n🏗️ Model Analysis:\")\n",
        "    print(f\"   Attention layers: {total_attention_layers}\")\n",
        "    print(f\"   Size category: {model_category}\")\n",
        "    print(f\"   Context window: {context_window if context_window else 'Unknown'} tokens\")\n",
        "    print(f\"   Architecture: {type(model).__name__}\")\n",
        "\n",
        "    # Show layer detection verification\n",
        "    print(f\"\\n🔍 Layer Detection Verification:\")\n",
        "    decoder_layers = [name for name, module in model.named_modules()\n",
        "                     if 'DecoderLayer' in type(module).__name__ and '.layers.' in name]\n",
        "    print(f\"   Found DecoderLayers: {len(decoder_layers)}\")\n",
        "\n",
        "    # Test all 5 complexity levels with simplified table\n",
        "    test_complexities = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "    print(\"\\n🧪 Layer Activation by Complexity Level:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"{'Level':<12} {'Active Layers':<15} {'Usage Ratio':<12}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for complexity in test_complexities:\n",
        "        active, level, groups, min_guaranteed, max_possible = calculate_active_layers(\n",
        "            total_attention_layers, model_category, complexity\n",
        "        )\n",
        "        ratio = active / total_attention_layers\n",
        "\n",
        "        print(f\"{level.capitalize():<12} {active:<15} {ratio:<12.1%}\")\n",
        "\n",
        "    print(f\"\\n📊 Summary for {model_category} model:\")\n",
        "    trivial_config = ADAPTIVE_CONFIG['model_size_ratios'][model_category]['trivial']\n",
        "    trivial_min = int(total_attention_layers * trivial_config['min_ratio'])\n",
        "    print(f\"   • Range: {trivial_min}-{total_attention_layers} layers ({trivial_min/total_attention_layers:.1%}-100%)\")\n",
        "    print(f\"   • Context window: {context_window if context_window else 'Unknown'} tokens\")\n",
        "    print(f\"   • All complexity levels can reach 100% layer usage\")\n",
        "\n",
        "else:\n",
        "    print(\"⏳ Load your model first to test the configuration\")\n",
        "    print(\"\\nTo test, make sure you have:\")\n",
        "    print(\"1. model = ... (your loaded model)\")\n",
        "    print(\"2. tokenizer = ... (optional, your tokenizer)\")"
      ],
      "metadata": {
        "id": "5rUlhopw8s63",
        "outputId": "d01f1b11-2ff9-4c0f-d076-eb75be3b7c6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Detected model size: 3.21B parameters\n",
            "\n",
            "🏗️ Model Analysis:\n",
            "   Attention layers: 28\n",
            "   Size category: 3B-10B\n",
            "   Context window: 131072 tokens\n",
            "   Architecture: LlamaForCausalLM\n",
            "\n",
            "🔍 Layer Detection Verification:\n",
            "   Found DecoderLayers: 28\n",
            "\n",
            "🧪 Layer Activation by Complexity Level:\n",
            "==================================================\n",
            "Level        Active Layers   Usage Ratio \n",
            "--------------------------------------------------\n",
            "Trivial      10              35.7%       \n",
            "Simple       16              57.1%       \n",
            "Medium       21              75.0%       \n",
            "Complex      24              85.7%       \n",
            "Very_complex 28              100.0%      \n",
            "\n",
            "📊 Summary for 3B-10B model:\n",
            "   • Range: 9-28 layers (32.1%-100%)\n",
            "   • Context window: 131072 tokens\n",
            "   • All complexity levels can reach 100% layer usage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0ifeungrbHw"
      },
      "source": [
        "## Study the structure.\n",
        "* Llama-3.2-1B\n",
        "```\n",
        "LlamaForCausalLM(\n",
        "  (model): LlamaModel(\n",
        "    (embed_tokens): Embedding(128256, 2048)\n",
        "    (layers): ModuleList(\n",
        "      (0-15): 16 x LlamaDecoderLayer(\n",
        "        (self_attn): LlamaSdpaAttention(\n",
        "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
        "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
        "          (rotary_emb): LlamaRotaryEmbedding()\n",
        "        )\n",
        "        (mlp): LlamaMLP(\n",
        "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
        "          (act_fn): SiLU()\n",
        "        )\n",
        "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "      )\n",
        "    )\n",
        "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "    (rotary_emb): LlamaRotaryEmbedding()\n",
        "  )\n",
        "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "The model follows the typical structure of modern Llama models, consisting of blocks made up of an Attention layer and an MLP layer with a GLU structure.\n",
        "\n",
        "> If you want to see an example of how to perform pruning on the MLP layers of the model, you can check out the notebook:[Pruning Llama 3.2.](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/6_3_pruning_structured_llama3.2-1b_OK.ipynb) y leer el paper [Exploring GLU expansion ratios: Structured pruning in Llama-3.2 models](https://osf.io/preprints/osf/qgxea)\n",
        "\n",
        "\n",
        "\n",
        "Since the layers form a block, the attention layer cannot be removed without also removing the accompanying MLP layer. For this reason, the decision was made to deactivate their execution during inference.\n",
        "\n",
        "The 1B model has 16 layers, as shown in the structure above, while the 3B model has 28 layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference function & Test Base Model\n",
        "\n",
        "The `get_output` function is designed to generate text  and measure the time taken for different stages of the generation process.\n",
        "\n",
        "It provides insights into the performance of the model and can be used to evaluate the efficiency of text generation."
      ],
      "metadata": {
        "id": "vF4cHUb_rICs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2igvy4z6rGgy"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def get_output(prompt, model=model, tokenizer=tokenizer, num_runs=1, max_length=50):\n",
        "    total_time = 0\n",
        "    generated_outputs = []\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        # Start timing\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Tokenization time\n",
        "        token_start = time.time()\n",
        "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "        token_time = time.time() - token_start\n",
        "\n",
        "        # Generation time\n",
        "        gen_start = time.time()\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            temperature=None,\n",
        "            top_p=None,\n",
        "            do_sample=False,  # Disable sampling\n",
        "            num_beams=5,      # Use beam search\n",
        "            early_stopping=True,  # Stop when end-of-sequence token is generated\n",
        "            no_repeat_ngram_size=2  # Prevent repetition of 2-grams\n",
        "        )\n",
        "        gen_time = time.time() - gen_start\n",
        "\n",
        "        # Decoding time\n",
        "        decode_start = time.time()\n",
        "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        decode_time = time.time() - decode_start\n",
        "\n",
        "        # Total time for this run\n",
        "        total_time += time.time() - start_time\n",
        "        generated_outputs.append(generated)\n",
        "\n",
        "        if num_runs > 1:\n",
        "            print(f\"\\nRun {run + 1}:\")\n",
        "        print(f\"Tokenization time: {token_time*1000:.2f} ms\")\n",
        "        print(f\"Generation time: {gen_time*1000:.2f} ms\")\n",
        "        print(f\"Decoding time: {decode_time*1000:.2f} ms\")\n",
        "        print(f\"Total time: {(time.time() - start_time)*1000:.2f} ms\")\n",
        "\n",
        "    if num_runs > 1:\n",
        "        avg_time = total_time / num_runs\n",
        "        print(f\"\\nAverage time over {num_runs} runs: {avg_time*1000:.2f} ms\")\n",
        "\n",
        "    return generated_outputs[0] if num_runs == 1 else generated_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb87a5cb-1b19-4232-925e-5a794aa0eb6e",
        "id": "lH7cotAxrhO3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run 1:\n",
            "Tokenization time: 1.46 ms\n",
            "Generation time: 2300.10 ms\n",
            "Decoding time: 0.31 ms\n",
            "Total time: 2301.96 ms\n",
            "\n",
            "Run 2:\n",
            "Tokenization time: 0.59 ms\n",
            "Generation time: 1299.41 ms\n",
            "Decoding time: 0.21 ms\n",
            "Total time: 1300.30 ms\n",
            "\n",
            "Average time over 2 runs: 1801.04 ms\n",
            "Generated text: ['Paris is the capital of France and one of the most visited cities in the world. It is a city with a rich history and culture, as well as a vibrant and diverse population. Paris is home to many famous landmarks, including the Eiff', 'Paris is the capital of France and one of the most visited cities in the world. It is a city with a rich history and culture, as well as a vibrant and diverse population. Paris is home to many famous landmarks, including the Eiff']\n"
          ]
        }
      ],
      "source": [
        "# Test the original model\n",
        "prompt = \"Paris is the capital of\"\n",
        "generated = get_output(prompt, num_runs=2)\n",
        "print(f\"Generated text: {generated}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text generation of the original model, as expected, works perfectly and returns a correct and meaningful sentence."
      ],
      "metadata": {
        "id": "mo4IjOYGry0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cpu\")               # actual data moves ↙\n",
        "torch.cuda.empty_cache()      # allocator drops cached blocks"
      ],
      "metadata": {
        "id": "bLN1_gLdt7Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjgf4WA_vF3B"
      },
      "source": [
        "# Pruning the Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQDCkoL6RW3C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdQmEud3zqSp"
      },
      "source": [
        "This function `measure_unpruned_layer_importances` is designed to calculate importance scores for the attention layers in a model.\n",
        "\n",
        "The basic idea is: if a layer's output is very similar to its input, it might not be doing much important work and could be a candidate for pruning. To check the difference I'm using the cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4jGSXw-yGYs"
      },
      "outputs": [],
      "source": [
        "def measure_unpruned_layer_importances(pruned_model, tokenizer, input_texts):\n",
        "    \"\"\"\n",
        "    Measures and returns importance scores for all unpruned (non-bypassed) layers\n",
        "    averaged across multiple input texts.\n",
        "    \"\"\"\n",
        "    # PREPARATION\n",
        "    pruned_model.eval()\n",
        "    device = next(pruned_model.parameters()).device\n",
        "\n",
        "    # Identify unpruned layers\n",
        "    unpruned_layer_indices = [\n",
        "        idx for idx in range(len(pruned_model.model.layers))\n",
        "        if idx not in pruned_model.config.drop_attn_list\n",
        "    ]\n",
        "\n",
        "    # Dictionary to accumulate importance scores across all prompts\n",
        "    accumulated_scores = {idx: 0.0 for idx in unpruned_layer_indices}\n",
        "\n",
        "    # Process each input text\n",
        "    for text_idx, input_text in enumerate(input_texts):\n",
        "        print(f\"\\nProcessing prompt {text_idx + 1}/{len(input_texts)}: '{input_text[:50]}...'\")\n",
        "\n",
        "        # Tokenize the current input\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Temporary storage for each layer's input/output for this prompt\n",
        "        layer_inputs = {}\n",
        "        layer_outputs = {}\n",
        "\n",
        "        # Create hooks\n",
        "        def q_proj_input_hook(layer_idx):\n",
        "            def _hook(module, module_input):\n",
        "                inp = module_input[0] if isinstance(module_input, tuple) else module_input\n",
        "                layer_inputs[layer_idx] = inp.detach().clone()\n",
        "            return _hook\n",
        "\n",
        "        def o_proj_output_hook(layer_idx):\n",
        "            def _hook(module, module_input, module_output):\n",
        "                out = module_output[0] if isinstance(module_output, tuple) else module_output\n",
        "                layer_outputs[layer_idx] = out.detach().clone()\n",
        "            return _hook\n",
        "\n",
        "        # Register hooks for each unpruned layer\n",
        "        handles = []\n",
        "        for idx in unpruned_layer_indices:\n",
        "            layer = pruned_model.model.layers[idx]\n",
        "            handles.append(layer.self_attn.q_proj.register_forward_pre_hook(q_proj_input_hook(idx)))\n",
        "            handles.append(layer.self_attn.o_proj.register_forward_hook(o_proj_output_hook(idx)))\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            _ = pruned_model(**inputs)\n",
        "\n",
        "        # Remove hooks\n",
        "        for h in handles:\n",
        "            h.remove()\n",
        "\n",
        "        # Compute importance scores for this prompt\n",
        "        for idx in unpruned_layer_indices:\n",
        "            if idx in layer_inputs and idx in layer_outputs:\n",
        "                inp = layer_inputs[idx]\n",
        "                out = layer_outputs[idx]\n",
        "\n",
        "                inp_flat = inp.view(inp.size(0), -1)\n",
        "                out_flat = out.view(out.size(0), -1)\n",
        "\n",
        "                similarity = F.cosine_similarity(inp_flat, out_flat, dim=1).mean().item()\n",
        "                importance_score = 1 - similarity\n",
        "                accumulated_scores[idx] += importance_score\n",
        "\n",
        "    # Calculate average importance scores\n",
        "    num_prompts = len(input_texts)\n",
        "    importance_scores = []\n",
        "\n",
        "    print(\"\\nAverage importance scores across all prompts:\")\n",
        "    for idx in unpruned_layer_indices:\n",
        "        avg_score = accumulated_scores[idx] / num_prompts\n",
        "        importance_scores.append((idx, avg_score))\n",
        "        print(f\"Layer {idx} average importance score: {avg_score:.4f}\")\n",
        "\n",
        "    return importance_scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8M8su2qcjg2"
      },
      "source": [
        "The function `bypass_single_layer` is used to disable the attention mechanism of a specific layer in the model without permanently removing or modifying the layer.\n",
        "\n",
        "This is achieved by dynamically overriding the layer’s forward method to bypass its attention computation.\n",
        "\n",
        "As the attention layers are grouped with the MLP Layers we can just remove an attention layer without removing the associated MLP layer. But we can bypass the layer.\n",
        "\n",
        "The bypassed layer skips computationally expensive attention operations, reducing inference time and memory usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yblvEOSgyKd5"
      },
      "outputs": [],
      "source": [
        "def bypass_single_layer(pruned_model, layer_idx):\n",
        "    \"\"\"\n",
        "    Modifies the specified layer's forward method so that attention is bypassed.\n",
        "    \"\"\"\n",
        "    layer = pruned_model.model.layers[layer_idx]\n",
        "    # Store the original forward.\n",
        "    if not hasattr(layer.self_attn, '_original_forward'):\n",
        "        layer.self_attn._original_forward = layer.self_attn.forward\n",
        "\n",
        "    # A new forward that checks whether to bypass\n",
        "    def new_attention_forward(self, hidden_states, attention_mask=None, position_ids=None,\n",
        "                              past_key_value=None, output_attentions=False, use_cache=False,\n",
        "                              **kwargs):\n",
        "        # If this layer is in drop_attn_list, bypass\n",
        "        if getattr(self, 'layer_idx', -1) in pruned_model.config.drop_attn_list:\n",
        "            if use_cache:\n",
        "              # When cache is used, original forward returns 3 values\n",
        "              return hidden_states, None\n",
        "            else:\n",
        "              # Otherwise, it returns 2 values\n",
        "              return hidden_states, None\n",
        "        # Otherwise, use the original forward\n",
        "        return self._original_forward(hidden_states, attention_mask, position_ids,\n",
        "                                      past_key_value, output_attentions, use_cache, **kwargs)\n",
        "\n",
        "    # Set the layer index and forward\n",
        "    layer.self_attn.layer_idx = layer_idx\n",
        "    layer.self_attn.forward = new_attention_forward.__get__(layer.self_attn, type(layer.self_attn))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_shot_pruning_inplace(model, tokenizer, input_texts, num_layers_to_prune):\n",
        "    \"\"\"\n",
        "    Performs pruning on the original model without creating a copy.\n",
        "    Uses multiple input texts to calculate average layer importance.\n",
        "    \"\"\"\n",
        "    # Save original device\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Convert single string to list for backward compatibility\n",
        "    if isinstance(input_texts, str):\n",
        "        input_texts = [input_texts]\n",
        "\n",
        "    # Set up pruning list\n",
        "    if not hasattr(model.config, 'drop_attn_list'):\n",
        "        model.config.drop_attn_list = []\n",
        "\n",
        "    # Measure importance using multiple prompts\n",
        "    scores = measure_unpruned_layer_importances(model, tokenizer, input_texts)\n",
        "\n",
        "    if len(scores) < num_layers_to_prune:\n",
        "        raise ValueError(\"Requested more layers to prune than exist\")\n",
        "\n",
        "    # Sort and select layers to bypass\n",
        "    scores.sort(key=lambda x: x[1])  # ascending order (least important first)\n",
        "    layers_to_bypass = [idx for idx, _ in scores[:num_layers_to_prune]]\n",
        "\n",
        "    # Bypass selected layers\n",
        "    for idx in layers_to_bypass:\n",
        "        model.config.drop_attn_list.append(idx)\n",
        "        bypass_single_layer(model, idx)\n",
        "        print(f\"Bypassing layer {idx} with average importance score {dict(scores)[idx]:.4f}\")\n",
        "\n",
        "    print(f\"\\nBypassed layers: {sorted(model.config.drop_attn_list)}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "5xf5Uq6YJVm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOuKyH0MjyJM"
      },
      "source": [
        "## Execute Pruning."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disclaimer**\n",
        "\n",
        "I'm using a single illustrative prompt so that the code path is easy to follow. In any research or production setting you must feed hundreds or thousands of diverse prompts before deciding which layers to deactivate"
      ],
      "metadata": {
        "id": "wyI5XiqYyBnc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvhSYyE2Rk1H",
        "outputId": "6c0bcec2-1882-44ef-9d23-002da7398026"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing prompt 1/5: 'Hi I'm a sample text, used to calculate the cosine...'\n",
            "\n",
            "Processing prompt 2/5: 'The quick brown fox jumps over the lazy dog....'\n",
            "\n",
            "Processing prompt 3/5: 'Machine learning models can be optimized through v...'\n",
            "\n",
            "Processing prompt 4/5: 'Paris is the capital of France and a major Europea...'\n",
            "\n",
            "Processing prompt 5/5: 'What is the meaning of life, the universe, and eve...'\n",
            "\n",
            "Average importance scores across all prompts:\n",
            "Layer 0 average importance score: 1.1273\n",
            "Layer 1 average importance score: 0.9923\n",
            "Layer 2 average importance score: 0.9773\n",
            "Layer 3 average importance score: 1.0557\n",
            "Layer 4 average importance score: 1.0411\n",
            "Layer 5 average importance score: 1.0980\n",
            "Layer 6 average importance score: 1.1469\n",
            "Layer 7 average importance score: 1.1026\n",
            "Layer 8 average importance score: 1.1545\n",
            "Layer 9 average importance score: 1.0130\n",
            "Layer 10 average importance score: 1.1739\n",
            "Layer 11 average importance score: 0.8901\n",
            "Layer 12 average importance score: 0.8444\n",
            "Layer 13 average importance score: 0.8245\n",
            "Layer 14 average importance score: 0.8109\n",
            "Layer 15 average importance score: 1.1473\n",
            "Bypassing layer 14 with average importance score 0.8109\n",
            "Bypassing layer 13 with average importance score 0.8245\n",
            "Bypassing layer 12 with average importance score 0.8444\n",
            "Bypassing layer 11 with average importance score 0.8901\n",
            "\n",
            "Bypassed layers: [11, 12, 13, 14]\n"
          ]
        }
      ],
      "source": [
        "# Using multiple prompts for calibration\n",
        "calibration_prompts = [\n",
        "    \"Hi I'm a sample text, used to calculate the cosine difference between input and output.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Machine learning models can be optimized through various techniques.\",\n",
        "    \"Paris is the capital of France and a major European city.\",\n",
        "    \"What is the meaning of life, the universe, and everything?\"\n",
        "]\n",
        "\n",
        "pruned_model = one_shot_pruning_inplace(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    calibration_prompts,  # Now passing a list of prompts\n",
        "    num_layers_to_prune=4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46_tIN1brKZW"
      },
      "source": [
        "# Test Pruned Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFdCesVcnnrM"
      },
      "source": [
        "Now, let's test the pruned model, which is a Llama-3.2-3B model where I have marked 4 Attention layers to be bypassed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oCJ8EYLhJg-",
        "outputId": "8ecaa3b1-5d72-44ba-b033-d82d70eb097f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run 1:\n",
            "Tokenization time: 0.46 ms\n",
            "Generation time: 1251.89 ms\n",
            "Decoding time: 0.23 ms\n",
            "Total time: 1252.68 ms\n",
            "\n",
            "Run 2:\n",
            "Tokenization time: 0.59 ms\n",
            "Generation time: 1245.58 ms\n",
            "Decoding time: 0.21 ms\n",
            "Total time: 1246.48 ms\n",
            "\n",
            "Average time over 2 runs: 1249.48 ms\n",
            "Generated text: ['Paris is the capital of France and/or world-wide fame for its beautiful skyline skyline-top-top-bottom-bottomside-side-side sidesidednessnessNESSNESSnessinessinessnessesivenessivenessfulnessnessfulnessfulnessinessesenessesinesssinessESness', 'Paris is the capital of France and/or world-wide fame for its beautiful skyline skyline-top-top-bottom-bottomside-side-side sidesidednessnessNESSNESSnessinessinessnessesivenessivenessfulnessnessfulnessfulnessinessesenessesinesssinessESness']\n"
          ]
        }
      ],
      "source": [
        "# Test the pruned model\n",
        "pruned_model = pruned_model.to(device) #Move the model to GPU again.\n",
        "generated = get_output(prompt, pruned_model, num_runs=2)\n",
        "print(f\"Generated text: {generated}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ic4ux7goGu_"
      },
      "source": [
        "\n",
        "The execution of this second model is slightly faster than that of the base model, and the generated text is fairly accurate, although some repetition can be noticed towards the end of the sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Store the Model.\n"
      ],
      "metadata": {
        "id": "H7R_VaIPTQhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_model_name = 'attnprun-llama-3.2-3B'\n",
        "output_dir = './'+new_model_name\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "pruned_model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "#new_config.save_pretrained(output_dir)\n",
        "print(f\"Pruned model saved to {output_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERXYQsy5TUtn",
        "outputId": "50e669d9-04ed-40bd-835e-463ed60a3d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned model saved to ./attnprun-llama-3.2-3B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Check that config contains layers to skip\n",
        "from transformers import AutoConfig\n",
        "config = AutoConfig.from_pretrained(output_dir)\n",
        "\n",
        "if hasattr(config, \"drop_attn_list\"):\n",
        "    print(f\"drop_attn_list stored: {config.drop_attn_list}\")\n",
        "else:\n",
        "    print(\"drop_attn_list isn't present.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdE7o0PETggr",
        "outputId": "66718550-f72e-47d2-b2d3-a96d1cb04c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drop_attn_list stored: [14, 13, 12, 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload to Hugging Face."
      ],
      "metadata": {
        "id": "hG63t8jqVdOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El proceso de subida de este modelo a Hugging es ligeramente más complejo por que se debe almacenar no tan solo el modelo en si, sino tambien el código de la función _bypass_single_layer. Que como recordarás es la función que se encarga de decidir cuando ejecutar o simplemente bypasear una capa de atención.  "
      ],
      "metadata": {
        "id": "YRDoj5bYKWVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, upload_folder, whoami"
      ],
      "metadata": {
        "id": "0bi3zX7FVjwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Get your HF username from the current token\n",
        "username = whoami()[\"name\"]  # Returns a dict like {'name': 'your_username', 'email': ...}\n",
        "username"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_AnSaJPuWffg",
        "outputId": "01f84a9a-6d3c-469f-9cf2-67225de8a998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'oopere'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define repo name\n",
        "repo_id = f\"{username}/{new_model_name}\""
      ],
      "metadata": {
        "id": "KDuY1p8zWjax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define path to your model\n",
        "output_dir = \"./\"+new_model_name\n"
      ],
      "metadata": {
        "id": "-MNgDvImWmFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function must be saved in a .py file, but since this notebook runs on Colab, I’ve decided the best approach is to create a cell that generates the file to be uploaded.\n",
        "\n",
        "The file contains the custom class PrunedLlamaForCausalLM, which extends Hugging Face’s LlamaForCausalLM.\n",
        "\n",
        "This custom class calls the base constructor, ensuring that the model's configuration file includes the drop_attn_list, which specifies the layers that should be skipped.\n",
        "\n",
        "The forward function is modified only for the layers that need to be skipped; the rest continue executing their standard forward function.\n"
      ],
      "metadata": {
        "id": "dJSb5z2nLRS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_model_code = '''\n",
        "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
        "\n",
        "class PrunedLlamaForCausalLM(LlamaForCausalLM):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        if not hasattr(config, \"drop_attn_list\"):\n",
        "            config.drop_attn_list = []\n",
        "\n",
        "        for idx in config.drop_attn_list:\n",
        "            self._bypass_single_layer(idx)\n",
        "\n",
        "    def _bypass_single_layer(self, layer_idx):\n",
        "        \"\"\"\n",
        "        Modifies the specified layer's forward method so that attention is bypassed.\n",
        "        \"\"\"\n",
        "        layer = self.model.layers[layer_idx]\n",
        "        if not hasattr(layer.self_attn, \"_original_forward\"):\n",
        "            layer.self_attn._original_forward = layer.self_attn.forward\n",
        "\n",
        "        def new_attention_forward(self, hidden_states, attention_mask=None, position_ids=None,\n",
        "                                  past_key_value=None, output_attentions=False, use_cache=False,\n",
        "                                  **kwargs):\n",
        "            if getattr(self, \"layer_idx\", -1) in self.config.drop_attn_list:\n",
        "                if use_cache:\n",
        "                    return hidden_states, None\n",
        "                else:\n",
        "                    return hidden_states, None\n",
        "            return self._original_forward(hidden_states, attention_mask, position_ids,\n",
        "                                          past_key_value, output_attentions, use_cache, **kwargs)\n",
        "\n",
        "        layer.self_attn.layer_idx = layer_idx\n",
        "        layer.self_attn.forward = new_attention_forward.__get__(layer.self_attn, type(layer.self_attn))\n",
        "\n",
        "'''\n",
        "\n",
        "# Define path and write the file\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "with open(os.path.join(output_dir, \"modeling_attnprun_llama.py\"), \"w\") as f:\n",
        "    f.write(custom_model_code.strip())\n",
        "\n",
        "print(\"Custom model script modeling_attnprun_llama.py created successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_xaG0FDXojc",
        "outputId": "f79b2cf2-59fb-48c6-fece-b2410f91b7cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom model script modeling_attnprun_llama.py created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the model's configuration file is updated by adding the `auto_map` field, which tells the Transformers library which class to use to construct the model: `modeling_attnprun_llama.PrunedLlamaForCausalLM.`\n"
      ],
      "metadata": {
        "id": "r59Yae3jVMpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Path to the config file\n",
        "config_path = os.path.join(output_dir, \"config.json\")\n",
        "\n",
        "# Load the existing config\n",
        "with open(config_path, \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Add or update the auto_map section\n",
        "config[\"auto_map\"] = {\n",
        "    \"AutoModelForCausalLM\": \"modeling_attnprun_llama.PrunedLlamaForCausalLM\"\n",
        "}\n",
        "\n",
        "# Optional: ensure the architecture field is aligned\n",
        "config[\"architectures\"] = [\"PrunedLlamaForCausalLM\"]\n",
        "\n",
        "# Save the updated config\n",
        "with open(config_path, \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"config.json updated with auto_map and architecture.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBbxbBbmbeRl",
        "outputId": "1105e204-17e1-4cd5-b93e-733cad7b00ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json updated with auto_map and architecture.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time to upload the folder containing the weights, the config file and the new function to HF."
      ],
      "metadata": {
        "id": "0ciXxElsWsQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Upload the folder to the Hub\n",
        "upload_folder(\n",
        "    folder_path=output_dir,\n",
        "    path_in_repo=\"\",  # Upload everything to root\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "print(f\"Model uploaded successfully to https://huggingface.co/{repo_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCUToiTmWp9M",
        "outputId": "e0c99d01-b250-471e-91ef-39959ab662a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model uploaded successfully to https://huggingface.co/oopere/attnprun-llama-3.2-3B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download model from Hugging Face."
      ],
      "metadata": {
        "id": "8Lp3bWMOehLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "del pruned_model\n",
        "del tokenizer\n",
        "del model\n",
        "\n",
        "# 2. Libera la caché de la GPU\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()  # Opcional, ayuda en Colab\n",
        "\n",
        "# 3. Forza recolección de basura en Python\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2eDLdnEmapS",
        "outputId": "66087f86-ec21-4fa5-a240-a4d78ff549e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "186"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is downloaded normally from Hugging Face, but you must remember to set `trust_remote_code=True` since the model includes the custom code you previously created and uploaded.\n"
      ],
      "metadata": {
        "id": "UgHQ5-2MW6q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_model_name=\"oopere/attnprun-llama-3.2-3B\"\n",
        "\n",
        "model_hf = AutoModelForCausalLM.from_pretrained(\n",
        "    pruned_model_name,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pruned_model_name)"
      ],
      "metadata": {
        "id": "d4AB09pNemii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_hf = model_hf.to(device) #Move the model to GPU again.\n",
        "generated = get_output(prompt, model_hf, num_runs=2)\n",
        "print(f\"Generated text: {generated}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2feKDCrPll0E",
        "outputId": "79f8010b-6fed-4460-c2ea-5442036f7a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run 1:\n",
            "Tokenization time: 0.77 ms\n",
            "Generation time: 1245.88 ms\n",
            "Decoding time: 0.23 ms\n",
            "Total time: 1246.97 ms\n",
            "\n",
            "Run 2:\n",
            "Tokenization time: 0.49 ms\n",
            "Generation time: 1239.63 ms\n",
            "Decoding time: 0.19 ms\n",
            "Total time: 1240.41 ms\n",
            "\n",
            "Average time over 2 runs: 1243.60 ms\n",
            "Generated text: ['Paris is the capital of France and/or world-wide fame for its beautiful skyline skyline-top-top-bottom-bottomside-side-side sidesidednessnessNESSNESSnessinessinessnessesivenessivenessfulnessnessfulnessfulnessinessesenessesinesssinessESness', 'Paris is the capital of France and/or world-wide fame for its beautiful skyline skyline-top-top-bottom-bottomside-side-side sidesidednessnessNESSNESSnessinessinessnessesivenessivenessfulnessnessfulnessfulnessinessesenessesinesssinessESness']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CsaXViPtmft"
      },
      "source": [
        "# Conclusion.\n",
        "Based on the findings in the paper and the results obtained, I believe this type of pruning may work better with larger models where attention layers tend to have redundancy.\n",
        "\n",
        "Since this type of pruning does not alter the model's structure, it does not result in a reduction in its size or the memory required to load it. The main advantage of using this pruning approach is the reduction of computational load during inference, leading to a more efficient model with faster responses and lower resource consumption.\n",
        "\n",
        "Unlike the original paper, which describes \"removing\" selected attention layers but provides limited implementation details, this implementation takes a transparent functional approach by explicitly overriding the `forward` method only in the specified layers. As a result, the model retains its full architecture and parameter set, but selectively skips computations at runtime. This makes the method reversible, modular, and fully compatible with the Hugging Face ecosystem using `trust_remote_code=True`. While both approaches achieve similar computational savings, this one emphasizes clarity, portability, and practical integration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XDtwh-dwMHh"
      },
      "source": [
        "# Authors Note.\n",
        "\n",
        "In addition to creating content like this notebook and offering it under the MIT license, I have also contributed to repositories such as those of Hugging Face and Google Gemini.\n",
        "\n",
        "I am especially proud of my book: [Large Language Models: Apply and Implement Strategies for Large Language Models (Apress)(https://amzn.to/3DSepLb).\n",
        "\n",
        "You can find it on both [Amazon](https://amzn.to/3DSepLb) and [Springer](https://link.springer.com/book/10.1007/979-8-8688-0515-8), where they often have good deals on the purchase price.\n",
        "\n",
        "If you take a look and end up purchasing it, keep in mind that you can reach out with any questions via the Discussions section of this same repository or on any of my social media channels. I’ll do my best to respond as quickly as possible."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "authorship_tag": "ABX9TyOPQWHbQhK2FHV3+5s48mJI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}