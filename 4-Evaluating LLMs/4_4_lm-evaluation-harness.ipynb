{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Large-Language-Model-Notebooks-Course/blob/main/4-Evaluating%20LLMs/4_4_lm-evaluation-harness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "    <h1>Large Language Models Projects</a></h1>\n",
        "    <h3>Apply and Implement Strategies for Large Language Models</h3>\n",
        "    <h2>4.4-Evaluating Open Source Models with lm-evaluation-harness.\n",
        "  </h2>\n",
        "    <h3>Evaluate models from Hugging Face using EleutherAI/lm-evaluation-harness.</h3>\n",
        "</div>\n",
        "\n",
        "by [Pere Martra](https://www.linkedin.com/in/pere-martra)\n",
        "\n",
        "____________\n",
        "Models: .\n",
        "\n",
        "Colab Environment:\n",
        "\n",
        "Keys:\n",
        "* EleutherAI.\n",
        "* lm-evaluation-harness\n",
        "* Hugging Face\n",
        "___________\n",
        "\n",
        "This is the unofficial repository for the book:\n",
        "        <a href=\"https://amzn.to/4eanT1g\"> <b>Large Language Models:</b> Apply and Implement Strategies for Large Language Models</a> (<a href=\"https://link.springer.com/book/10.1007/979-8-8688-0515-8\">Apress</a>).\n",
        "        The book is based on the content of this repository, but the notebooks are being updated, and I am incorporating new examples and chapters.\n",
        "        If you are looking for the official repository for the book, with the original notebooks, you should visit the\n",
        "        <a href=\"https://github.com/Apress/Large-Language-Models-Projects\">Apress repository</a>, where you can find all the notebooks in their original format as they appear in the book."
      ],
      "metadata": {
        "id": "f3cvnSVemTSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installing libraries"
      ],
      "metadata": {
        "id": "KNW-LniD2I2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q lm-eval\n",
        "from lm_eval import evaluator, tasks, models"
      ],
      "metadata": {
        "id": "-icExtBXMD7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Models."
      ],
      "metadata": {
        "id": "o2uIRXCpMC_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_hf_model(model_name, tasks=['arc_easy'], num_fewshot=0):\n",
        "    \"\"\"\n",
        "    It calls the evaluator to evaluate a model available on Hugging Face.\n",
        "\n",
        "    Args:\n",
        "    - model_name: The model name in hugging Face.\n",
        "    - tasks: Tasks to evaluate.\n",
        "    - num_fewshot: Number of examples of few-shot learning\n",
        "\n",
        "    Returns:\n",
        "    - metrics.\n",
        "    \"\"\"\n",
        "    model_args = f\"pretrained={model_name},device=cuda\"\n",
        "    tasks = tasks\n",
        "\n",
        "    results = evaluator.simple_evaluate(\n",
        "      model=\"hf\",\n",
        "      model_args=model_args,\n",
        "      tasks=tasks,\n",
        "      num_fewshot=0,  # Number of few-shot smaples.\n",
        "      limit=None,  # Use all the samples in the Evaluate Dataset.\n",
        "      bootstrap_iters=10\n",
        "    )\n",
        "\n",
        "    metrics = results.get('results', {})\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "Uq_oA65hML0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select tasks to evaluate.\n",
        "tasks = ['lambada', 'boolq']"
      ],
      "metadata": {
        "id": "0AJGD6qgMUDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_base = evaluate_hf_model(\"meta-llama/Llama-3.2-1B\", tasks=tasks)"
      ],
      "metadata": {
        "id": "pCIBfX7LMVU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_base"
      ],
      "metadata": {
        "id": "C03M9XrxMbjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_pruned = evaluate_hf_model(\"oopere/pruned40-llama-1b\", tasks=tasks)"
      ],
      "metadata": {
        "id": "ekNn5BoNMdRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_pruned"
      ],
      "metadata": {
        "id": "84UkXukOMkUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykg5TYA033yR"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}